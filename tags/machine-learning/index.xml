<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Luyao Peng&#39;s Blog</title>
    <link>/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on Luyao Peng&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 20 Apr 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Back Propogation in LSTM</title>
      <link>/post/stat209/</link>
      <pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/stat209/</guid>
      <description>R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.
When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:</description>
    </item>
    
    <item>
      <title>Machine Learning STAT209 Review</title>
      <link>/post/stat209/</link>
      <pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/stat209/</guid>
      <description>1. Bayes Classification Rule 1.1 Decision Rule Let \(\delta(\mathbf{x}) \rightarrow \left\{0, 1 \right\}\) be the classification rule for class 0 or 1. The expected cost is
\(R(\delta) = \int_{R_1(\mathbf{x})}\pi_0 Cost(1|0) f(\mathbf{x}|c = 0) + \int_{R_0(\mathbf{x})}\pi_1 Cost(0|1) f(\mathbf{x}|c = 1)\)
To minimize \(R(\delta)\), the decision rule is
\(\delta(\mathbf{x}) = \begin{cases} 1, \frac{\pi_0Cost(1|0)f_0(\mathbf{x})}{\pi_0 f_0+ \pi_1 f_1} &amp;lt; \frac{\pi_1Cost(0|1)f_1(\mathbf{x})}{\pi_0 f_0+ \pi_1 f_1} \\ 0, ow\end{cases}\)
equivalently,
\(\delta(\mathbf{x}) = \begin{cases} 1, p(c=1|\mathbf{x} = x) &amp;gt; \frac{Cost(1|0)}{C(1|0)+Cost(0|1)}\\ 0, ow\end{cases}\)</description>
    </item>
    
    <item>
      <title>Project: KPCA for one-class classification in automated essay scoring and forensic analysis</title>
      <link>/post/kpca-for-one-class-classification/</link>
      <pubDate>Thu, 07 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/kpca-for-one-class-classification/</guid>
      <description>One-class classification problem is useful in many applications. For example, if people want to diagnoze the healthy condition of machines, measurements on the normal operation of the machine are easy to obtain, and most faults will not haveoccurred so one will have little or no training data for the negative class. Another example is web security detection, people only have data for normal web behavior , since once abnormal behavior occurs, the web security will be attacked, which is a situation people try to prevent from happening.</description>
    </item>
    
  </channel>
</rss>