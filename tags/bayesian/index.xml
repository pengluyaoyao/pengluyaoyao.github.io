<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>bayesian on Luyao Peng&#39;s Blog</title>
    <link>/tags/bayesian/</link>
    <description>Recent content in bayesian on Luyao Peng&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 10 Dec 2020 16:01:23 +0800</lastBuildDate>
    
	<atom:link href="/tags/bayesian/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deep Probabilistic Programming</title>
      <link>/post/deep-bandipt-showdown/</link>
      <pubDate>Thu, 10 Dec 2020 16:01:23 +0800</pubDate>
      
      <guid>/post/deep-bandipt-showdown/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sparse Variational Gaussian Process in Multiclass Classification</title>
      <link>/post/vgpc/</link>
      <pubDate>Thu, 05 Nov 2020 16:01:23 +0800</pubDate>
      
      <guid>/post/vgpc/</guid>
      <description>In this notebook, sparse variational gaussian process model (VGP) is applied to a multiclass classification problem. VGP is easily scalable to large scale dataset.
Background Consider making inference about a stochastic function \(f\) given a likelihood \(p(y|f)\) and \(N\) observations \(y=\{y_1, y_2, \dots, y_N\}^T\) at observation index points \(X=\{x_1, x_2, \dots, x_N\}^T\). Place a GP prior on \(f\): \(p(f) \sim N(f|m(X), K(X, X))\). The joint distribution of data and latent stochastic function is</description>
    </item>
    
    <item>
      <title>Empirical Bayesian Neural Network</title>
      <link>/post/empirical-bnn/</link>
      <pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/empirical-bnn/</guid>
      <description>Bayesian approaches to neural networks (BNNs) training [1] have gained increased popularity in NLP community due to its merits of scalability, providing inference uncertainty, and its ensemble learning nature. Despite those features, BNNs are sensitive to the choice of prior and require to tune the parameters in the prior distribution assumed for model weights and biases [2][3]. Sensitivity to the prior and its initialization makes BNNs difficult to train in practice.</description>
    </item>
    
  </channel>
</rss>