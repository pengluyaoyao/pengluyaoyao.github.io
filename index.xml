<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Luyao Peng&#39;s Blog</title>
    <link>/</link>
    <description>Recent content on Luyao Peng&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 10 Dec 2020 16:01:23 +0800</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Sun, 20 Aug 2017 21:38:52 +0800</pubDate>
      
      <guid>/about/</guid>
      <description>My name is Luyao Peng. I am a data scientist in NLP (natural language processing) with industry and research experience in text generation, general language understanding evaluation and chatbot, and educational background in statistics (Ph.D.) and linguistics (M.A.).
My current C.V.
My Personal Research Projects:
 Gaussian Process Classification by Tensorflow-probability (GPC jupyter notebook example) Bert Summarization and Generation of Chinese Text Neural Text Generation Using linear mixed-effects model to detect fraudulent erasures at an aggregate level (National Council on Measurement in Education 2020, conference abstract, pg113) An algorithmic construction of all unbiased estimators of variance components in linear mixed effects models (Joint Statistical Meetings 2019, conference abstract)  My Visualization of Data Science Projects:</description>
    </item>
    
    <item>
      <title>Deep Probabilistic Programming</title>
      <link>/post/deep-bandipt-showdown/</link>
      <pubDate>Thu, 10 Dec 2020 16:01:23 +0800</pubDate>
      
      <guid>/post/deep-bandipt-showdown/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sparse Variational Gaussian Process in Multiclass Classification</title>
      <link>/post/vgpc/</link>
      <pubDate>Thu, 05 Nov 2020 16:01:23 +0800</pubDate>
      
      <guid>/post/vgpc/</guid>
      <description>In this notebook, sparse variational gaussian process model (VGP) is applied to a multiclass classification problem. VGP is easily scalable to large scale dataset.
Background Consider making inference about a stochastic function \(f\) given a likelihood \(p(y|f)\) and \(N\) observations \(y=\{y_1, y_2, \dots, y_N\}^T\) at observation index points \(X=\{x_1, x_2, \dots, x_N\}^T\). Place a GP prior on \(f\): \(p(f) \sim N(f|m(X), K(X, X))\). The joint distribution of data and latent stochastic function is</description>
    </item>
    
    <item>
      <title>State Space Model for Sequential Data</title>
      <link>/post/state-space-model/</link>
      <pubDate>Sat, 10 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/state-space-model/</guid>
      <description>A state space model defines the hidden state at time \(t\), \(z_t\), to be a linear function of the input at time \(t\), \(u_t\), and the hidden state at time \(t-1\), \(z_{t-1}\), and defines the output at time \(t\) to be a linear function of the hidden state \(z_t\). The model can be written as
\[\begin{equation} \begin{array}{rcl} \mbox{Transition model} \mathbf{z}_t &amp;amp;=&amp;amp; \mathbf{A}_t \mathbf{z}_{t-1} + \mathbf{B}_t\mathbf{u}_t + \mathbf{\epsilon}_t \\ \mbox{Observation model} \mathbf{y}_t &amp;amp;=&amp;amp; \mathbf{C}_t\mathbf{z}_t + \mathbf{D} \mathbf{u}_t + \mathbf{\delta}_t \\ &amp;amp;&amp;amp; \mathbf{\epsilon}_t \sim N(\mathbf{0}, \mathbf{Q}_t), \mathbf{\delta}_t \sim N(\mathbf{0}, \mathbf{R}_t) \end{array} \end{equation}\]</description>
    </item>
    
    <item>
      <title>Empirical Bayesian Neural Network</title>
      <link>/post/empirical-bnn/</link>
      <pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/empirical-bnn/</guid>
      <description>Bayesian approaches to neural networks (BNNs) training [1] have gained increased popularity in NLP community due to its merits of scalability, providing inference uncertainty, and its ensemble learning nature. Despite those features, BNNs are sensitive to the choice of prior and require to tune the parameters in the prior distribution assumed for model weights and biases [2][3]. Sensitivity to the prior and its initialization makes BNNs difficult to train in practice.</description>
    </item>
    
    <item>
      <title>Doubly Stochastic Deep Gaussian Process</title>
      <link>/post/doubly-deep-gp/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/doubly-deep-gp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Gaussian Process Classification by Tensorflow-probability</title>
      <link>/post/gpc/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/gpc/</guid>
      <description>In binary classification, a response function (or a link function) is used to turn the output of a regression model into class probability, so the domain of the output from a regression model is squashed into [0, 1] range. Common link function is logistic function (another one is probit function, which will not be discussed here). For a data point \((x_i, c_i)\),
\[ p(c_i|x_i, w) = \frac{1}{1+exp(-x_i&amp;#39;w)} = \sigma(x_i&amp;#39;w) \]</description>
    </item>
    
    <item>
      <title>Neural Text Generation</title>
      <link>/post/2020-03-07-content-generation/</link>
      <pubDate>Sat, 07 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020-03-07-content-generation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Correspondence Between Infinitely Wide Deep Neural Networks and Gaussian Process</title>
      <link>/post/gp-in-dw/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/gp-in-dw/</guid>
      <description>Equivalency between Gaussian Process and DNNs Previous studies have shown the equivalency between Gaussian Process (GP) [1] and infinitely wide fully-connected Neural Networks (NNs) [2][4], which implies that if we choose a fully-connected NN with infinite width, the function computed by the NN is equivalent to a function drawn from a GP under appropriate statistical assumptions. The analytic forms of GP kernel for DNNs were also subsequentially developed.
The following figure shows the consistency between the theoretical GP (contour rings) and the sampled \(z_i\) at the initial layer (left) and the third layer (right) in a DNN.</description>
    </item>
    
    <item>
      <title>CS224n/assignment3/: RNN/GRU Name Entity Recognition</title>
      <link>/post/rnn_ner/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/rnn_ner/</guid>
      <description>This post deals with the name entity recognition task using RNN model. The RNN model for NER Let \(\mathbf{x}_t\) be a one-hot vector for word at time \(t\), define \(\mathbf{E}\in \mathbb{R}^{V\times D}, \mathbf{W}_h \in \mathbb{R}^{H\times H}, \mathbf{W}_e\in \mathbb{R}^{D\times H}, \mathbf{U} \in \mathbb{R}^{H\times (C=5)},\mathbf{b}_1\in \mathbb{R}^{H}, \mathbf{b}_2 \in \mathbb{R}^{C}\), the RNN model to make prediction at time step \(t\) can be expressed as \[\mathbf{e}^t = \mathbf{x}^t\mathbf{E}\\ \mathbf{h}^t = \sigma(\mathbf{h}^{t-1}\mathbf{W}_h + \mathbf{e}^t\mathbf{W}_e+\mathbf{b}_1) \\</description>
    </item>
    
    <item>
      <title>CS224n/assignment3/: Window-based Name Entity Recognition (Baseline Model)</title>
      <link>/post/window_based_name_entity_recognition/</link>
      <pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/window_based_name_entity_recognition/</guid>
      <description>Introduction This assignment built 3 different models for named entity recognition (NER) task. For a given word in a context, we want to predict the name entity of the word in one of the following 5 categories:
 Person (PER): Organization (ORG): Location (LOC): Miscellaneous (MISC): Null (O): the word do not represent a named entity and most of the words fall into this categroy.  This is a 5-class classification problem, which implies a label vector of [PER, ORG, LOC, MISC, O].</description>
    </item>
    
    <item>
      <title>Basics of RNN and LSTM</title>
      <link>/post/basics-of-rnn-and-lstm/</link>
      <pubDate>Wed, 24 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/basics-of-rnn-and-lstm/</guid>
      <description>RNN The language model computes the probability of a sequence of previous words, \(P(word_1, word_2, \dots, word_t)\). The traditional language model is based on naive bayes model, the probability of a sequence of words is:
\[ P(word_1, word_2, \dots, word_{t}) = \prod_{i = 1}^{t} P(word_i|word_1, \dots, word_{i-1}). \]
The cons of traditional language models assumes independence among words (this is the naive part), which is not true in reality, it also requires a lot of RAM to compute the conditional probabilities for each word.</description>
    </item>
    
    <item>
      <title>Machine Learning STAT209 Review</title>
      <link>/post/stat209/</link>
      <pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/stat209/</guid>
      <description>--- title: &#39;Machine Learning STAT209 Review&#39; author: &#39;&#39; date: &#39;2019-03-11&#39; slug: STAT209 categories: - Statistics tags: - Machine Learning header-includes: \usepackage{amsmath} \usepackage{bm} --- 1. Bayes Classification Rule 1.1 Decision Rule Let \(\delta(\mathbf{x}) \rightarrow \left\{0, 1 \right\}\) be the classification rule for class 0 or 1. The expected cost is
\(R(\delta) = \int_{R_1(\mathbf{x})}\pi_0 Cost(1|0) f(\mathbf{x}|c = 0) + \int_{R_0(\mathbf{x})}\pi_1 Cost(0|1) f(\mathbf{x}|c = 1)\)
To minimize \(R(\delta)\), the decision rule is</description>
    </item>
    
    <item>
      <title>R Package “regrrr”: Compiling and Visualizing Regression Results</title>
      <link>/post/regrrr-one-stop-r-toolkit-for-compiling-regression-results/</link>
      <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/regrrr-one-stop-r-toolkit-for-compiling-regression-results/</guid>
      <description>This is an R package “regrrr” on Cran I coauthored with Rui Yang.
In strategy/management research, we always need to compile the regression results into the publishable format and sometimes plot the moderating effects. This package does the job.
Here is the quickstart guide.
 
Installation To install from CRAN:
install.packages(&amp;quot;regrrr&amp;quot;) library(regrrr) You can also use devtools to install the latest development version:
devtools::install_github(&amp;quot;raykyang/regrrr&amp;quot;) library(regrrr)  Examples compile the correlation table library(regrrr) ## Warning: package &amp;#39;regrrr&amp;#39; was built under R version 3.</description>
    </item>
    
    <item>
      <title>KPCA for One-class Classification in Automated Essay Scoring and Forensic Analysis</title>
      <link>/post/kpca-for-one-class-classification/</link>
      <pubDate>Thu, 07 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/kpca-for-one-class-classification/</guid>
      <description>One-class classification problem is useful in many applications. For example, if people want to diagnoze the healthy condition of machines, measurements on the normal operation of the machine are easy to obtain, and most faults will not haveoccurred so one will have little or no training data for the negative class. Another example is web security detection, people only have data for normal web behavior , since once abnormal behavior occurs, the web security will be attacked, which is a situation people try to prevent from happening.</description>
    </item>
    
    <item>
      <title>R Package “MMeM”: Multivariate Mixed-effects Model</title>
      <link>/post/multivariate-mixed-effects-model-r-package/</link>
      <pubDate>Thu, 07 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/multivariate-mixed-effects-model-r-package/</guid>
      <description>This package estimates the variance covariance components for data under multivariate mixed effects model using multivariate REML and multivariate Henderson3 methods. See Meyer (1985) doi:10.2307/2530651 and Wesolowska Janczarek (1984) doi:10.1002/bimj.4710260613.
It is available on CRAN and my github page:
Link to the MMeM github
Link to the MMeM CRAN
The package supports the variance covariance component estimations for the multivariate mixed effects model for one-way randomized block design with equal design matrices:</description>
    </item>
    
    <item>
      <title>CS224n/assignment2/: Dependency Parsing Using Deep Learning NN Model</title>
      <link>/post/dependency-parsing/</link>
      <pubDate>Mon, 05 Nov 2018 16:01:23 +0800</pubDate>
      
      <guid>/post/dependency-parsing/</guid>
      <description>This project extends Neural Transition-based dependeny Parsing (Stanford U cs224n A#2 Q2). The goal is to build a three layer neural network using TensorFlow to parse the dependency structure of sentences. The orignal code contributed by hankcs is built in Python2. I revised it so it runs in Python3. The training data for the neural dependency parsing model is Penn Treebank, and each sentence has &amp;lsquo;word&amp;rsquo;, &amp;lsquo;POS&amp;rsquo;, &amp;lsquo;head&amp;rsquo; and &amp;lsquo;label&amp;rsquo;,</description>
    </item>
    
  </channel>
</rss>