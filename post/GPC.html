

<p>In binary classification, a response function (or a link function) is used to turn the output of a regression model into class probability, so the domain of the output from a regression model is squashed into [0, 1] range. Common link function is logistic function (another one is probit function, which will not be discussed here). For a data point <span class="math inline">\((x_i, c_i)\)</span>,</p>
<p><span class="math display">\[
p(c_i|x_i, w) = \frac{1}{1+exp(-x_i&#39;w)} = \sigma(x_i&#39;w)
\]</span></p>
<p>Encode class <span class="math inline">\(c_i\)</span> as <span class="math inline">\(+1\)</span> or <span class="math inline">\(-1\)</span>, Equation  can be written as</p>
<p><span class="math display">\[
p(c_i|x_i, w) = p(c_i|f_i)=\sigma(c_if_i), f_i = x_i&#39;w.
\]</span></p>
<p>Here, <span class="math inline">\(f\)</span> is the latent function <span class="math inline">\(f(x)\)</span> that we do not observe it, it is also stochastic, so we assume the priors for <span class="math inline">\(f\)</span> and weights <span class="math inline">\(w\)</span> as <span class="math inline">\(f\sim MVN(0, \Sigma_f)\)</span> and <span class="math inline">\(w \sim MVN(0, \Sigma_w)\)</span>, then given a dataset <span class="math inline">\(D=\left\{(x_i, c_i), i=1, \dots, n\right\}\)</span> and new data points <span class="math inline">\((x^*, c^*)\)</span>, the posterior conditional distribution for predicting <span class="math inline">\(f^*\)</span> for <span class="math inline">\((x^*, c^*)\)</span> is</p>
<p><span class="math display">\[
p(f^*|D, x^*) = \int p(f^*|x, x^*, f)p(f|D)df, \mbox{ where } p(f|D)\propto p(c|f)p(f|x).
\]</span></p>
<!-- $$ -->
<!-- logp(w|x, c) \propto log p (c|x, w)+log p(w)\\ -->
<!-- \hspace{3.5cm}= const-\frac{1}{2}w'\Sigma^{-1}w+\Sigma_{i=1}^{n} log\sigma\left(c_if_i\right) -->
<!-- $$ -->
<!-- then the posterior distribution of $c$ is  -->
<p>Since <span class="math inline">\(p(f|D)\)</span> is not Gaussian, then we approximate it by <span class="math display">\[q(f|D) = MVN(\hat{f}, (K^{-1}+W)^{-1}), \mbox{ where }(K^{-1}+W) \mbox{ is the hessian matrix of }-log p(f|D),\]</span></p>
<p><span class="math inline">\(\hat{f}\)</span> is the posterior mode that maximizes <span class="math inline">\(p(f|D)\)</span> and is computed iteratively using Newton’s method.</p>
<p>With <span class="math inline">\(q(f|D)\)</span>, <span class="math inline">\(p(f^*|D, x^*)\)</span> above replaced by <span class="math inline">\(q(f^*|D, x^*)\)</span> with mean and variance in [1], pg44.</p>
<p>Given dataset <span class="math inline">\(D\)</span> and new data points <span class="math inline">\((c^*, x^*)\)</span>, the posterior predictive probability is</p>
<p><span class="math display">\[
p(c^*=+1|D, x^*) = \int \sigma(f^*)q(f^*|D, x^*) df^*
\]</span>
Approximate this integral using Gaussian integral of an error function <a href="http:\blitiri.blogspot.de/2012/11/gaussian-integral-of-error-function.html">link</a>.</p>
<p>For tensorflow implementation, check out my repo <a href="https://github.com/pengluyaoyao/Gaussian-Process-Classification-Using-Tensorflow-probability">GPC-tensorflow</a>. Also check Tensorflow-probability <a href="https://www.tensorflow.org/probability">Tensorflow-probability</a>, <a href="https://github.com/tensorflow/probability">Tensorflow-probability repo</a></p>
<p>[1] Williams, Christopher KI, and Carl Edward Rasmussen. Gaussian processes for machine learning. Vol. 2. No. 3. Cambridge, MA: MIT press, 2006.</p>
