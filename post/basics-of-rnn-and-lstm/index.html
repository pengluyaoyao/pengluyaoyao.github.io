<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Basics of RNN and LSTM - Luyao Peng&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Luyao Peng" /><meta name="description" content="RNN The language model computes the probability of a sequence of previous words, \(P(word_1, word_2, \dots, word_t)\). The traditional language model is based on naive bayes model, the probability of a sequence of words is:
\[ P(word_1, word_2, \dots, word_{t}) = \prod_{i = 1}^{t} P(word_i|word_1, \dots, word_{i-1}). \]
The cons of traditional language models assumes independence among words (this is the naive part), which is not true in reality, it also requires a lot of RAM to compute the conditional probabilities for each word." /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.54.0 with even 4.0.0" />


<link rel="canonical" href="/post/basics-of-rnn-and-lstm/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">


<link href="/dist/even.c2a46f00.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Basics of RNN and LSTM" />
<meta property="og:description" content="RNN The language model computes the probability of a sequence of previous words, \(P(word_1, word_2, \dots, word_t)\). The traditional language model is based on naive bayes model, the probability of a sequence of words is:
\[ P(word_1, word_2, \dots, word_{t}) = \prod_{i = 1}^{t} P(word_i|word_1, \dots, word_{i-1}). \]
The cons of traditional language models assumes independence among words (this is the naive part), which is not true in reality, it also requires a lot of RAM to compute the conditional probabilities for each word." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/basics-of-rnn-and-lstm/" />
<meta property="article:published_time" content="2019-04-24T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2019-04-24T00:00:00&#43;00:00"/>

<meta itemprop="name" content="Basics of RNN and LSTM">
<meta itemprop="description" content="RNN The language model computes the probability of a sequence of previous words, \(P(word_1, word_2, \dots, word_t)\). The traditional language model is based on naive bayes model, the probability of a sequence of words is:
\[ P(word_1, word_2, \dots, word_{t}) = \prod_{i = 1}^{t} P(word_i|word_1, \dots, word_{i-1}). \]
The cons of traditional language models assumes independence among words (this is the naive part), which is not true in reality, it also requires a lot of RAM to compute the conditional probabilities for each word.">


<meta itemprop="datePublished" content="2019-04-24T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2019-04-24T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="1973">



<meta itemprop="keywords" content="Machine Learning," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Basics of RNN and LSTM"/>
<meta name="twitter:description" content="RNN The language model computes the probability of a sequence of previous words, \(P(word_1, word_2, \dots, word_t)\). The traditional language model is based on naive bayes model, the probability of a sequence of words is:
\[ P(word_1, word_2, \dots, word_{t}) = \prod_{i = 1}^{t} P(word_i|word_1, \dots, word_{i-1}). \]
The cons of traditional language models assumes independence among words (this is the naive part), which is not true in reality, it also requires a lot of RAM to compute the conditional probabilities for each word."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">LP&#39;s NLP Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">LP&#39;s NLP Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<script async type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Basics of RNN and LSTM</h1>

      <div class="post-meta">
        <span class="post-time"> 2019-04-24 </span>
        <div class="post-category">
            <a href="/categories/deep-learning/"> Deep Learning </a>
            </div>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    
  </div>
</div>
    <div class="post-content">
      
<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/viz/viz.js"></script>
<link href="/rmarkdown-libs/DiagrammeR-styles/styles.css" rel="stylesheet" />
<script src="/rmarkdown-libs/grViz-binding/grViz.js"></script>


<div id="rnn" class="section level1">
<h1>RNN</h1>
<p>The language model computes the probability of a sequence of previous words, <span class="math inline">\(P(word_1, word_2, \dots, word_t)\)</span>. The traditional language model is based on naive bayes model, the probability of a sequence of words is:</p>
<p><span class="math display">\[ P(word_1, word_2, \dots, word_{t}) = \prod_{i = 1}^{t} P(word_i|word_1, \dots, word_{i-1}). \]</span></p>
<p>The cons of traditional language models assumes independence among words (this is the naive part), which is not true in reality, it also requires a lot of RAM to compute the conditional probabilities for each word. One solution is to apply RNNs, which does not consider each input word independently, instead, RNNs can capture the information in previous sequence of words and evaluate their effects on the prediction on the current word (recurrent part).</p>
<div id="the-model" class="section level2">
<h2>The Model</h2>
<!-- <figure> -->
<!--   <img src="/post/2019-04-20-RNN_files/figure-html/rnn.jpg" alt="RNN" width=58% height=60%/> -->
<!--   <figcaption>RNNs, source: Nature</figcaption> -->
<!-- </figure> -->
<div id="htmlwidget-1" style="width:672px;height:480px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"diagram":"\ndigraph a_nice_graph {\n\ngraph [rankdir=TB; splines=\"line\"; esep = 1; forcelabels=true;]\n\n\nsubgraph output{\nrank = same; z1 [label = <y&#770;<FONT POINT-SIZE=\"8\"><SUB>t-1<\/SUB><\/FONT>>, shape = circle, fixedsize=true];\nz2 [label = <y&#770;<FONT POINT-SIZE=\"8\"><SUB>t<\/SUB><\/FONT>>, shape = circle, fixedsize=true];\nz3 [label = <y&#770;<FONT POINT-SIZE=\"8\"><SUB>t+1<\/SUB><\/FONT>>, shape = circle, fixedsize=true];\n}\n\nsubgraph hidden{\nrank = same; h0 [style = invis];h1 [label = <h&#770;<FONT POINT-SIZE=\"8\"><SUB>t-1<\/SUB><\/FONT>>, shape = circle, fixedsize=true];\nh2 [label = <h&#770;<FONT POINT-SIZE=\"8\"><SUB>t<\/SUB><\/FONT>>, shape = circle, fixedsize=true];\nh3 [label = <h&#770;<FONT POINT-SIZE=\"8\"><SUB>t+1<\/SUB><\/FONT>>, shape = circle, fixedsize=true];\nh4 [style= invis];\n}\n\n\nsubgraph input{\nrank = same; x1 [label = <x&#770;<FONT POINT-SIZE=\"8\"><SUB>t-1<\/SUB><\/FONT>>, shape = plaintext, fixedsize=true];\nx2 [label = <x&#770;<FONT POINT-SIZE=\"8\"><SUB>t<\/SUB><\/FONT>>, shape = plaintext, fixedsize=true];\nx3 [label = <x&#770;<FONT POINT-SIZE=\"8\"><SUB>t+1<\/SUB><\/FONT>>, shape = plaintext, fixedsize=true];\n\n}\n\n{\nrank = same;  edge [ style = invis ]; z1->z2->z3\n}\n{\nrank = same;  edge [ style = invis ]; h0->h1->h2->h3->h4 \n}\n{\nrank = same;  edge [ style = invis ]; x1->x2->x3\n}\nedge [ color=\"black\" ]; h0-> h1 [xlabel = \"W\" ] ; h3->h4;  h1 -> h2 [xlabel = \"W\" ]; h1-> z1 [xlabel = \"V\" ]; h2-> h3[xlabel = \"W\" ] ; h2-> z2 [xlabel = \"V\" ]; h3 -> z3 [xlabel = \"V\" ]; x1->h1 [xlabel = \"U\" ]; x2->h2[xlabel = \"U\" ] ; x3->h3 [xlabel = \"U\" ]; edge [ color=\"red\" ]; h1->{h0 x1}; z1-> h1; z2->h2; z3->h3; h2 -> {h1 x2}; h3-> {h2 x3}; h4-> h3; edge [ style = invis ]; z1-> {h0 h1 h2 h3 h4};  h1 -> {x1 x2 x3};\n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p>Define</p>
<p><span class="math display">\[\mathbf{x}_t \in \mathbb{R}^{d}, \mathbf{s}_t \in \mathbb{R}^{D_s}, \mathbf{W} \in \mathbb{R}^{D_s \times D_s}, \mathbf{U} \in \mathbb{R}^{D_s \times d}, \mathbf{V} \in \mathbb{R}^{Voc \times D_s}, \mathbf{\hat{y}} \in \mathbb{R}^{Voc},\]</span></p>
<p>Converting the figure above into math expressions: <span class="math display">\[
\begin{array}{rcl}
\mathbf{s}_t &amp;=&amp; f\left(\mathbf{W}\mathbf{s}_{t-1} + \mathbf{U}\mathbf{x}_{t}\right)\\
\mathbf{\hat{y}}_t &amp;=&amp; softmax\left(\mathbf{V}\mathbf{s_t}\right),  t = 1,2,\dots, T\\
\hat{p}(w_{t+1}&amp;=&amp; v_j|w_t, \dots,w_1) = \hat{y}_{t,j}, j = 1, 2, \dots, Voc
\end{array}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\mathbf{s}_t\)</span> is the hidden state at the time <span class="math inline">\(t\)</span>, it is a function (<span class="math inline">\(f\)</span> can be tanh, sigmoid, ReLU) of the previous hidden state <span class="math inline">\(\mathbf{s}_{t-1}\)</span> and word vector of the current input word <span class="math inline">\(\mathbf{x}_t\)</span></li>
<li><span class="math inline">\(\hat{\mathbf{y}}_t\)</span> is the predicted outcome (a vector of probabilities over all vocabulary) at time <span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(\hat{p}\)</span> is the probability of the predicted word <span class="math inline">\(w_{t+1}\)</span> (at time <span class="math inline">\(t+1\)</span>) is equal to the <span class="math inline">\(j\)</span>th word in the vocabulary <span class="math inline">\(v_j\)</span> given a sequence of all previous input words, and <span class="math inline">\(\hat{p}(w_{t+1}= v_j|w_t, \dots,w_1)\)</span> is equal to the <span class="math inline">\(j\)</span>th element in <span class="math inline">\(\hat{\mathbf{y}}_t\)</span>.</li>
</ul>
<p>At each time point, the <span class="math inline">\(\mathbf{s}_{t}\)</span> is a function of <span class="math inline">\(\mathbf{s}_{t-1}\)</span> and <span class="math inline">\(\mathbf{s}_{t-1}\)</span> is also a function of <span class="math inline">\(\mathbf{s}_{t-2}\)</span> etc., so the effects of all the previous input words will be taken into account when predicting the word at <span class="math inline">\(t+1\)</span>, this is the recurrent part in RNN.</p>
</div>
<div id="the-cost-fucntion-of-rnn" class="section level2">
<h2>The Cost Fucntion of RNN</h2>
<p>The loss function at time <span class="math inline">\(t\)</span> for the RNN model above is</p>
<p><span class="math display">\[E_{t}(\hat{y}_{t}, y_{t}) = -\sum_{j=1}^{Voc}y_{t,j}log(\hat{y}_{t,j}), j = 1, 2, \dots, Voc,\]</span></p>
<p><span class="math display">\[E(\hat{y}, y)= -\frac{1}{T}\sum_{t=1}^{T}E_{t}(\theta),\]</span></p>
<p>where <span class="math inline">\(y_{t,j}\)</span> is the correct word at time <span class="math inline">\(t\)</span> located at the <span class="math inline">\(j\)</span>th element in <span class="math inline">\(\hat{\mathbf{y}}_t\)</span>, the loss function <span class="math inline">\(J\)</span> tries to minimize the difference between <span class="math inline">\(y_{t,j}\)</span> and <span class="math inline">\(\hat{y}_{t,j}\)</span> over all word <span class="math inline">\(j\)</span> and time <span class="math inline">\(t\)</span>.</p>
</div>
<div id="back-propagation-of-rnn" class="section level2">
<h2>Back Propagation of RNN</h2>
<p>Our goal is to compute the gradient of total cost (the cost function is the loss <span class="math inline">\(E\)</span> across all t, loss function is the <span class="math inline">\(E_t\)</span> at time t) <span class="math inline">\(E(\hat{y}, y)\)</span> wrt <span class="math inline">\(\mathbf{W}, \mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}\)</span>. From the figure above, the gradient wrt <span class="math inline">\(\mathbf{V}\)</span> only involves the values at current time <span class="math inline">\(t\)</span>, while the gradients wrt <span class="math inline">\(\mathbf{W}\)</span> and <span class="math inline">\(\mathbf{U}\)</span> involves the values at the current time <span class="math inline">\(t\)</span> and at all the previous time.</p>
<p>Notice <span class="math inline">\(E(\hat{y}, y)\)</span> is the sum of <span class="math inline">\(E_{t}(\hat{y}_{t}, y_{t})\)</span>, each of which involves <span class="math inline">\(\mathbf{W}, \mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}\)</span>, so we have</p>
<p><span class="math display">\[\frac{\partial{E}}{\partial{\mathbf{W}}} = \sum_{t=1}^{T}\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{W}}}}, \frac{\partial{E}}{\partial{\mathbf{U}}} = \sum_{t=1}^{T}\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{U}}}}, \frac{\partial{E}}{\partial{\mathbf{V}}} = \sum_{t=1}^{T}\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{V}}}}\]</span> Define <span class="math inline">\(\mathbf{z}_t = \mathbf{Vs}_t\)</span>, <span class="math inline">\(\mathbf{z}_t \in \mathbb{R}^{Voc}\)</span>, recall <span class="math inline">\(\frac{\partial{E}_t}{\partial{\mathbf{z}_t}} = \hat{\mathbf{y}_t}-\mathbf{y}_t = \mathbf{\delta}_{1t}\)</span>, then</p>
<p><span class="math display">\[\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{V}}}} = \mathbf{\delta}_{1t} \mathbf{s}_t&#39;, \mathbf{\delta}_{1t} \in \mathbb{R}^{Voc}, \mathbf{s}_t \in \mathbb{R}^{Ds}, \mathbf{V} \in \mathbb{R}^{Voc\times D_s}\]</span>, the gradient wrt <span class="math inline">\(\mathbf{V}\)</span> at time <span class="math inline">\(t\)</span> is only dependent on the values at the current time point, <span class="math inline">\(\hat{\mathbf{y}}_t, \mathbf{y}_t, \mathbf{s}_t\)</span>.</p>
<p>Assuming the <span class="math inline">\(f\)</span> function is tanh function, <span class="math display">\[\begin{array}{rcl}\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{W}}}} &amp;=&amp; \frac{\partial{E}_t}{\partial{\mathbf{s}_t}}\frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{h}_t}}\frac{\partial{\mathbf{h}}_t}{\partial{\mathbf{W}}}\\
&amp;=&amp; \frac{\partial{E}_t}{\partial{\mathbf{s}_t}} f&#39;(\mathbf{h}_t)\left[\mathbf{s}_{t-1}+\mathbf{W}\frac{\partial{\mathbf{h}_t}}{\partial{\mathbf{W}}}\right]\\
&amp;=&amp; \frac{\partial{E}_t}{\partial{\mathbf{s}_t}} \left[f&#39;\mathbf{s}_{t-1}+f&#39;\mathbf{W} \frac{\partial{\mathbf{h}_t}}{\partial{\mathbf{W}}}\right]\\
&amp;=&amp; \frac{\partial{E}_t}{\partial{\mathbf{s}_t}} \frac{\partial{\mathbf{s}_t}}{\partial{\mathbf{W}}} + \frac{\partial{E}_t}{\partial{\mathbf{s}_t}} \frac{\partial{s}_t}{\partial{s}_{t-1}}\frac{\partial{s}_{t-1}}{\partial{\mathbf{W}}}\end{array},\]</span> where <span class="math inline">\(\mathbf{h}_t = \mathbf{W}\mathbf{s}_{t-1} +\mathbf{U}\mathbf{x}_t\)</span>.</p>
<p>Since <span class="math inline">\(\frac{\partial{s}_{t-1}}{\partial{\mathbf{W}}} = \frac{\partial{\mathbf{s}_{t-1}}}{\partial{\mathbf{W}}} + \frac{\partial{s}_{t-1}}{\partial{s}_{t-2}}\frac{\partial{s}_{t-2}}{\partial{\mathbf{W}}}\)</span>, plug it into above equation, we have</p>
<p><span class="math display">\[\begin{array}{rcl} \frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{W}}}} &amp;=&amp; \frac{\partial{E}_t}{\partial{\mathbf{s}_t}} \frac{\partial{\mathbf{s}_t}}{\partial{\mathbf{W}}} + \frac{\partial{E}_t}{\partial{\mathbf{s}_t}} \frac{\partial{s}_t}{\partial{s}_{t-1}}\frac{\partial{s}_{t-1}}{\partial{\mathbf{W}}} + \frac{\partial{E}_t}{\partial{\mathbf{s}_t}} \frac{\partial{s}_t}{\partial{s}_{t-2}}\frac{\partial{s}_{t-2}}{\partial{\mathbf{W}}} + \dots + \frac{\partial{E}_t}{\partial{\mathbf{s}_t}} \frac{\partial{s}_t}{\partial{s}_{1}}\frac{\partial{s}_{1}}{\partial{\mathbf{W}}}\\
&amp;=&amp; \sum_{t=1}^{T}\sum_{k=1}^{t}\frac{\partial{E}_t}{\partial{\mathbf{s}_t}}\frac{\partial{s}_t}{\partial{s}_{t-k}}\frac{\partial{s}_{t-k}}{\partial{\mathbf{W}}}\end{array}, \mathbf{W} \in \mathbb{R}^{D_s\times D_s}\]</span>.</p>
<p>Similarly, the gradient of <span class="math inline">\(E_t\)</span> wrt <span class="math inline">\(\mathbf{U}\)</span> is</p>
<p><span class="math display">\[
\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{U}}}} = \frac{\partial{E}_t}{\partial{\mathbf{s}_t}}\frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{U}}}
\]</span> In <span class="math inline">\(\mathbf{s}_t\)</span>, <span class="math inline">\(\mathbf{U}\)</span> is involved in <span class="math inline">\(\mathbf{s}_{t-1}\)</span> and <span class="math inline">\(\mathbf{Ux}_t\)</span>, so</p>
<p><span class="math display">\[
\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{U}}}} =  \frac{\partial{E}_t}{\partial{\mathbf{s}_t}}\left(\frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{U}}} + \frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{s}_{t-1}}} \frac{\partial{\mathbf{s}}_{t-1}}{\partial{\mathbf{U}}}\right)
\]</span> since <span class="math inline">\(\frac{\partial{\mathbf{s}}_{t-1}}{\partial{\mathbf{U}}} = \left(\frac{\partial{\mathbf{s}}_{t-1}}{\partial{\mathbf{U}}} + \frac{\partial{\mathbf{s}}_{t-1}}{\partial{\mathbf{s}_{t-2}}} \frac{\partial{\mathbf{s}}_{t-2}}{\partial{\mathbf{U}}}\right)\)</span>, plug it into the above equation, we have</p>
<p><span class="math display">\[
\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{U}}}} =  \frac{\partial{E}_t}{\partial{\mathbf{s}_t}}\left(\frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{U}}} + \frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{s}_{t-1}}} \left(\frac{\partial{\mathbf{s}}_{t-1}}{\partial{\mathbf{U}}} + \frac{\partial{\mathbf{s}}_{t-1}}{\partial{\mathbf{s}_{t-2}}} \frac{\partial{\mathbf{s}}_{t-2}}{\partial{\mathbf{U}}}\right)\right)
\]</span></p>
<p>continue this plug-in procedure until 0 time point, finally we have</p>
<p><span class="math display">\[
\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{U}}}} =  \sum_{t=1}^{T}\sum_{k=1}^{t}\frac{\partial{E}_t}{\partial{\mathbf{s}_t}} \frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{s}_{t-k}}}\frac{\partial{s}_{t-k}}{\partial{\mathbf{U}}}
\]</span> Notice that the gradient wrt <span class="math inline">\(\mathbf{W}\)</span> and <span class="math inline">\(\mathbf{U}\)</span> at time <span class="math inline">\(t\)</span> is dependent on both current and all previous values.</p>
<blockquote>
<p>PRACTICE: Based on a sketch of network at a single timestep: <div id="htmlwidget-2" style="width:672px;height:480px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"diagram":"\ndigraph a_nice_graph {\ngraph [rankdir=BT; splines=\"line\"; esep = 1; forcelabels=true;]\nsubgraph output{\nz [label = <y&#770;<FONT POINT-SIZE=\"8\"><SUB>t<\/SUB><\/FONT>>, shape = circle, fixedsize=true];\n}\nsubgraph hidden{\nrank = same; h1 [label = <h&#770;<FONT POINT-SIZE=\"8\"><SUB>t-1<\/SUB><\/FONT>>, shape = circle, fixedsize=true];\nh2 [label = <h&#770;<FONT POINT-SIZE=\"8\"><SUB>t<\/SUB><\/FONT>>, shape = circle, fixedsize=true];\nh3 [label = <h&#770;<FONT POINT-SIZE=\"8\"><SUB>t+1<\/SUB><\/FONT>>, shape = circle, fixedsize=true];\n}\nsubgraph input{\nx2 [label = <x&#770;<FONT POINT-SIZE=\"8\"><SUB>t<\/SUB><\/FONT>>, shape = circle, fixedsize=true];\n}\n{\nrank = same; h1->h2->h3\n}\nedge [ color=\"black\" ]; h2->z; x2->h2\n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script> Consider feed-forward $$ Draw the ‘unrolled’ network for the previous 3 timesteps (t-1, t-2, t-3) and compute <span class="math inline">\(\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{U}}}}\bigg\rvert_{t-1}=\)</span>, <span class="math inline">\(\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{W}}}}\bigg\rvert_{t-1}=\)</span>, <span class="math inline">\(\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{V}}}}=\)</span>, <span class="math inline">\(\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{x}_t}}}=\)</span>, the complexity of back propagation wrt the model parameters across the entire T timesteps, <span class="math inline">\(\sum_{t=1}^T J^{(t)}\)</span>, is <span class="math inline">\(O\left[T(VD_h + dD_h + t(D_h^2 + dD_h))\right]\)</span>, the costy term is <span class="math inline">\(VD_h\)</span>, the dimension of the whole vocabulary, since <span class="math inline">\(V&gt;&gt;D_h\)</span>,</p>
</blockquote>
</div>
<div id="the-vanishing-gradient-problem" class="section level2">
<h2>The Vanishing Gradient Problem</h2>
<p>If we look deeper at the <span class="math inline">\(\frac{\partial{s}_t}{\partial{s}_{k}}\)</span> part in <span class="math inline">\(\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{W}}}} = \sum_{k=1}^{t}\frac{\partial{E}_t}{\partial{\mathbf{s}_t}}\frac{\partial{s}_t}{\partial{s}_{k}}\frac{\partial{s}_{k}}{\partial{\mathbf{W}}}\)</span>, <span class="math inline">\(\frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{s}}_{k}} = \frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{s}}_{t-1}}\frac{\partial{\mathbf{s}}_{t-1}}{\partial{\mathbf{s}}_{t-2}}\dots\frac{\partial{\mathbf{s}}_{t-k+1}}{\partial{\mathbf{s}}_{t-k}}\)</span>, each partial derivative is a <span class="math inline">\(D_s \times D_s\)</span> Jacobian matrix, for example,</p>
<p><span class="math display">\[\frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{s}}_{t-1}} = \begin{bmatrix}\frac{\partial{s}_{t_1}}{\partial{s}_{t-1_1}} &amp; \dots &amp; \frac{\partial{s}_{t_{D_s}}}{\partial{s}_{t-1_1}} \\ \vdots &amp; \ddots &amp; \vdots\\ \frac{\partial{s}_{t_1}}{\partial{s}_{t-1_{D_s}}} &amp; \dots &amp; \frac{\partial{s}_{t_{D_s}}}{\partial{s}_{t-1_{D_s}}}\end{bmatrix}\]</span> if the function <span class="math inline">\(f\)</span> is a sigmoid or tanh function, the gradient at the end sides are almost 0, so the elements in <span class="math inline">\(\frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{s}}_{t-1}}\)</span> will become small (saturated neurons) if the corresponding values in the input word vector have either high positive or low negative values, also the gradient matrix will multiply with each other <span class="math inline">\(k\)</span> times, the gradient contribution of the saturated neurons becomes smaller and smaller and eventually vanishes. On the other hand, if the function <span class="math inline">\(f\)</span> is ReLU function and the gradient is greater than 1 if the input values of the word vector are positive, then the contribution of the gradient will be explode after k times multiplications. Clipping tricks work for exploding problems, but not vanishing problems, because we are artificially inflating the influence of an input word ‘far away’ from time <span class="math inline">\(t\)</span> to prevent it from vanishing, which is not realistic (‘far away’ word hardly has influence in predicting the current word).</p>
</div>
<div id="lstm" class="section level2">
<h2>LSTM</h2>
<div id="the-model-1" class="section level3">
<h3>The Model</h3>
<p>LSTM is one of the solutions to the vanishing gradient problems in RNN. LSTM operations are, at time <span class="math inline">\(t\)</span>,</p>
<div id="htmlwidget-3" style="width:672px;height:480px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-3">{"x":{"diagram":"\ndigraph a_nice_graph {\nrankdir=BT;\n\nsubgraph {\nrank = same\nx1 [label = <x<FONT POINT-SIZE=\"8\"><SUB>t<\/SUB><\/FONT>>, shape=circle,fixedsize=true]\nx2 [style = invis]\nx3 [label = <x<FONT POINT-SIZE=\"8\"><SUB>t+1<\/SUB><\/FONT>>, shape = circle, fixedsize = true]\nx1->x2 [style = invis, minlen = 14]; x2->x3 [style = invis, minlen =2]\n}\n\nsubgraph {\nrank = same\na1 [label = <out<FONT POINT-SIZE=\"8\"><SUB>t-1<\/SUB><\/FONT>>, shape=circle,fixedsize=true]\na2 [ shape=point,fixedsize=true]\na3 [ shape=point,fixedsize=true]\na4 [ shape=point,fixedsize=true]\na5 [ shape=point,fixedsize=true]\na6 [ shape=point,fixedsize=true]\n#a7 [style = invis]\na8 [label = <out<FONT POINT-SIZE=\"8\"><SUB>t<\/SUB><\/FONT>>, shape=circle,fixedsize=true]\na9 [style = invis]\na1->a2 [minlen=0.5]; a2->a3 [arrowhead=none, minlen = 1]; a3->a4 [arrowhead=none, minlen=4]; a4->a5->a6 [arrowhead=none]; edge [style = invis] a6->a8; edge [style = dotted, minlen = 2] a8->a9; a9->a8 [color = \"red\"]\n}\n\nsubgraph {\nrank = same\nb1 [label = <&sigma;>, shape = box]\nb2 [label = \"tanh\", shape = box]\nb3 [label = <&sigma;>, shape = box]\nb4 [label = <&sigma;>, shape = box]\nedge [style = invis] b1->b2->b3->b4\n}\n\nsubgraph {\nrank = same\nc0 [label = \"x\", shape = box, fixedsize = true]\nc1 [label = <f<FONT POINT-SIZE=\"8\"><SUB>t<\/SUB><\/FONT>>, shape = circle,fixedsize=true]\nc2 [label = <a<FONT POINT-SIZE=\"8\"><SUB>t<\/SUB><\/FONT>>, shape = circle,fixedsize=true]\nc3 [label = <i<FONT POINT-SIZE=\"8\"><SUB>t<\/SUB><\/FONT>>, shape = circle,fixedsize=true]\nc4 [label = <o<FONT POINT-SIZE=\"8\"><SUB>t<\/SUB><\/FONT>>, shape = circle,fixedsize=true]\nc5 [label = \"x\", shape = box,fixedsize=true]\nc4->c5 [minlen=1]; edge [style = invis]c0->c1->c2->c3->c4\n}\n\nsubgraph{\nrank = same\nd0 [label=\"tanh\", shape = box]\nd1 [label = \"x\", shape = box]\nd3 [style = invis]\nd2 [label = \"tanh\", shape = box]\nedge [style = invis] d0->d1->d3->d2\n}\n\nsubgraph{\nrank = same\ne1 [label = <state<FONT POINT-SIZE=\"8\"><SUB>t-1<\/SUB><\/FONT>>, shape = circle, fixedsize= true]\ne2 [label = \"x\", shape = box, fixedsize= true]\ne3 [label = \"+\", shape = box, fixedsize= true]\ne4 [label = <state<FONT POINT-SIZE=\"8\"><SUB>t<\/SUB><\/FONT>>, shape = circle, fixedsize= true]\ne5 [style = invis]\ne1->e2 [minlen = 1]; e2->e3 [minlen = 4]; e3->e4 [minlen=5.2]; e4->e5 [style = dotted, minlen = 2]; e5->e4 [style = dotted, minlen = 2, color = \"blue\"]\n}\nx1->a2; a3->b1; a4->b2; a5->b3; a6->b4; b1->c1; b2->c2; b3->c3; b4->c4; c2->d1; c3->d1; d2->c5;d0->c0; c0->a1;  x3->a9 [style = dotted]; x2->a8 [color = red];  c5->a8; c5->d2 [color =\"blue\"]; a8->c5 [color =\"blue\"]; c1->e2; d1->e3;  edge [ dir = none, color= \"blue\"] d2->e4; edge [ dir = none, color = \"black\" ] d2->e4; d0->e1 [arrowhead = none];\n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<ul>
<li>activation: the new memory cell, controlling whether current input <span class="math inline">\(\mathbf{x}_t\)</span> matters or not, <span class="math display">\[\mathbf{a}_t = tanh(\mathbf{W}_a\mathbf{x}_t+\mathbf{U}_a\mathbf{out}_{t-1}+\mathbf{b}_a)\]</span></li>
<li>input gate: controlling whether <span class="math inline">\(\mathbf{a}_t\)</span> matters in <span class="math inline">\(\mathbf{state}_t\)</span> or not. If <span class="math inline">\(\mathbf{i}_t\)</span> is 1, <span class="math inline">\(\mathbf{a}_{t-1}\)</span> will be dirrectly copied at time <span class="math inline">\(t\)</span>, if <span class="math inline">\(\mathbf{i}_t\)</span> is 0, <span class="math inline">\(\mathbf{a}_{t-1}\)</span> will be completely ignored at time <span class="math inline">\(t\)</span>. <span class="math display">\[\mathbf{i}_t = \sigma(\mathbf{W}_i\mathbf{x}_t + \mathbf{U}_i \mathbf{out}_{t-1} + \mathbf{b}_i)\]</span></li>
<li>forget gate: controlling how much <span class="math inline">\(state_{t-1}\)</span> influence <span class="math inline">\(state_t\)</span> (how much past state should matter now), if <span class="math inline">\(\mathbf{f}_t\)</span> is 1, <span class="math inline">\(state_{t=1}\)</span> will be dirrectly copied at time <span class="math inline">\(t\)</span>, if <span class="math inline">\(\mathbf{f}_t\)</span> is 0, <span class="math inline">\(state_{t_1}\)</span> will be completely ignored at time <span class="math inline">\(t\)</span>,</li>
</ul>
<p><span class="math display">\[\mathbf{f}_t = \sigma(\mathbf{W}_f\mathbf{x}_t + \mathbf{U}_f \mathbf{out}_{t-1} + \mathbf{b}_f)\]</span></p>
<ul>
<li>output gate: controlling how much <span class="math inline">\(\mathbf{state}_t\)</span> is exposed in <span class="math inline">\(\mathbf{out}_t\)</span>,</li>
</ul>
<p><span class="math display">\[\mathbf{o}_t = \sigma(\mathbf{W}_o\mathbf{x}_t + \mathbf{U}_o \mathbf{out}_{t-1} + \mathbf{b}_o)\]</span> leading to:</p>
<ul>
<li>internal state:</li>
</ul>
<p><span class="math display">\[\mathbf{state}_t = \mathbf{a}_t \odot \mathbf{i}_t + \mathbf{f}_t\odot \mathbf{state}_{t-1}\]</span></p>
<ul>
<li>output at time <span class="math inline">\(t\)</span>:</li>
</ul>
<p><span class="math display">\[\mathbf{out}_t = tanh(\mathbf{state}_{t})\odot \mathbf{o}_t\]</span></p>
<p>Due to the elementwise product <span class="math inline">\(\odot\)</span> in <span class="math inline">\([\mathbf{f}_t\odot \mathbf{state}_{t-1}, \mathbf{a}_t \odot \mathbf{i}_t, tanh(\mathbf{state}_{t})\odot \mathbf{o}_t]\)</span>, in the back propagations wrt parameters (<span class="math inline">\(\mathbf{W}, \mathbf{U}, \mathbf{b}, \mathbf{x}\)</span> in activation, forget, input and output gate), <span class="math inline">\(\mathbf{state}_{t-1}, \mathbf{f}_t, \mathbf{a}_t, \mathbf{i}_t, \mathbf{o}_t, \mathbf{state}_{t}\)</span> will be directly copied (completely or partially depenending on the gates) to current time step rather than being involved in matrix multiplication , thus the vanishing gradient problems in RNN can be solved.</p>
</div>
<div id="the-back-propagation-of-lstm" class="section level3">
<h3>The Back Propagation of LSTM</h3>
<p>Let <span class="math inline">\(\mathbf{z}_o, \mathbf{z}_i, \mathbf{z}_f, \mathbf{z}_a\)</span> represents the <span class="math inline">\(\mathbf{W}\mathbf{x}_t + \mathbf{U} \mathbf{out}_{t-1} + \mathbf{b}\)</span> in <span class="math inline">\(\mathbf{a}_t, \mathbf{f}_t,\mathbf{i}_t, \mathbf{o}_t\)</span>, respectively, and let <span class="math inline">\(\frac{\partial{E}}{\partial{\mathbf{out}_t}}=(\mathbf{out}_t-\mathbf{y}_t) + (\mathbf{out}_{t+1}-\mathbf{y}_{t+1})\frac{\partial{\mathbf{out}_{t+1}}}{\partial{\mathbf{out}_t}} = \mathbf{\delta}_t + \mathbf{\delta}_{t+1}\frac{\partial{\mathbf{out}_{t+1}}}{\partial{\mathbf{out}_t}}\)</span> (the first term corresponds to the red solid line, the second term, the gradient at future time steps, corresponding to the red dotted line), then following the blue line in the flowchart,</p>
<p><span class="math display">\[\frac{\partial{E}}{\partial{\mathbf{s}_t}}  =\frac{\partial{E}}{\partial{\mathbf{out}_t}}\odot \mathbf{o}_t\odot[1-tanh^2(\mathbf{s}_t)] + \frac{\partial{E}}{\partial{\mathbf{s}_{t+1}}}\odot\mathbf{f}_{t+1},\]</span> where the first term corresponding to the blue solid back propagation line, the second term comes from the blue dotted line.</p>
<p>The gradient of <span class="math inline">\(\mathbf{s}_t\)</span> involves the gradient of future state <span class="math inline">\(\mathbf{s}_{t+1}\)</span>, when <span class="math inline">\(t\)</span> is the last time step, the second term in <span class="math inline">\(\frac{\partial{E}}{\partial{\mathbf{s}_t}}\)</span> will be <span class="math inline">\(\mathbf{0}\)</span>.</p>
<p>Next,</p>
<p><span class="math display">\[\frac{\partial{E}}{\partial{\mathbf{z}_{a_t}}} = \frac{\partial{E}}{\partial{\mathbf{s}_t}}\odot\mathbf{i}_t \odot[1-tanh^2(\mathbf{z}_{a_t})],\]</span> <span class="math display">\[\frac{\partial{E}}{\partial{\mathbf{z}_{o_t}}}=\frac{\partial{E}}{\partial{\mathbf{out}_t}}\odot tanh(\mathbf{s}_t)\odot \sigma_o\odot(1-\sigma_o),\]</span> <span class="math display">\[\frac{\partial{E}}{\partial{\mathbf{z}_{f_t}}} =  \frac{\partial{E}}{\partial{\mathbf{s}_t}}\odot\mathbf{s}_{t-1}\odot \sigma_f\odot(1-\sigma_f),\]</span> <span class="math display">\[\frac{\partial{E}}{\partial{\mathbf{z}_{i_t}}} = \frac{\partial{E}}{\partial{\mathbf{s}_t}}\odot\mathbf{a}_t \odot \sigma_i\odot(1-\sigma)_i.\]</span></p>
<p>Notice, <span class="math inline">\(\frac{\partial{E}}{\partial{\mathbf{out}_t}}\)</span> involves <span class="math inline">\(\mathbf{\delta}\)</span> at time <span class="math inline">\(t\)</span> and gradient from future time steps, <span class="math inline">\(\frac{\partial{E}}{\partial{\mathbf{s}_t}}\)</span> also involves gradient wrt <span class="math inline">\(\mathbf{s}\)</span> at time <span class="math inline">\(t\)</span> and gradient at future time steps, because from the flowchart of LSTM, each gates at time <span class="math inline">\(t\)</span> will be involved in all the future time steps.</p>
</div>
</div>
</div>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">Luyao Peng</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        2019-04-24
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/machine-learning/">Machine Learning</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/even-preview/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Theme preview</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/stat209/">
            <span class="next-text nav-default">Machine Learning STAT209 Review</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  <div id="disqus_thread"></div>
  <script>
  (function() {
  var d = document, s = d.createElement('script');
  s.src = 'https://luyao-1.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:your@email.com" class="iconfont icon-email" title="email"></a>
      <a href="http://localhost:1313" class="iconfont icon-stack-overflow" title="stack-overflow"></a>
      <a href="http://localhost:1313" class="iconfont icon-twitter" title="twitter"></a>
      <a href="http://localhost:1313" class="iconfont icon-facebook" title="facebook"></a>
      <a href="http://localhost:1313" class="iconfont icon-linkedin" title="linkedin"></a>
      <a href="http://localhost:1313" class="iconfont icon-google" title="google"></a>
      <a href="http://localhost:1313" class="iconfont icon-github" title="github"></a>
      <a href="http://localhost:1313" class="iconfont icon-weibo" title="weibo"></a>
      <a href="http://localhost:1313" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="http://localhost:1313" class="iconfont icon-douban" title="douban"></a>
      <a href="http://localhost:1313" class="iconfont icon-pocket" title="pocket"></a>
      <a href="http://localhost:1313" class="iconfont icon-tumblr" title="tumblr"></a>
      <a href="http://localhost:1313" class="iconfont icon-instagram" title="instagram"></a>
      <a href="http://localhost:1313" class="iconfont icon-gitlab" title="gitlab"></a>
      <a href="http://localhost:1313" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  
  
  <span class="copyright-year">
    &copy; 
    2017 - 
    2019
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Luyao Peng</span>
  </span>
</div>


    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script><script>window.flowchartDiagramsOptions = { 'x': 0, 'y': 0, 'line-width': 3, 'line-length': 50, 'text-margin': 10, 'font-size': 14, 'font-color': 'black', 'line-color': 'black', 'element-color': 'black', 'fill': 'white', 'yes-text': 'yes', 'no-text': 'no', 'arrow-end': 'block', 'scale': 1, 'i-am-a-comment-1': 'Do not use //!', 'i-am-a-comment-2': 'style symbol types', 'symbols': { 'start': { 'font-color': 'red', 'element-color': 'green', 'fill': 'yellow' }, 'end': { 'class': 'end-element' } }, 'i-am-a-comment-3': 'even flowstate support ;-)', 'flowstate': { 'request': {'fill': 'blue'} } };</script><script src="https://cdn.jsdelivr.net/npm/raphael@2.2.7/raphael.min.js" integrity="sha256-67By+NpOtm9ka1R6xpUefeGOY8kWWHHRAKlvaTJ7ONI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/flowchart.js@1.8.0/release/flowchart.min.js" integrity="sha256-zNGWjubXoY6rb5MnmpBNefO0RgoVYfle9p0tvOQM+6k=" crossorigin="anonymous"></script>
<script type="text/javascript" src="/dist/even.26188efa.min.js"></script>








</body>
</html>
