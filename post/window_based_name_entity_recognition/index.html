<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>CS224n/assignment3/: Window-based Name Entity Recognition (Baseline Model) - Luyao Peng&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Luyao Peng" /><meta name="description" content="Introduction This assignment built 3 different models for named entity recognition (NER) task. For a given word in a context, we want to predict the name entity of the word in one of the following 5 categories:
 Person (PER): Organization (ORG): Location (LOC): Miscellaneous (MISC): Null (O): the word do not represent a named entity and most of the words fall into this categroy.  This is a 5-class classification problem, which implies a label vector of [PER, ORG, LOC, MISC, O]." /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.73.0 with even 4.0.0" />


<link rel="canonical" href="/post/window_based_name_entity_recognition/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">


<link href="/dist/even.c2a46f00.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="CS224n/assignment3/: Window-based Name Entity Recognition (Baseline Model)" />
<meta property="og:description" content="Introduction This assignment built 3 different models for named entity recognition (NER) task. For a given word in a context, we want to predict the name entity of the word in one of the following 5 categories:
 Person (PER): Organization (ORG): Location (LOC): Miscellaneous (MISC): Null (O): the word do not represent a named entity and most of the words fall into this categroy.  This is a 5-class classification problem, which implies a label vector of [PER, ORG, LOC, MISC, O]." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/window_based_name_entity_recognition/" />
<meta property="article:published_time" content="2019-05-06T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-05-06T00:00:00+00:00" />
<meta itemprop="name" content="CS224n/assignment3/: Window-based Name Entity Recognition (Baseline Model)">
<meta itemprop="description" content="Introduction This assignment built 3 different models for named entity recognition (NER) task. For a given word in a context, we want to predict the name entity of the word in one of the following 5 categories:
 Person (PER): Organization (ORG): Location (LOC): Miscellaneous (MISC): Null (O): the word do not represent a named entity and most of the words fall into this categroy.  This is a 5-class classification problem, which implies a label vector of [PER, ORG, LOC, MISC, O].">
<meta itemprop="datePublished" content="2019-05-06T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2019-05-06T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="2866">



<meta itemprop="keywords" content="Name Entity Recognition," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="CS224n/assignment3/: Window-based Name Entity Recognition (Baseline Model)"/>
<meta name="twitter:description" content="Introduction This assignment built 3 different models for named entity recognition (NER) task. For a given word in a context, we want to predict the name entity of the word in one of the following 5 categories:
 Person (PER): Organization (ORG): Location (LOC): Miscellaneous (MISC): Null (O): the word do not represent a named entity and most of the words fall into this categroy.  This is a 5-class classification problem, which implies a label vector of [PER, ORG, LOC, MISC, O]."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">LP&#39;s NLP Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/post/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">LP&#39;s NLP Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/post/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<script async type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">CS224n/assignment3/: Window-based Name Entity Recognition (Baseline Model)</h1>

      <div class="post-meta">
        <span class="post-time"> 2019-05-06 </span>
        <div class="post-category">
            <a href="/categories/deep-learning-and-nlp/"> Deep Learning and NLP </a>
            </div>
        
      </div>
    </header>

    
    <div class="post-content">
      


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This assignment built 3 different models for named entity recognition (NER) task. For a given word in a context, we want to predict the name entity of the word in one of the following 5 categories:</p>
<ul>
<li>Person (PER):</li>
<li>Organization (ORG):</li>
<li>Location (LOC):</li>
<li>Miscellaneous (MISC):</li>
<li>Null (O): the word do not represent a named entity and most of the words fall into this categroy.</li>
</ul>
<p>This is a 5-class classification problem, which implies a label vector of [PER, ORG, LOC, MISC, O]. Example as follow:</p>
<p><img src="/post/Window_based_Name_Entity_Recognition_files/figure-html/fig1.png" alt="Luyao Peng" width=90% height=80%/></p>
</div>
<div id="window-based-model-of-ner-baseline-model" class="section level1">
<h1>Window-based Model of NER (Baseline Model)</h1>
<p>Let <span class="math inline">\(\mathbf{x} = [\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_T]\)</span> be a sentence of length <span class="math inline">\(T\)</span>, where <span class="math inline">\(\mathbf{x}_t, t=1, 2, \dots,T\)</span> is a one-hot vector of size of the vocabulary, representing the index of the word at position <span class="math inline">\(t\)</span>. To construct the windowed input on the raw input sentence <span class="math inline">\(\mathbf{x}\)</span>, given the window size <span class="math inline">\(w\)</span>, the windowed-input for the <span class="math inline">\(t\)</span>th word in <span class="math inline">\(\mathbf{x}\)</span> is <span class="math inline">\(\mathbf{x}^{t} = [\mathbf{x}_{t-w}, \dots, \mathbf{x}_{t}, \dots, \mathbf{x}_{t+w}]\)</span>. For the first word in <span class="math inline">\(\mathbf{x}\)</span>, the windowed-input is <span class="math inline">\(\mathbf{x}^{1} = [&lt;start&gt;,\dots, &lt;start&gt;, \mathbf{x}_{1}, \dots, \mathbf{x}_{1+w}]\)</span>, where the number of <span class="math inline">\(&lt;start&gt;\)</span> is <span class="math inline">\(w\)</span>. Similarly, the the last word in <span class="math inline">\(\mathbf{x}\)</span>, the windowed-input is <span class="math inline">\(\mathbf{x}^{T} = [\mathbf{x}_{T-w},\dots, \mathbf{x}_{T}, &lt;end&gt;, \dots. &lt;end&gt;]\)</span>, where the number of <span class="math inline">\(&lt;end&gt;\)</span> is <span class="math inline">\(w\)</span>. Each <span class="math inline">\(\mathbf{x}\)</span> corresponds to the lables <span class="math inline">\(\mathbf{y} = [\mathbf{y}^1, \mathbf{y}^2, \dots, \mathbf{y}^T]\)</span> of the same length <span class="math inline">\(T\)</span>, each <span class="math inline">\(\mathbf{y}^t\)</span> is also a one-hot vector. When constructing the windowed input from <span class="math inline">\(\mathbf{x}\)</span> for the word at <span class="math inline">\(t\)</span>, its corresponding label vector will be just <span class="math inline">\(\mathbf{y}^t\)</span> and the label of the word is at index <span class="math inline">\(i\)</span> in <span class="math inline">\(\mathbf{y}^t\)</span>, denoted by <span class="math inline">\(y_i^t\)</span>.</p>
<ul>
<li>Example:
<span class="math inline">\(\mathbf{x} = [\mbox{Jim}_1 \mbox{ bought}_2 \mbox{ 300}_3 \mbox{ shares}_3 \mbox{ of}_4 \mbox{ Acme}_5 \mbox{ Corp}_5 \mbox{ in}_6 \mbox{ 2006}_7.]\)</span>, where <span class="math inline">\(T = 7\)</span>. Let <span class="math inline">\(w= 1\)</span>, then</li>
</ul>
<p><span class="math display">\[\mathbf{x}^{1} = [&lt;start&gt;, \mbox{ Jim}, \mbox{ bought}], \mathbf{y}^1 = [1,0,0,0,0] \rightarrow{PER, \mbox{label of &#39;Jim&#39;}}\]</span>
<span class="math display">\[\dots\]</span>
<span class="math display">\[\mathbf{x}^{7} = [\mbox{in}, \mbox{ 2006}, &lt;end&gt;], \mathbf{y}^7 = [0,0,0,0,1] \rightarrow{O, \mbox{label of &#39;2006&#39;}}\]</span></p>
<ul>
<li>Model:
Using the windowed input <span class="math inline">\(\mathbf{x}^t\)</span>, want to predict the label, <span class="math inline">\(\mathbf{y}^t\)</span>, for the central word in <span class="math inline">\(\mathbf{x}^t\)</span>, i.e. the <span class="math inline">\(t\)</span>th word in the raw input <span class="math inline">\(\mathbf{x}\)</span>.</li>
</ul>
<p>Define <span class="math inline">\(\mathbf{E} \in \mathbb{R}^{V\times D}, \mathbf{W} \in \mathbb{R}^{D \times H}, \mathbf{U} \in \mathbb{R}^{H\times 5}, \mathbf{b}_1 \in \mathbb{R}^{1\times H}, \mathbf{b}_2 \in \mathbb{R}^{1\times 5}\)</span>, for the <span class="math inline">\(t\)</span>th word in the raw input <span class="math inline">\(\mathbf{x}\)</span>, its windowed input is <span class="math inline">\(\mathbf{x}^t = [\mathbf{x}^{t-w}, \dots, \mathbf{x}^t, \dots, \mathbf{x}^{t+w}],\)</span> the model for this <span class="math inline">\(\mathbf{x}^t\)</span> and a window size <span class="math inline">\(w\)</span> is</p>
<p><span class="math display">\[\begin{array}{rcl} \mathbf{e}^t &amp;=&amp; [\mathbf{x}^{t-w}\mathbf{E}, \dots, \mathbf{x}^t\mathbf{E}, \dots, \mathbf{x}^{t+w}\mathbf{E}]\\
\mathbf{h}^t &amp;=&amp; ReLU(\mathbf{e}^t\mathbf{W} + \mathbf{b}_1)\\
\hat{\mathbf{y}}^t &amp;=&amp; softmax(\mathbf{h}^t\mathbf{U}+\mathbf{b}_2)\\
J &amp;=&amp;  \sum_tCE(\mathbf{y}^t, \hat{\mathbf{y}^t}) \\
&amp;=&amp; -\sum_t\sum_iy_i^t log(\hat{y_i^t})
\end{array}\]</span></p>
<ul>
<li>Code:</li>
</ul>
<ol style="list-style-type: decimal">
<li>Load and preprocess data</li>
</ol>
<p>The first two (sentence, label) pairs from the data are</p>
<pre class="python"><code>[([&#39;EU&#39;, &#39;rejects&#39;, &#39;German&#39;, &#39;call&#39;, &#39;to&#39;, &#39;boycott&#39;, &#39;British&#39;, &#39;lamb&#39;, &#39;.&#39;],
  [&#39;ORG&#39;, &#39;O&#39;, &#39;MISC&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;MISC&#39;, &#39;O&#39;, &#39;O&#39;]),
 ([&#39;Peter&#39;, &#39;Blackburn&#39;], [&#39;PER&#39;, &#39;PER&#39;])]</code></pre>
<p>After loading the data, a dictionary of ‘token to id’ (tok2id) is built:</p>
<pre class="python"><code>{&#39;eu&#39;: 1,
 &#39;rejects&#39;: 2,
 &#39;german&#39;: 3,
 &#39;call&#39;: 4,
 &#39;to&#39;: 5,
 &#39;boycott&#39;: 6,
 &#39;british&#39;: 7,
 &#39;lamb&#39;: 8,
 &#39;.&#39;: 9,
 &#39;peter&#39;: 10,
 &#39;blackburn&#39;: 11,
 &#39;CASE:aa&#39;: 11,
 &#39;CASE:AA&#39;: 12,
 &#39;CASE:Aa&#39;: 13,
 &#39;CASE:aA&#39;: 14,
 &#39;&lt;s&gt;&#39;: 15,
 &#39;&lt;/s&gt;&#39;: 16,
 &#39;UUUNKKK&#39;: 17}</code></pre>
<p>In the dictionary, in addition to the word in each sentences, the 4 case types of the word (‘CASE:’) and the start (“<span class="math inline">\(&lt;s&gt;\)</span>”), the end (“<span class="math inline">\(&lt;/s&gt;\)</span>”) are also added to the dictionary for later use.</p>
<p>Having the tok2id dictionary, each word is represented by a vector of [id, case type], each sentence is a list of [id, case type], and each sentence and its corresponding labels are in a tuple, in the example we have 2 sentences, so the <em>train_data</em> and <em>dev_data</em> returned from the <strong>load_and_preprocess_data</strong> are assigned to <em>train</em> and <em>dev</em>, and is a list of 2 tuples (here the train and dev are the same dataset):</p>
<pre class="python"><code>[([[1, 12],
   [2, 11],
   [3, 13],
   [4, 11],
   [5, 11],
   [6, 11],
   [7, 13],
   [8, 11],
   [9, 14]],
  [1, 4, 3, 4, 4, 4, 3, 4, 4]),
 ([[10, 13], [11, 13]], [0, 0])]</code></pre>
<p><em>train</em> and <em>dev</em> returned from the <strong>load_and_preprocess_data</strong> are the raw data and assigned to <em>train_raw</em> and <em>dev_raw</em>, respectively, as</p>
<pre class="python"><code>[([&#39;EU&#39;, &#39;rejects&#39;, &#39;German&#39;, &#39;call&#39;, &#39;to&#39;, &#39;boycott&#39;, &#39;British&#39;, &#39;lamb&#39;, &#39;.&#39;],
  [&#39;ORG&#39;, &#39;O&#39;, &#39;MISC&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;MISC&#39;, &#39;O&#39;, &#39;O&#39;]),
 ([&#39;Peter&#39;, &#39;Blackburn&#39;], [&#39;PER&#39;, &#39;PER&#39;])]</code></pre>
<p><em>helper</em> returned from the <strong>load_and_preprocess_data</strong> is a class with two attributes: tok2id, max_length (in our example is 9).</p>
<p>The relation is</p>
<pre class="python"><code>helper, train, dev, train_raw, dev_raw = return helper, train_data, dev_data,  train, dev </code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Load embeddings</li>
</ol>
<p>For each word, the word vector is of the order 50, the embedding matrix is of the shape 19*50 (the number of word in tok2dic is 18, in the function the rows in the embeddings +1, with the first vector all 0s),</p>
<pre class="python"><code>embeddings = np.array(np.random.randn(len(helper.tok2id) + 1, EMBED_SIZE), dtype=np.float32)
embeddings[0] = 0.</code></pre>
<p>Since in data, we have a file of all vocabulary (vocab.txt, containing 100232 words) and a file with all word vectors (wordVector.txt, containing 100232 vectors), we then pair the <span class="math inline">\(i\)</span>th word in the vocabulary with the <span class="math inline">\(i\)</span>th vector using a function <strong>load_word_vector_mapping()</strong>, the first 10 records in the resulting ordered dictionary (ret) are</p>
<pre class="python"><code>[&#39;UUUNKKK&#39;, &#39;the&#39;, &#39;,&#39;, &#39;.&#39;, &#39;of&#39;, &#39;and&#39;, &#39;in&#39;, &#39;&quot;&#39;, &#39;a&#39;, &#39;to&#39;],
[array([.....size50]),array([.....size50]),.....,array([.....size50])]</code></pre>
<p>10 words (keys) matched with 10 arrays (values), and each array has order of 50, the embedding size.</p>
<p>For our 2-sentence tiny example, the word vectors of the 18 words in tok2id are found from ret, then constituting an embedding matrix for the 18 words</p>
<pre class="python"><code>for word, vec in load_word_vector_mapping(args.vocab, args.vectors).items():
    word = normalize(word)
    if word in helper.tok2id:
        embeddings[helper.tok2id[word]] = vec
embeddings.shape
Out[109]: (19, 50)</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Making windowed input and output pair
Each word in the two sentences are featurized as a vector in order of 2 [id, case_type]. The train data (also the dev data) is</li>
</ol>
<pre class="python"><code>In: train

Out: [([[1, 12],
   [2, 11],
   [3, 13],
   [4, 11],
   [5, 11],
   [6, 11],
   [7, 13],
   [8, 11],
   [9, 14]],
  [1, 4, 3, 4, 4, 4, 3, 4, 4]),
 ([[10, 13], [11, 13]], [0, 0])]</code></pre>
<p>The start <span class="math inline">\(&lt;s&gt;\)</span> and end <span class="math inline">\(&lt;/s&gt;\)</span> are featurized as</p>
<pre class="python"><code>helper.START
[15,11]

helper.END
[16,11]</code></pre>
<p>Using the following code to make pair of ([windowed data], label) for each word</p>
<pre class="python"><code>def make_windowed_data(data= train, start = helper.START, end = helper.END, window_size = 1):

    windowed_data = []
    for sentence, label in data:
        l = len(sentence)
        sentence_extended = [start]*window_size + sentence + [end]*window_size
        for i in range(window_size, l+window_size):
            temp = []
            for j in range(i-window_size, i+window_size+1):
                temp.extend(sentence_extended[j])

                window_label = (temp, label[i-window_size])
            windowed_data.append(window_label)

    return windowed_data

make_windowed_data(train_data, helper.START, helper.END)</code></pre>
<p>The windowed data returned is</p>
<pre class="python"><code>[([15, 11, 1, 12, 2, 11], 1),
 ([1, 12, 2, 11, 3, 13], 4),
 ([2, 11, 3, 13, 4, 11], 3),
 ([3, 13, 4, 11, 5, 11], 4),
 ([4, 11, 5, 11, 6, 11], 4),
 ([5, 11, 6, 11, 7, 13], 4),
 ([6, 11, 7, 13, 8, 11], 3),
 ([7, 13, 8, 11, 9, 14], 4),
 ([8, 11, 9, 14, 16, 11], 4),
 ([15, 11, 10, 13, 11, 13], 0),
 ([10, 13, 11, 13, 16, 11], 0)]</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Define the window model</li>
</ol>
<p>Build the model using tensorflow:</p>
<ul>
<li>add placeholders for input <span class="math inline">\(\mathbf{x}\)</span>, if window size =1, for the <span class="math inline">\(t\)</span>th word, <span class="math inline">\(t=1, \dots, T\)</span>, <span class="math inline">\(\mathbf{x}^t \in \mathbb{R}^{3*D}, D=50\)</span> so <span class="math inline">\(\mathbf{x} \in \mathbb{R}^{T, 3*D}\)</span> , labels <span class="math inline">\(\mathbf{y}\)</span>, which is a vector of correct lables for the <span class="math inline">\(t\)</span>th word, <span class="math inline">\(\mathbf{y} \in \mathbb{R}^{T*1}\)</span>, dropout is a scalar from 0 to 1:</li>
</ul>
<pre class="python"><code>def add_placeholders(self):
    self.input_placeholder = tf.placeholder(tf.int32, [None, self.config.n_window_features])
    self.labels_placeholder = tf.placeholder(tf.int32, [None,])
    self.dropout_placeholder = tf.placeholder(tf.float32)</code></pre>
<ul>
<li>create feed dictionary for the placeholders</li>
</ul>
<pre class="python"><code> def create_feed_dict(self, inputs_batch, labels_batch=None, dropout=1):
        feed_dict = {self.input_placeholder: inputs_batch, self.dropout_placeholder: dropout}
        if labels_batch is not None:
            feed_dict[self.labels_placeholder] = labels_batch 
        return feed_dict</code></pre>
<p>The inputs_batch and labels_batch are obtained as follow:</p>
<p>(i). make window data and assigned the windowed_data pair for both <em>train</em> and <em>dev</em> as <em>train_examples</em> and <em>dev_set</em>:</p>
<pre class="python"><code>def preprocess_sequence_data(self, examples):
    return make_windowed_data(examples, start=helper.START, end=helper.END, window_size=config.window_size)

train_examples = self.preprocess_sequence_data(train)
dev_set = self.preprocess_sequence_data(dev)</code></pre>
<p>(ii). make minibatch on the <em>train_examples</em>:
The <em>train_examples</em> (as well as <em>dev_set</em>) is</p>
<pre class="python"><code>[([15, 11, 1, 12, 2, 11], 1),
  ([1, 12, 2, 11, 3, 13], 4),
  ([2, 11, 3, 13, 4, 11], 3),
  ([3, 13, 4, 11, 5, 11], 4),
  ([4, 11, 5, 11, 6, 11], 4),
  ([5, 11, 6, 11, 7, 13], 4),
  ([6, 11, 7, 13, 8, 11], 3),
  ([7, 13, 8, 11, 9, 14], 4),
  ([8, 11, 9, 14, 16, 11], 4),
  ([15, 11, 10, 13, 11, 13], 0),
  ([10, 13, 11, 13, 16, 11], 0)],</code></pre>
<p>consisting 11 windowed inputs. We want to make minibatch of these 11 inputs:</p>
<p>first, recombine inputs and labels by grouping inputs into one list and labels into another list</p>
<pre class="python"><code>In: batches = [np.array(col) for col in zip(*train_examples)]
    batches

Out: [array([[15, 11,  1, 12,  2, 11],
        [ 1, 12,  2, 11,  3, 13],
        [ 2, 11,  3, 13,  4, 11],
        [ 3, 13,  4, 11,  5, 11],
        [ 4, 11,  5, 11,  6, 11],
        [ 5, 11,  6, 11,  7, 13],
        [ 6, 11,  7, 13,  8, 11],
        [ 7, 13,  8, 11,  9, 14],
        [ 8, 11,  9, 14, 16, 11],
        [15, 11, 10, 13, 11, 13],
        [10, 13, 11, 13, 16, 11]]), array([1, 4, 3, 4, 4, 4, 3, 4, 4, 0, 0])]</code></pre>
<p>let</p>
<pre class="python"><code>minibatch_size = 3
data_size = len(batches[0]) #data_size = 11</code></pre>
<p>shuffle the indices</p>
<pre class="python"><code>In: indices = np.arange(data_size)
    np.random.shuffle(indices)
    indices

Out: array([ 7, 10,  8,  9,  4,  6,  2,  3,  5,  0,  1]) </code></pre>
<p>selecting the minibatches of minibatch_size according to the shuffled indices</p>
<pre class="python"><code>for minibatch_start in np.arange(0, data_size, minibatch_size):
    minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]
    print([d[minibatch_indices] if type(d) is np.ndarray for d in batches])</code></pre>
<p>the resulted minibatches from 11 inputs with batch size of 3 are</p>
<pre class="python"><code>[array([[ 8, 11,  9, 14, 16, 11],
       [ 6, 11,  7, 13,  8, 11],
       [15, 11,  1, 12,  2, 11]]), array([4, 3, 1])]
[array([[ 1, 12,  2, 11,  3, 13],
       [ 5, 11,  6, 11,  7, 13],
       [ 4, 11,  5, 11,  6, 11]]), array([4, 4, 4])]
[array([[ 2, 11,  3, 13,  4, 11],
       [10, 13, 11, 13, 16, 11],
       [ 7, 13,  8, 11,  9, 14]]), array([3, 0, 4])]
[array([[ 3, 13,  4, 11,  5, 11],
       [15, 11, 10, 13, 11, 13]]), array([4, 0])]</code></pre>
<p>where for each minibatch contains <em>inputs_batch</em> and <em>labels_batch</em> in <strong>create_feed_dict()</strong>. In the original code, it uses <strong>yield</strong> instead of <strong>print</strong>, so the training start one minibatch after another (train on the first minibatch, prog bar moves one step, then train on the next minibatch, prog bar moves on another step, and so on).</p>
<ul>
<li>add embedding matrix</li>
</ul>
<p>In Step 2. load embeddings, we have a <span class="math inline">\(19\times50\)</span> embedding matrix <span class="math inline">\(\mathbf{E}\)</span>. Since the <em>inputs_batch</em> are shuffled, so first look up the corresponding word vectors for each window input (<span class="math inline">\(\begin{bmatrix}\mathbf{x}^{t-w}\mathbf{E} \\ \mathbf{x}^{t}\mathbf{E}\\ \mathbf{x}^{t+w}\mathbf{E}\end{bmatrix}\)</span>), then vectorize to be <span class="math inline">\(\mathbf{e}^t = [\mathbf{x}^{t-w}\mathbf{E}, \mathbf{x}^{t}\mathbf{E}, \mathbf{x}^{t+w}\mathbf{E}]\)</span> of size <span class="math inline">\(1\times (3*2*50)\)</span> (3 = # of words in a window, 2 = the number of features for a word [id, case type], D =50) in our example. This corresponds to the first equation in the neural network model defined in the beginning.</p>
<pre class="python"><code>def add_embedding(self):
    embedded = tf.Variable(self.pretrained_embeddings)
    embeddings = tf.nn.embedding_lookup(embedded,self.input_placeholder)
    embeddings = tf.reshape(embeddings, [-1, self.config.n_window_features * self.config.embed_size])                                                     
    return embeddings</code></pre>
<ul>
<li>add hidden layer and the prediction</li>
</ul>
<ol style="list-style-type: lower-roman">
<li><p>define variables <span class="math inline">\(\mathbf{b}_1, \mathbf{b}_2, \mathbf{W}, \mathbf{U}\)</span></p></li>
<li><p>define functions</p></li>
</ol>
<pre class="python"><code>def add_prediction_op(self):
    x = self.add_embedding()
    dropout_rate = self.dropout_placeholder
    
    with tf.variable_scope(&quot;transformation&quot;):
            b1 = tf.get_variable(name=&#39;b1&#39;, shape = [self.config.hidden_size],initializer=tf.contrib.layers.xavier_initializer(seed=1))
            b2 = tf.get_variable(name=&#39;b2&#39;, shape = [self.config.n_classes],  initializer=tf.contrib.layers.xavier_initializer(seed=2))
            W = tf.get_variable(name=&#39;W&#39;, shape = [self.config.n_window_features * self.config.embed_size,self.config.hidden_size],initializer=tf.contrib.layers.xavier_initializer(seed=3))
            U = tf.get_variable(name=&#39;U&#39;, shape = [self.config.hidden_size, self.config.n_classes], initializer=tf.contrib.layers.xavier_initializer(seed=4))
            
            z1 = tf.matmul(x,W) + b1
            h = tf.nn.relu(z1)
            h_drop = tf.nn.dropout(h, dropout_rate)
            pred = tf.matmul(h_drop, U) + b2

    return pred</code></pre>
<ul>
<li>define loss function and optimization method</li>
</ul>
<pre class="python"><code>def add_loss_op(self, pred):

        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=self.labels_placeholder)
        loss = tf.reduce_mean(loss)

        return loss

def add_training_op(self, loss):

        adam_optim = tf.train.AdamOptimizer(self.config.lr)
        train_op = adam_optim.minimize(loss)

        return train_op</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>Fit the model</li>
</ol>
<ul>
<li>initialize the window model</li>
</ul>
<pre class="python"><code>config = Config()
helper, train, dev, train_raw, dev_raw = load_and_preprocess_data(args)
embeddings = load_embeddings(args, helper)
model = WindowModel(helper, config, embeddings)
# model.pretrained_embeddings = embeddings</code></pre>
<p>where the <em>helper</em> and <em>embeddings</em> are introduced in step 1 and 2 above.</p>
<ul>
<li>fit the model</li>
</ul>
<p>(i). the <em>train_examples</em> and <em>dev_set</em> are obtained above by using the <strong>make_windowed_data()</strong></p>
<p>(ii). we start run 10 epochs to train the model,</p>
<pre class="python"><code>def fit(self, sess, saver, train, dev):
    train_examples = self.preprocess_sequence_data(train)
    dev_set = self.preprocess_sequence_data(dev)
    for epoch in range(self.config.n_epochs):
                #logger.info(&quot;Epoch %d out of %d&quot;, epoch + 1, self.config.n_epochs)
            score = self.run_epoch(sess, train_examples, dev_set, train, dev)</code></pre>
<p>for each epochs in <strong>run_epoch()</strong>, first get the minibatches of from <em>train_examples</em> with <em>batch_size</em> is 3 using <strong>minibatches()</strong> above, recall each yielded minibatch is [<em>inputs_batch</em> = array([[,,,,,],[,,,,,],[,,,,,]]), <em>labels_batch</em> = array([,,])].</p>
<p>Feed <em>inputs_batch</em> and <em>labels_batch</em> into <strong>create_feed_dict()</strong> within <strong>train_on_batch()</strong>. With the <em>model.pretrained_embeddings</em>, we <strong>add_embedding</strong>, <strong>add_prediction_op</strong>, <strong>add_loss_op</strong> and <strong>add_train_op</strong> to obtain the <em>loss</em> and update the parameters <span class="math inline">\(\mathbf{e}^t \in minibatch, \mathbf{W}, \mathbf{U}, \mathbf{b}_1, \mathbf{b}_2\)</span>. After train on the first minibatch, prog bar +1 and train on the secon minibatch and so on.</p>
<pre class="python"><code>def train_on_batch(self, sess, inputs_batch, labels_batch):
    feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch,
                                       dropout=self.config.dropout)
    _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)
    return loss
       
def run_epoch(self, sess, train_examples, dev_set, train_examples_raw, dev_set_raw):
    prog = Progbar(target=1 + int(len(train_examples) / self.config.batch_size))
    for i, batch in enumerate(minibatches(train_examples, self.config.batch_size)):
        loss = self.train_on_batch(sess, *batch)
        prog.update(i + 1, [(&quot;train loss&quot;, loss)])
        if self.report: self.report.log_train_loss(loss)
    print(&quot;&quot;)

    logger.info(&quot;Evaluating on development data&quot;)
    token_cm, entity_scores = self.evaluate(sess, dev_set, dev)
    logger.debug(&quot;Token-level confusion matrix:\n&quot; + token_cm.as_table())
    logger.debug(&quot;Token-level scores:\n&quot; + token_cm.summary())
    logger.info(&quot;Entity level P/R/F1: %.2f/%.2f/%.2f&quot;, *entity_scores)

    f1 = entity_scores[-1]
    return f1</code></pre>
<p>To evaluate on the <em>dev</em>, first make window data using <strong>make_windowed_data</strong> within <strong>preprocess_sequence_data</strong></p>
<pre class="python"><code>dev_set = self.preprocess_sequence_data(self.helper.vectorize(dev))</code></pre>
<p>get the minibatches from <em>dev_set</em> within <em>shuffle = False</em>, the first minibatch is</p>
<pre class="python"><code>[array([[15, 11,  1, 12,  2, 11],
         [ 1, 12,  2, 11,  3, 13],
         [ 2, 11,  3, 13,  4, 11]]), array([1, 4, 3])]</code></pre>
<p>get the <em>inputs_batch</em> for <strong>create_feed_dict()</strong> while <em>labels_batch = None</em> from the each minibatch to get the predictions for the each minibatch</p>
<pre class="python"><code>def predict_on_batch(self, sess, inputs_batch):
        feed = self.create_feed_dict(inputs_batch)
        predictions = sess.run(tf.argmax(self.pred, axis=1), feed_dict=feed)
        return predictions
        
for i, batch in enumerate(minibatches(dev_set, self.config.batch_size = 3, shuffle=False)):
            # Ignore predict
            batch = batch[:1] + batch[2:] 
            &quot;&quot;&quot;
            batch = [array([[15, 11,  1, 12,  2, 11],
         [ 1, 12,  2, 11,  3, 13],
         [ 2, 11,  3, 13,  4, 11]])]
            &quot;&quot;&quot;
            preds_ = self.predict_on_batch(sess, *batch)
            preds += list(preds_)
            prog.update(i + 1, [])
            consolidate_predictions(dev, inputs=dev_set, preds)</code></pre>
<p>where <strong>consolidata_predictions</strong> returns [[sentence1, labels, labels_], [sentence2, labels, labels_], …, [sentence11, labels, labels_]], <em>labels_</em> is the predictions for the corresponding sentence (len(predictions) = 9 for the first sentence, len(predictions) = 2 for the second sentence), <em>labels</em> is the true labels for that sentence.</p>
<p>For each consolidated predictions [sentence, labels, labels_], compute the precison, recall and f1 scores:</p>
<pre class="python"><code>token_cm = ConfusionMatrix(labels=LBLS)

correct_preds, total_correct, total_preds = 0., 0., 0.
      for _, labels, labels_  in self.output(sess, examples_raw, examples):
            for l, l_ in zip(labels, labels_):
                token_cm.update(l, l_)
            gold = set(get_chunks(labels))
            pred = set(get_chunks(labels_))
            correct_preds += len(gold.intersection(pred))
            total_preds += len(pred)
            total_correct += len(gold)

      p = correct_preds / total_preds if correct_preds &gt; 0 else 0
      r = correct_preds / total_correct if correct_preds &gt; 0 else 0
      f1 = 2 * p * r / (p + r) if correct_preds &gt; 0 else 0</code></pre>
</div>
<div id="results-and-predictions" class="section level1">
<h1>Results and Predictions</h1>
<p>Train with “data/tiny.conll” as <em>dev_raw</em> and <em>train_raw</em>, which contains 721 sentences (the first 2 sentences are illustrated above).</p>
<pre class="python"><code>def do_test2():
    logger.info(&quot;Testing implementation of WindowModel&quot;)
    config = Config()
    #helper, train, dev, train_raw, dev_raw = load_and_preprocess_data(args)
    #embeddings = load_embeddings(args, helper)
    #config.embed_size = embeddings.shape[1]

    with tf.Graph().as_default():
        logger.info(&quot;Building model...&quot;,)
        start = time.time()
        model = WindowModel(helper, embeddings)
        logger.info(&quot;took %.2f seconds&quot;, time.time() - start)

        init = tf.global_variables_initializer()
        saver = None

        with tf.Session() as session:
            session.run(init)
            #[[IMPORTANT MAP]]: train_raw, dev_raw ---load_preprocess()--&gt; train, dev--precess_sequence_data()--&gt;train_examples, dev_set
            model.fit(session, saver, train_examples, dev_set, train, dev) 
            output = model.output(session, dev_raw, dev_set) #list of list: [[sentence, labels, labels_]]
            sentences, labels, predictions = zip(*output)
            predictions = [[LBLS[l] for l in preds] for preds in predictions]
            output = zip(sentences, labels, predictions)
            print(sentences[0], labels[0], predictions[0])
            with open(&quot;results/window/window_predictions.conll&quot;, &#39;w&#39;) as f:
                write_conll(f, output)
    
    logger.info(&quot;Model did not crash!&quot;)
    logger.info(&quot;Passed!&quot;)</code></pre>
<pre class="python"><code>#FIRST EPOCH
INFO:Epoch 1 out of 10
5/5 [==============================] - 0s - train loss: 1.4808     
INFO:Evaluating on development data


5/5 [==============================] - 0s     
DEBUG:Token-level confusion matrix:
go\gu   PER     ORG     LOC     MISC    O      
PER     134.00  57.00   84.00   6.00    543.00 
ORG     52.00   29.00   25.00   5.00    280.00 
LOC     47.00   36.00   70.00   4.00    420.00 
MISC    28.00   10.00   27.00   5.00    188.00 
O       167.00  93.00   81.00   14.00   6761.00

DEBUG:Token-level scores:
label   acc     prec    rec     f1   
PER     0.89    0.31    0.16    0.21 
ORG     0.94    0.13    0.07    0.09 
LOC     0.92    0.24    0.12    0.16 
MISC    0.97    0.15    0.02    0.03 
O       0.81    0.83    0.95    0.88 
micro   0.91    0.76    0.76    0.76 
macro   0.91    0.33    0.27    0.28 
not-O   0.93    0.24    0.12    0.16 

INFO:Entity level P/R/F1: 0.09/0.06/0.07

...

#LAST EPOCH
INFO:Epoch 10 out of 10


5/5 [==============================] - 0s - train loss: 0.3210     
INFO:Evaluating on development data


5/5 [==============================] - 0s     
DEBUG:Token-level confusion matrix:
go\gu   PER     ORG     LOC     MISC    O      
PER     670.00  40.00   33.00   14.00   67.00  
ORG     133.00  96.00   41.00   19.00   102.00 
LOC     41.00   25.00   402.00  16.00   93.00  
MISC    59.00   24.00   46.00   52.00   77.00  
O       95.00   44.00   43.00   23.00   6911.00

DEBUG:Token-level scores:
label   acc     prec    rec     f1   
PER     0.95    0.67    0.81    0.74 
ORG     0.95    0.42    0.25    0.31 
LOC     0.96    0.71    0.70    0.70 
MISC    0.97    0.42    0.20    0.27 
O       0.94    0.95    0.97    0.96 
micro   0.95    0.89    0.89    0.89 
macro   0.95    0.63    0.59    0.60 
not-O   0.96    0.64    0.60    0.62 

INFO:Entity level P/R/F1: 0.49/0.53/0.51</code></pre>
<p>The results are written and saved in “results/window/window_predictions.conll”. The saved predicted results for the first 2 sentences are:</p>
<pre class="python"><code>EU  ORG O #(False)
rejects O   O
German  MISC    ORG #(False)
call    O   O
to  O   O
boycott O   O
British MISC    MISC
lamb    O   O
.   O   O

Peter   PER PER
Blackburn   PER PER</code></pre>
</div>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">Luyao Peng</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        2019-05-06
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/name-entity-recognition/">Name Entity Recognition</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/rnn_ner/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">CS224n/assignment3/: RNN/GRU Name Entity Recognition</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/basics-of-rnn-and-lstm/">
            <span class="next-text nav-default">Basics of RNN and LSTM</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  <div id="disqus_thread"></div>
  <script>
  (function() {
  var d = document, s = d.createElement('script');
  s.src = 'https://luyao-1.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:your@email.com" class="iconfont icon-email" title="email"></a>
      <a href="http://localhost:1313" class="iconfont icon-stack-overflow" title="stack-overflow"></a>
      <a href="http://localhost:1313" class="iconfont icon-twitter" title="twitter"></a>
      <a href="http://localhost:1313" class="iconfont icon-facebook" title="facebook"></a>
      <a href="http://localhost:1313" class="iconfont icon-linkedin" title="linkedin"></a>
      <a href="http://localhost:1313" class="iconfont icon-google" title="google"></a>
      <a href="http://localhost:1313" class="iconfont icon-github" title="github"></a>
      <a href="http://localhost:1313" class="iconfont icon-weibo" title="weibo"></a>
      <a href="http://localhost:1313" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="http://localhost:1313" class="iconfont icon-douban" title="douban"></a>
      <a href="http://localhost:1313" class="iconfont icon-pocket" title="pocket"></a>
      <a href="http://localhost:1313" class="iconfont icon-tumblr" title="tumblr"></a>
      <a href="http://localhost:1313" class="iconfont icon-instagram" title="instagram"></a>
      <a href="http://localhost:1313" class="iconfont icon-gitlab" title="gitlab"></a>
      <a href="http://localhost:1313" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  
  
  <span class="copyright-year">
    &copy; 
    2017 - 
    2020
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Luyao Peng</span>
  </span>
</div>


    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
<script type="text/javascript" src="/dist/even.26188efa.min.js"></script>








</body>
</html>
