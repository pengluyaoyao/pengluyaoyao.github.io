---
autoCollapseToc: true
title: Basics of RNN and LSTM
author: ''
date: '2019-04-24'
categories:
  - Deep Learning
tags:
  - Machine Learning
draft: false
flowchartDiagrams:
  enable: true
  options: '{ ''x'': 0, ''y'': 0, ''line-width'': 3, ''line-length'': 50, ''text-margin'':
    10, ''font-size'': 14, ''font-color'': ''black'', ''line-color'': ''black'', ''element-color'':
    ''black'', ''fill'': ''white'', ''yes-text'': ''yes'', ''no-text'': ''no'', ''arrow-end'':
    ''block'', ''scale'': 1, ''i-am-a-comment-1'': ''Do not use //!'', ''i-am-a-comment-2'':
    ''style symbol types'', ''symbols'': { ''start'': { ''font-color'': ''red'', ''element-color'':
    ''green'', ''fill'': ''yellow'' }, ''end'': { ''class'': ''end-element'' } },
    ''i-am-a-comment-3'': ''even flowstate support ;-)'', ''flowstate'': { ''request'':
    {''fill'': ''blue''} } }'
header-includes: \usepackage{amsmath} \usepackage{bm} 
---

# RNN

The language model computes the probability of a sequence of previous words, $P(word_1, word_2, \dots, word_t)$. The traditional language model is based on naive bayes model, the probability of a sequence of words is:

$$ P(word_1, word_2, \dots, word_{t}) = \prod_{i = 1}^{t} P(word_i|word_1, \dots, word_{i-1}). $$

The cons of traditional language models assumes independence among words (this is the naive part), which is not true in reality, it also requires a lot of RAM to compute the conditional probabilities for each word. One solution is to apply RNNs, which does not consider each input word independently, instead, RNNs can capture the information in previous sequence of words and evaluate their effects on the prediction on the current word (recurrent part). 

## The Model

<!-- <figure> -->
<!--   <img src="/post/2019-04-20-RNN_files/figure-html/rnn.jpg" alt="RNN" width=58% height=60%/> -->
<!--   <figcaption>RNNs, source: Nature</figcaption> -->
<!-- </figure> -->

```{R, echo=FALSE}
DiagrammeR::grViz("
digraph a_nice_graph {

graph [rankdir=TB; splines='line'; esep = 1; forcelabels=true;]


subgraph output{
rank = same; z1 [label = 'y&#770;@_{t-1}', shape = circle, fixedsize=true];
z2 [label = 'y&#770;@_{t}', shape = circle, fixedsize=true];
z3 [label = 'y&#770;@_{t+1}', shape = circle, fixedsize=true];
}

subgraph hidden{
rank = same; h0 [style = invis];h1 [label = 'h&#770;@_{t-1}', shape = circle, fixedsize=true];
h2 [label = 'h&#770;@_{t}', shape = circle, fixedsize=true];
h3 [label = 'h&#770;@_{t+1}', shape = circle, fixedsize=true];
h4 [style= invis];
}


subgraph input{
rank = same; x1 [label = 'x&#770;@_{t-1}', shape = plaintext, fixedsize=true];
x2 [label = 'x&#770;@_{t}', shape = plaintext, fixedsize=true];
x3 [label = 'x&#770;@_{t+1}', shape = plaintext, fixedsize=true];

}

{
rank = same;  edge [ style = invis ]; z1->z2->z3
}
{
rank = same;  edge [ style = invis ]; h0->h1->h2->h3->h4 
}
{
rank = same;  edge [ style = invis ]; x1->x2->x3
}
edge [ color='black' ]; h0-> h1 [xlabel = 'W' ] ; h3->h4;  h1 -> h2 [xlabel = 'W' ]; h1-> z1 [xlabel = 'V' ]; h2-> h3[xlabel = 'W' ] ; h2-> z2 [xlabel = 'V' ]; h3 -> z3 [xlabel = 'V' ]; x1->h1 [xlabel = 'U' ]; x2->h2[xlabel = 'U' ] ; x3->h3 [xlabel = 'U' ]; edge [ color='red' ]; h1->{h0 x1}; z1-> h1; z2->h2; z3->h3; h2 -> {h1 x2}; h3-> {h2 x3}; h4-> h3; edge [ style = invis ]; z1-> {h0 h1 h2 h3 h4};  h1 -> {x1 x2 x3};
}")
```

Define 

$$\mathbf{x}_t \in \mathbb{R}^{d}, \mathbf{s}_t \in \mathbb{R}^{D_s}, \mathbf{W} \in \mathbb{R}^{D_s \times D_s}, \mathbf{U} \in \mathbb{R}^{D_s \times d}, \mathbf{V} \in \mathbb{R}^{Voc \times D_s}, \mathbf{\hat{y}} \in \mathbb{R}^{Voc},$$

Converting the figure above into math expressions:
$$
\begin{array}{rcl}
\mathbf{s}_t &=& f\left(\mathbf{W}\mathbf{s}_{t-1} + \mathbf{U}\mathbf{x}_{t}\right)\\
\mathbf{\hat{y}}_t &=& softmax\left(\mathbf{V}\mathbf{s_t}\right),  t = 1,2,\dots, T\\
\hat{p}(w_{t+1}&=& v_j|w_t, \dots,w_1) = \hat{y}_{t,j}, j = 1, 2, \dots, Voc
\end{array}
$$

where

- $\mathbf{s}_t$ is the hidden state at the time $t$, it is a function ($f$ can be tanh, sigmoid, ReLU) of the previous hidden state $\mathbf{s}_{t-1}$ and word vector of the current input word $\mathbf{x}_t$
- $\hat{\mathbf{y}}_t$ is the predicted outcome (a vector of probabilities over all vocabulary) at time $t$
- $\hat{p}$ is the probability of the predicted word $w_{t+1}$ (at time $t+1$) is equal to the $j$th word in the vocabulary $v_j$ given a sequence of all previous input words, and $\hat{p}(w_{t+1}= v_j|w_t, \dots,w_1)$ is equal to the $j$th element in $\hat{\mathbf{y}}_t$.


At each time point, the $\mathbf{s}_{t}$ is a function of $\mathbf{s}_{t-1}$ and $\mathbf{s}_{t-1}$ is also a function of $\mathbf{s}_{t-2}$ etc., so the effects of all the previous input words will be taken into account when predicting the word at $t+1$, this is the recurrent part in RNN.

## The Cost Fucntion of RNN

The loss function at time $t$ for the RNN model above is

$$E_{t}(\hat{y}_{t}, y_{t}) = -\sum_{j=1}^{Voc}y_{t,j}log(\hat{y}_{t,j}), j = 1, 2, \dots, Voc,$$

$$E(\hat{y}, y)= -\frac{1}{T}\sum_{t=1}^{T}E_{t}(\theta),$$

where $y_{t,j}$ is the correct word at time $t$ located at the $j$th element in $\hat{\mathbf{y}}_t$, the loss function $J$ tries to minimize the difference between $y_{t,j}$ and $\hat{y}_{t,j}$ over all word $j$ and time $t$.


## Back Propagation of RNN



Our goal is to compute the gradient of total cost (the cost function is the loss $E$ across all t, loss function is the $E_t$ at time t) $E(\hat{y}, y)$ wrt $\mathbf{W}, \mathbf{U}$ and $\mathbf{V}$. From the figure above, the gradient wrt $\mathbf{V}$ only involves the values at current time $t$, while the gradients wrt $\mathbf{W}$ and $\mathbf{U}$ involves the values at the current time $t$ and at all the previous time. 

Notice $E(\hat{y}, y)$ is the sum of $E_{t}(\hat{y}_{t}, y_{t})$, each of which involves $\mathbf{W}, \mathbf{U}$ and $\mathbf{V}$, so we have

$$\frac{\partial{E}}{\partial{\mathbf{W}}} = \sum_{t=1}^{T}\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{W}}}}, \frac{\partial{E}}{\partial{\mathbf{U}}} = \sum_{t=1}^{T}\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{U}}}}, \frac{\partial{E}}{\partial{\mathbf{V}}} = \sum_{t=1}^{T}\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{V}}}}$$
Define $\mathbf{z}_t = \mathbf{Vs}_t$,  $\mathbf{z}_t \in \mathbb{R}^{Voc}$, recall $\frac{\partial{E}_t}{\partial{\mathbf{z}_t}} = \hat{\mathbf{y}_t}-\mathbf{y}_t = \mathbf{\delta}_{1t}$, then 

$$\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{V}}}} = \mathbf{\delta}_{1t} \mathbf{s}_t', \mathbf{\delta}_{1t} \in \mathbb{R}^{Voc}, \mathbf{s}_t \in \mathbb{R}^{Ds}, \mathbf{V} \in \mathbb{R}^{Voc\times D_s}$$, the gradient wrt $\mathbf{V}$ at time $t$ is only dependent on the values at the current time point, $\hat{\mathbf{y}}_t, \mathbf{y}_t, \mathbf{s}_t$.

Assuming the $f$ function is tanh function, 
$$\begin{array}{rcl}\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{W}}}} &=& \frac{\partial{E}_t}{\partial{\mathbf{s}_t}}\frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{h}_t}}\frac{\partial{\mathbf{h}}_t}{\partial{\mathbf{W}}}\\
&=& \frac{\partial{E}_t}{\partial{\mathbf{s}_t}} f'(\mathbf{h}_t)\left[\mathbf{s}_{t-1}+\mathbf{W}\frac{\partial{\mathbf{h}_t}}{\partial{\mathbf{W}}}\right]\\
&=& \frac{\partial{E}_t}{\partial{\mathbf{s}_t}} \left[f'\mathbf{s}_{t-1}+f'\mathbf{W} \frac{\partial{\mathbf{h}_t}}{\partial{\mathbf{W}}}\right]\\
&=& \frac{\partial{E}_t}{\partial{\mathbf{s}_t}} \frac{\partial{\mathbf{s}_t}}{\partial{\mathbf{W}}} + \frac{\partial{E}_t}{\partial{\mathbf{s}_t}} \frac{\partial{s}_t}{\partial{s}_{t-1}}\frac{\partial{s}_{t-1}}{\partial{\mathbf{W}}}\end{array},$$
where $\mathbf{h}_t = \mathbf{W}\mathbf{s}_{t-1} +\mathbf{U}\mathbf{x}_t$. 

Since $\frac{\partial{s}_{t-1}}{\partial{\mathbf{W}}} =  \frac{\partial{\mathbf{s}_{t-1}}}{\partial{\mathbf{W}}} + \frac{\partial{s}_{t-1}}{\partial{s}_{t-2}}\frac{\partial{s}_{t-2}}{\partial{\mathbf{W}}}$, plug it into above equation, we have

$$\begin{array}{rcl} \frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{W}}}} &=& \frac{\partial{E}_t}{\partial{\mathbf{s}_t}} \frac{\partial{\mathbf{s}_t}}{\partial{\mathbf{W}}} + \frac{\partial{E}_t}{\partial{\mathbf{s}_t}} \frac{\partial{s}_t}{\partial{s}_{t-1}}\frac{\partial{s}_{t-1}}{\partial{\mathbf{W}}} + \frac{\partial{E}_t}{\partial{\mathbf{s}_t}} \frac{\partial{s}_t}{\partial{s}_{t-2}}\frac{\partial{s}_{t-2}}{\partial{\mathbf{W}}} + \dots + \frac{\partial{E}_t}{\partial{\mathbf{s}_t}} \frac{\partial{s}_t}{\partial{s}_{1}}\frac{\partial{s}_{1}}{\partial{\mathbf{W}}}\\
&=& \sum_{t=1}^{T}\sum_{k=1}^{t}\frac{\partial{E}_t}{\partial{\mathbf{s}_t}}\frac{\partial{s}_t}{\partial{s}_{t-k}}\frac{\partial{s}_{t-k}}{\partial{\mathbf{W}}}\end{array}, \mathbf{W} \in \mathbb{R}^{D_s\times D_s}$$.


Similarly, the gradient of $E_t$ wrt $\mathbf{U}$ is

$$
\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{U}}}} = \frac{\partial{E}_t}{\partial{\mathbf{s}_t}}\frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{U}}}
$$
In $\mathbf{s}_t$, $\mathbf{U}$ is involved in $\mathbf{s}_{t-1}$ and $\mathbf{Ux}_t$, so

$$
\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{U}}}} =  \frac{\partial{E}_t}{\partial{\mathbf{s}_t}}\left(\frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{U}}} + \frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{s}_{t-1}}} \frac{\partial{\mathbf{s}}_{t-1}}{\partial{\mathbf{U}}}\right)
$$
since $\frac{\partial{\mathbf{s}}_{t-1}}{\partial{\mathbf{U}}} = \left(\frac{\partial{\mathbf{s}}_{t-1}}{\partial{\mathbf{U}}} + \frac{\partial{\mathbf{s}}_{t-1}}{\partial{\mathbf{s}_{t-2}}} \frac{\partial{\mathbf{s}}_{t-2}}{\partial{\mathbf{U}}}\right)$, plug it into the above equation, we have

$$
\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{U}}}} =  \frac{\partial{E}_t}{\partial{\mathbf{s}_t}}\left(\frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{U}}} + \frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{s}_{t-1}}} \left(\frac{\partial{\mathbf{s}}_{t-1}}{\partial{\mathbf{U}}} + \frac{\partial{\mathbf{s}}_{t-1}}{\partial{\mathbf{s}_{t-2}}} \frac{\partial{\mathbf{s}}_{t-2}}{\partial{\mathbf{U}}}\right)\right)
$$

continue this plug-in procedure until 0 time point, finally we have

$$
\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{U}}}} =  \sum_{t=1}^{T}\sum_{k=1}^{t}\frac{\partial{E}_t}{\partial{\mathbf{s}_t}} \frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{s}_{t-k}}}\frac{\partial{s}_{t-k}}{\partial{\mathbf{U}}}
$$
Notice that the gradient wrt $\mathbf{W}$ and $\mathbf{U}$ at time $t$ is dependent on both current and all previous values.

> PRACTICE: 
Based on a sketch of network at a single timestep:
```{R, echo=FALSE}
DiagrammeR::grViz("
digraph a_nice_graph {
graph [rankdir=BT; splines='line'; esep = 1; forcelabels=true;]
subgraph output{
z [label = 'y&#770;@_{t}', shape = circle, fixedsize=true];
}
subgraph hidden{
rank = same; h1 [label = 'h&#770;@_{t-1}', shape = circle, fixedsize=true];
h2 [label = 'h&#770;@_{t}', shape = circle, fixedsize=true];
h3 [label = 'h&#770;@_{t+1}', shape = circle, fixedsize=true];
}
subgraph input{
x2 [label = 'x&#770;@_{t}', shape = circle, fixedsize=true];
}
{
rank = same; h1->h2->h3
}
edge [ color='black' ]; h2->z; x2->h2
}")
```
Consider feed-forward $$
Draw the 'unrolled' network for the previous 3 timesteps (t-1, t-2, t-3) and compute $\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{U}}}}\bigg\rvert_{t-1}=$, $\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{W}}}}\bigg\rvert_{t-1}=$, $\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{V}}}}=$, $\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{x}_t}}}=$, the complexity of back propagation wrt the model parameters across the entire T timesteps, $\sum_{t=1}^T J^{(t)}$, is $O\left[T(VD_h + dD_h + t(D_h^2 + dD_h))\right]$, the costy term is $VD_h$, the dimension of the whole vocabulary, since $V>>D_h$,  


## The Vanishing Gradient Problem

If we look deeper at the $\frac{\partial{s}_t}{\partial{s}_{k}}$ part in $\frac{\partial{E}_{t}}{\partial{\mathbf{\mathbf{W}}}}  = \sum_{k=1}^{t}\frac{\partial{E}_t}{\partial{\mathbf{s}_t}}\frac{\partial{s}_t}{\partial{s}_{k}}\frac{\partial{s}_{k}}{\partial{\mathbf{W}}}$, $\frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{s}}_{k}} = \frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{s}}_{t-1}}\frac{\partial{\mathbf{s}}_{t-1}}{\partial{\mathbf{s}}_{t-2}}\dots\frac{\partial{\mathbf{s}}_{t-k+1}}{\partial{\mathbf{s}}_{t-k}}$, each partial derivative is a $D_s \times D_s$ Jacobian matrix, for example,


$$\frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{s}}_{t-1}} = \begin{bmatrix}\frac{\partial{s}_{t_1}}{\partial{s}_{t-1_1}} & \dots & \frac{\partial{s}_{t_{D_s}}}{\partial{s}_{t-1_1}} \\ \vdots & \ddots & \vdots\\ \frac{\partial{s}_{t_1}}{\partial{s}_{t-1_{D_s}}} & \dots & \frac{\partial{s}_{t_{D_s}}}{\partial{s}_{t-1_{D_s}}}\end{bmatrix}$$
if the function $f$ is a sigmoid or tanh function, the gradient at the end sides are almost 0, so the elements in $\frac{\partial{\mathbf{s}}_t}{\partial{\mathbf{s}}_{t-1}}$ will become small (saturated neurons) if the corresponding values in the input word vector have either high positive or low negative values, also the gradient matrix will multiply with each other $k$ times, the gradient contribution of the saturated neurons becomes smaller and smaller and eventually vanishes. On the other hand, if the function $f$ is ReLU function and the gradient is greater than 1 if the input values of the word vector are positive, then the contribution of the gradient will be explode after k times multiplications. Clipping tricks work for exploding problems, but not vanishing problems, because we are artificially inflating the influence of an input word 'far away' from time $t$ to prevent it from vanishing, which is not realistic ('far away' word hardly has influence in predicting the current word). 


## LSTM

### The Model
LSTM is one of the solutions to the vanishing gradient problems in RNN. LSTM operations are, at time $t$,

```{R, echo=FALSE}
DiagrammeR::grViz("
digraph a_nice_graph {
rankdir=BT;

subgraph {
rank = same
x1 [label = 'x@_{t}', shape=circle,fixedsize=true]
x2 [style = invis]
x3 [label = 'x@_{t+1}', shape = circle, fixedsize = true]
x1->x2 [style = invis, minlen = 14]; x2->x3 [style = invis, minlen =2]
}

subgraph {
rank = same
a1 [label = 'out@_{t-1}', shape=circle,fixedsize=true]
a2 [ shape=point,fixedsize=true]
a3 [ shape=point,fixedsize=true]
a4 [ shape=point,fixedsize=true]
a5 [ shape=point,fixedsize=true]
a6 [ shape=point,fixedsize=true]
#a7 [style = invis]
a8 [label = 'out@_{t}', shape=circle,fixedsize=true]
a9 [style = invis]
a1->a2 [minlen=0.5]; a2->a3 [arrowhead=none, minlen = 1]; a3->a4 [arrowhead=none, minlen=4]; a4->a5->a6 [arrowhead=none]; edge [style = invis] a6->a8; edge [style = dotted, minlen = 2] a8->a9; a9->a8 [color = 'red']
}

subgraph {
rank = same
b1 [label = <&sigma;>, shape = box]
b2 [label = 'tanh', shape = box]
b3 [label = <&sigma;>, shape = box]
b4 [label = <&sigma;>, shape = box]
edge [style = invis] b1->b2->b3->b4
}

subgraph {
rank = same
c0 [label = 'x', shape = box, fixedsize = true]
c1 [label = 'f@_{t}', shape = circle,fixedsize=true]
c2 [label = 'a@_{t}', shape = circle,fixedsize=true]
c3 [label = 'i@_{t}', shape = circle,fixedsize=true]
c4 [label = 'o@_{t}', shape = circle,fixedsize=true]
c5 [label = 'x', shape = box,fixedsize=true]
c4->c5 [minlen=1]; edge [style = invis]c0->c1->c2->c3->c4
}

subgraph{
rank = same
d0 [label='tanh', shape = box]
d1 [label = 'x', shape = box]
d3 [style = invis]
d2 [label = 'tanh', shape = box]
edge [style = invis] d0->d1->d3->d2
}

subgraph{
rank = same
e1 [label = 'state@_{t-1}', shape = circle, fixedsize= true]
e2 [label = 'x', shape = box, fixedsize= true]
e3 [label = '+', shape = box, fixedsize= true]
e4 [label = 'state@_{t}', shape = circle, fixedsize= true]
e5 [style = invis]
e1->e2 [minlen = 1]; e2->e3 [minlen = 4]; e3->e4 [minlen=5.2]; e4->e5 [style = dotted, minlen = 2]; e5->e4 [style = dotted, minlen = 2, color = 'blue']
}
x1->a2; a3->b1; a4->b2; a5->b3; a6->b4; b1->c1; b2->c2; b3->c3; b4->c4; c2->d1; c3->d1; d2->c5;d0->c0; c0->a1;  x3->a9 [style = dotted]; x2->a8 [color = red];  c5->a8; c5->d2 [color ='blue']; a8->c5 [color ='blue']; c1->e2; d1->e3;  edge [ dir = none, color= 'blue'] d2->e4; edge [ dir = none, color = 'black' ] d2->e4; d0->e1 [arrowhead = none];
}")
```

- activation: the new memory cell, controlling whether current input $\mathbf{x}_t$ matters or not,
$$\mathbf{a}_t = tanh(\mathbf{W}_a\mathbf{x}_t+\mathbf{U}_a\mathbf{out}_{t-1}+\mathbf{b}_a)$$
- input gate: controlling whether $\mathbf{a}_t$ matters in $\mathbf{state}_t$ or not. If $\mathbf{i}_t$ is 1, $\mathbf{a}_{t-1}$ will be dirrectly copied at time $t$, if $\mathbf{i}_t$ is 0, $\mathbf{a}_{t-1}$ will be completely ignored at time $t$.
$$\mathbf{i}_t = \sigma(\mathbf{W}_i\mathbf{x}_t + \mathbf{U}_i \mathbf{out}_{t-1} + \mathbf{b}_i)$$
- forget gate: controlling how much $state_{t-1}$ influence $state_t$ (how much past state should matter now), if $\mathbf{f}_t$ is 1, $state_{t=1}$ will be dirrectly copied at time $t$, if $\mathbf{f}_t$ is 0, $state_{t_1}$ will be completely ignored at time $t$,

$$\mathbf{f}_t = \sigma(\mathbf{W}_f\mathbf{x}_t + \mathbf{U}_f \mathbf{out}_{t-1} + \mathbf{b}_f)$$

- output gate: controlling how much $\mathbf{state}_t$ is exposed in $\mathbf{out}_t$,

$$\mathbf{o}_t = \sigma(\mathbf{W}_o\mathbf{x}_t + \mathbf{U}_o \mathbf{out}_{t-1} + \mathbf{b}_o)$$
leading to:

- internal state:

$$\mathbf{state}_t = \mathbf{a}_t \odot \mathbf{i}_t + \mathbf{f}_t\odot \mathbf{state}_{t-1}$$

- output at time $t$:

$$\mathbf{out}_t = tanh(\mathbf{state}_{t})\odot \mathbf{o}_t$$

Due to the elementwise product $\odot$ in $[\mathbf{f}_t\odot \mathbf{state}_{t-1}, \mathbf{a}_t \odot \mathbf{i}_t, tanh(\mathbf{state}_{t})\odot \mathbf{o}_t]$, in the back propagations wrt parameters ($\mathbf{W}, \mathbf{U}, \mathbf{b}, \mathbf{x}$ in activation, forget, input and output gate), $\mathbf{state}_{t-1}, \mathbf{f}_t,  \mathbf{a}_t, \mathbf{i}_t, \mathbf{o}_t, \mathbf{state}_{t}$ will be directly copied (completely or partially depenending on the gates) to current time step rather than being involved in matrix multiplication , thus the vanishing gradient problems in RNN can be solved.

### The Back Propagation of LSTM
Let $\mathbf{z}_o, \mathbf{z}_i, \mathbf{z}_f, \mathbf{z}_a$ represents the $\mathbf{W}\mathbf{x}_t + \mathbf{U} \mathbf{out}_{t-1} + \mathbf{b}$ in $\mathbf{a}_t, \mathbf{f}_t,\mathbf{i}_t, \mathbf{o}_t$, respectively, and
let $\frac{\partial{E}}{\partial{\mathbf{out}_t}}=(\mathbf{out}_t-\mathbf{y}_t) + (\mathbf{out}_{t+1}-\mathbf{y}_{t+1})\frac{\partial{\mathbf{out}_{t+1}}}{\partial{\mathbf{out}_t}} = \mathbf{\delta}_t + \mathbf{\delta}_{t+1}\frac{\partial{\mathbf{out}_{t+1}}}{\partial{\mathbf{out}_t}}$ (the first term corresponds to the red solid line, the second term, the gradient at future time steps, corresponding to the red dotted line), then following the blue line in the flowchart, 

$$\frac{\partial{E}}{\partial{\mathbf{s}_t}}  =\frac{\partial{E}}{\partial{\mathbf{out}_t}}\odot \mathbf{o}_t\odot[1-tanh^2(\mathbf{s}_t)] + \frac{\partial{E}}{\partial{\mathbf{s}_{t+1}}}\odot\mathbf{f}_{t+1},$$
where the first term corresponding to the blue solid back propagation line, the second term comes from the blue dotted line.

The gradient of $\mathbf{s}_t$ involves the gradient of future state $\mathbf{s}_{t+1}$, when $t$ is the last time step, the second term in $\frac{\partial{E}}{\partial{\mathbf{s}_t}}$ will be $\mathbf{0}$. 

Next, 

$$\frac{\partial{E}}{\partial{\mathbf{z}_{a_t}}} = \frac{\partial{E}}{\partial{\mathbf{s}_t}}\odot\mathbf{i}_t \odot[1-tanh^2(\mathbf{z}_{a_t})],$$ 
$$\frac{\partial{E}}{\partial{\mathbf{z}_{o_t}}}=\frac{\partial{E}}{\partial{\mathbf{out}_t}}\odot tanh(\mathbf{s}_t)\odot \sigma_o\odot(1-\sigma_o),$$ 
$$\frac{\partial{E}}{\partial{\mathbf{z}_{f_t}}} =  \frac{\partial{E}}{\partial{\mathbf{s}_t}}\odot\mathbf{s}_{t-1}\odot \sigma_f\odot(1-\sigma_f),$$ 
$$\frac{\partial{E}}{\partial{\mathbf{z}_{i_t}}} = \frac{\partial{E}}{\partial{\mathbf{s}_t}}\odot\mathbf{a}_t \odot \sigma_i\odot(1-\sigma)_i.$$ 

Notice, $\frac{\partial{E}}{\partial{\mathbf{out}_t}}$ involves $\mathbf{\delta}$ at time $t$ and gradient from future time steps, $\frac{\partial{E}}{\partial{\mathbf{s}_t}}$ also involves gradient wrt $\mathbf{s}$ at time $t$ and gradient at future time steps, because from the flowchart of LSTM, each gates at time $t$ will be involved in all the future time steps. 