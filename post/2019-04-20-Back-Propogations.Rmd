---
title: Recurrent Neural Networks (RNNs)
author: ''
date: '2019-04-20'
slug: 
categories:
  - Deep Learning
tags:
  - Machine Learning
header-includes: \usepackage{amsmath} \usepackage{bm} 
---


The language model computes the probability of a sequence of previous words, $P(word_1, word_2, \dots, word_{t})$. The traditional language model is based on naive bayes model, the probability of a sequence of words is:

\begin{equation}
P(word_1, word_2, \dots, word_{t}) = \prod_{i = 1}^{t} P(word_i|word_1, \dots, word_{i-1}).
\end{equation}

The cons of traditional language models assumes independence among words (this is the naive part), which is not true in reality, it also requires a lot of RAM to compute the conditional probabilities for each word. One solution is to apply RNNs, which does not consider each input word independently, instead, RNNs can capture the information in previous sequence of words and evaluate their effects on the prediction on the current word (recurrent part). 

<figure>
  <img src="/post/2019-04-20-RNN_files/figure-html/rnn.jpg" alt="RNN" width=58% height=60%/>
  <figcaption>RNNs, source: Nature</figcaption>
</figure>