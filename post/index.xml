<?xml version="1.0" encoding="utf-8" standalone="yes" ?>



<?xml-stylesheet type="text/xsl" href="#stylesheet" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <xsl:stylesheet id="stylesheet" version="1.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform" exclude-result-prefixes="xsl">
    <xsl:output method="html" doctype-system="about:legacy-compat" />
    <xsl:template match="/rss">
      <html lang="en-us">
        <head>
          <meta name="viewport" content="width=device-width, initial-scale=1.0" />
          <title>Posts RSS | Luyao Peng&#39;s Blog</title>
          <style>body {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen-Sans, Ubuntu, Cantarell, "Helvetica Neue", sans-serif;
  line-height: 1.6;
  display: flex;
  flex-direction: column;
  min-height: 100vh;
  margin: 0;
  padding: 0;
}

main {
  flex-grow: 1;
}

img {
  max-width: 100%;
  border-radius: 0.2rem;
}

pre {
  overflow-x: auto;
  border: 0.1rem solid lightgray;
  padding: 1rem;
}

code {
  font-family: SFMono-Regular, Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.Banner {
  list-style: none;
  display: flex;
  flex-flow: row-reverse wrap-reverse;
  justify-content: space-between;
  margin: 0;
  padding: 0;
}




.Banner-item:nth-child(1) {
  order: 5;
}



.Banner-item:nth-child(2) {
  order: 4;
}



.Banner-item:nth-child(3) {
  order: 3;
}



.Banner-item:nth-child(4) {
  order: 2;
}



.Banner-item:nth-child(5) {
  order: 1;
}



.Banner-item--title {
  flex-grow: 1;
}

.Banner-link {
  font-size: 1.25rem;
  color: white;
  padding: 0.5rem 1rem;
}

.Heading {
  display: flex;
  flex-wrap: wrap;
  justify-content: space-between;
  align-items: baseline;
}

.Heading-title {
  margin: 1.5rem 0.5rem 0 0;
}

.Heading-link {
  color: inherit;
}

.Tags {
  list-style: none;
  display: flex;
  flex-wrap: wrap;
  justify-content: center;
  margin: 1.5rem 0;
  padding: 0;
}

.Tags-item {
  border-radius: 0.2rem;
  margin: 0.2rem;
  padding: 0 0.3rem;
}

.Tags-link {
  color: white;
}

.Pagination {
  font-size: 1.25rem;
  color: inherit;
}

.Pagination--right {
  float: right;
}

.Footer {
  text-align: center;
  margin: 1rem 0;
}

.u-wrapper {
  max-width: 42rem;
  margin: auto;
}

.u-padding {
  padding: 0 1rem;
}

.u-background {
  background: teal;
}

.u-clickable {
  font-weight: bold;
  text-decoration: none;
  display: inline-block;
}</style>
        </head>
        <body>
          <nav class="u-background">
  <div class="u-wrapper">
    <ul class="Banner">
      <li class="Banner-item Banner-item--title">
        <a class="Banner-link u-clickable" href="/">Luyao Peng&#39;s Blog</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="/about/">About</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="/post/">Posts</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="/tags/">Tags</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="/categories/">Categories</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="/index.xml">RSS</a>
      </li>
      
    </ul>
  </div>
</nav>
          <main>
            <div class="u-wrapper">
              <div class="u-padding">
                <h2 class="Heading-title">
                  
                  <a class="Heading-link u-clickable" href="/post/index.xml" rel="bookmark">Posts RSS</a>
                  
                </h2>
                
                <p>
                  To subscribe to this RSS feed, copy its address and paste it into your favorite feed reader.
                </p>
                
              </div>
            </div>
          </main>
          
<footer class="Footer">
  <div class="u-wrapper">
    <div class="u-padding">
      © 2019 Luyao Peng
    </div>
  </div>
</footer>


<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "pengluyaoyao" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
        </body>
      </html>
    </xsl:template>
  </xsl:stylesheet>
  <channel>
    <title>Posts on Luyao Peng&#39;s Blog</title>
    <link>/post/</link>
    <description>Recent content in Posts on Luyao Peng&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 10 Mar 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Project: Web Application for Interactive Visualization of Stock Price </title>
      <link>/post/project/</link>
      <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/project/</guid>
      <description>


&lt;p&gt;In this project, I developed a simple web application for interactive visualization of stock price using Bokeh and Flask modules in Python. Users can select any company to compare the trends of Close Price, Adjusted Close Price, Open Price and Adjusted Open Price for that selected company.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://luyao-tdi-milestone.herokuapp.com/&#34;&gt;Link to the Stock Prince app&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2015-07-23-r-rmarkdown_files/figure-html/stockprice_screenshot2.jpg&#34; /&gt;

&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2015-07-23-r-rmarkdown_files/figure-html/stockprice_screenshot.jpg&#34; /&gt;

&lt;/div&gt;
</description>
    </item>
    <item>
      <title>R Package “regrrr”: Compiling and Visualizing Regression Results</title>
      <link>/post/regrrr-one-stop-r-toolkit-for-compiling-regression-results/</link>
      <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/regrrr-one-stop-r-toolkit-for-compiling-regression-results/</guid>
      <description>


&lt;p&gt;This is an R package &lt;a href=&#34;https://CRAN.R-project.org/package=regrrr&#34; target=&#34;_blank&#34;&gt; “regrrr”&lt;/a&gt; on &lt;a href=&#34;https://cran.r-project.org&#34; target=&#34;_blank&#34;&gt;Cran&lt;/a&gt; I coauthored with Rui Yang.&lt;/p&gt;
&lt;p&gt;In strategy/management research, we always need to compile the regression results into the publishable format and sometimes plot the moderating effects. This package does the job.&lt;/p&gt;
&lt;p&gt;Here is the quickstart guide.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.rdocumentation.org/packages/regrrr&#34;&gt;&lt;img src=&#34;http://www.rdocumentation.org/badges/version/regrrr&#34; alt=&#34;Rdoc&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://cran.r-project.org/package=regrrr&#34;&gt;&lt;img src=&#34;https://cranlogs.r-pkg.org/badges/regrrr&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;installation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Installation&lt;/h1&gt;
&lt;p&gt;To install from CRAN:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;regrrr&amp;quot;)
library(regrrr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use devtools to install the latest development version:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;devtools::install_github(&amp;quot;raykyang/regrrr&amp;quot;)
library(regrrr)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Examples&lt;/h1&gt;
&lt;div id=&#34;compile-the-correlation-table&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;compile the correlation table&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(regrrr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;regrrr&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)
m0 &amp;lt;- lm(mpg ~ vs + carb + hp + wt, data = mtcars)
m1 &amp;lt;- update(m0, . ~ . + wt * hp)
m2 &amp;lt;- update(m1, . ~ . + wt * vs)
cor.table(data = m2$model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Mean  S.D.     1     2    3    4    5
## 1.mpg   20.09  6.03  1.00                     
## 2.vs     0.44  0.50  0.66  1.00               
## 3.carb   2.81  1.62 -0.55 -0.57 1.00          
## 4.hp   146.69 68.56 -0.78 -0.72 0.75 1.00     
## 5.wt     3.22  0.98 -0.87 -0.55 0.43 0.66 1.00&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compile-the-regression-table&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;compile the regression table&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;regression_table &amp;lt;- rbind(
combine_long_tab(to_long_tab(summary(m0)$coef),
                 to_long_tab(summary(m1)$coef),
                 to_long_tab(summary(m2)$coef)),
compare_models(m0, m1, m2))
rownames(regression_table) &amp;lt;- NULL
print(regression_table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        Variables   Model 0   Model 1   Model 2
## 1    (Intercept) 35.435*** 48.157*** 46.698***
## 2                  (2.503)   (4.097)   (9.272)
## 3             vs     1.353     1.077     2.171
## 4                  (1.382)   (1.152)   (6.320)
## 5           carb    -0.057    -0.043    -0.009
## 6                  (0.449)   (0.374)   (0.426)
## 7             hp   -0.024† -0.113***   -0.107*
## 8                  (0.014)   (0.027)   (0.044)
## 9             wt -3.792*** -8.071***   -7.594*
## 10                 (0.658)   (1.307)   (3.016)
## 11         hp:wt             0.027**    0.025†
## 12                           (0.008)   (0.015)
## 13         vs:wt                        -0.367
## 14                                     (2.081)
## 15     R_squared     0.833     0.889     0.889
## 16 Adj_R_squared     0.808     0.867     0.862
## 17       Delta_F            12.517**     0.031&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-moderating-effect&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;plot the moderating effect&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_effect(reg.coef = summary(m2)$coefficients, data = mtcars, model = m2,
            x_var.name = &amp;quot;wt&amp;quot;, y_var.name = &amp;quot;mpg&amp;quot;, moderator.name = &amp;quot;hp&amp;quot;,
            confidence_interval = TRUE,  CI_Ribbon = FALSE, 
            xlab = &amp;quot;Weight&amp;quot;, ylab = &amp;quot;MPG&amp;quot;, moderator.lab = &amp;quot;Horsepower&amp;quot;) +
ggplot2::theme(text=ggplot2::element_text(family=&amp;quot;Times New Roman&amp;quot;, size = 16))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-10-regrrr-one-stop-r-toolkit-for-compiling-regression-results_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see from the last line of code, the plot is customizable using &lt;a href=&#34;https://CRAN.R-project.org/package=ggplot2&#34; target=&#34;_blank&#34;&gt;“ggplot2”&lt;/a&gt;. There are a couple of other functions. Please see the &lt;a href=&#34;https://www.rdocumentation.org/packages/regrrr&#34; target=&#34;_blank&#34;&gt;reference manual on R documentation&lt;/a&gt; for details.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    <item>
      <title>Research: Dissertation</title>
      <link>/post/dissertation/</link>
      <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/dissertation/</guid>
      <description>&lt;p&gt;Abstract&lt;/p&gt;

&lt;p&gt;The closed form expressions of all unbiased estimators of variance components in
linear mixed effects models are presented. The method of moment (MOM) estimators
are chosen as benchmarks for finding the unbiased estimators having smaller estimated
variances than the corresponding MOM estimators. Examples are presented from small
area estimation.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Research: Using linear mixed-effects model to detect fraudulent erasures at an aggregate level</title>
      <link>/post/using-linear-mixed-effects-model-to-detect-fraudulent-erasures-at-an-aggregate-level/</link>
      <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/using-linear-mixed-effects-model-to-detect-fraudulent-erasures-at-an-aggregate-level/</guid>
      <description>


&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;Wollack and Eckerly (2017) extended the ‘erasure detection index’ (EDI) to detect fraudulent erasures at the group level. Sinharay (2018) suggested two modifications of the EDI at the group level. Those EDIs were not developed specifically for the fraudulent schools with small number of examinees with erasures. This paper suggested a new modification of the group-level EDI in Wollack and Eckerly (2017) by incorporating the ‘empirical best linear unbiased predictor’ (EBLUP) from the linear mixed-effects model. Simulation study shows that the suggested EDI has higher power than the indices of Wollack and Eckerly (2017) and Sinharay (2018), especially in schools with small number of examinees with erasures. It also has satisfactory Type I error rates. A real data example is also included.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Project: KPCA for one-class classification in automated essay scoring and forensic analysis</title>
      <link>/post/kpca-for-one-class-classification/</link>
      <pubDate>Thu, 07 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/kpca-for-one-class-classification/</guid>
      <description>


&lt;p&gt;One-class classification problem is useful in many applications. For example, if people want to diagnoze the healthy condition of machines, measurements on the normal operation of the machine are easy to obtain, and most faults will not haveoccurred so one will have little or no training data for the negative class. Another example is web security detection, people only have data for normal web behavior , since once abnormal behavior occurs, the web security will be attacked, which is a situation people try to prevent from happening.&lt;/p&gt;
&lt;p&gt;The challenge of one-class classification problem is that people only have the data for normal class to train and to decide whether a new observation is alike or not. In this project, we used Kernel Principal Component Analysis (KPCA) and Support Vector Machine (SVM) to detect cheating behavior in automated essay scoring and forensic analysis when only the data for normal essays are available.&lt;/p&gt;
&lt;p&gt;In automated essay scoring, an abnormal essay can be gibberish, copying, writing in foreign language etc. We train the KPCA and SVM models using normal essay data for argumentative essays to identify whether a new essay is similar to normal essay or not, if a new essay is not similar to the normal essay in terms of KPCA score, we consider it as an abnormal essay.&lt;/p&gt;
&lt;p&gt;Similarly, in erasure analysis, we extraced the features of the pattern of pencil bubbles for each student, which are the training data of normal bubbles, then we randomly sample the pencil bubbles from another student as the new observations and compare the features of the new pencil bubbles with that of the normal bubbles to see how many new pencil bubbles can be classified as abnormal bubbles using KPCA and SVM models.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2015-07-23-r-rmarkdown_files/figure-html/kpca1.jpg&#34; /&gt; &lt;img src=&#34;/post/2015-07-23-r-rmarkdown_files/figure-html/kpca2.jpg&#34; /&gt; &lt;a href=&#34;https://kpca-outlier-detection.shinyapps.io/Shiny/&#34;&gt;Link to the KPCA app&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Project: NLP in Automated Essay Scoring</title>
      <link>/post/nlp-in-automated-essay-scoring/</link>
      <pubDate>Thu, 07 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/nlp-in-automated-essay-scoring/</guid>
      <description>


&lt;p&gt;In The Data Incubator, my capstone project was NLP in automated essay scoring, which was completed in Nov,2018, at that time, I haven’t learned the techniques of NLP with deep learning, so this project only involves basic NLP methods such as BOW, n-gram and tree-based boosting model.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://easygrader.herokuapp.com/&#34;&gt;Link to the MMeM github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(The heroku platform only allows 30s response time for any request, sometimes the app takes takes more than 30s to respond due to an external API, please try multiple times to get the results. This is what I need to improve to add some ‘backend worker’ in Python to avoid the situation)&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;About this project&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Essay are crucial testing tools for assessing academic achievement, integration of ideas and ability, but are expensive and time consuming to grade manually. Automated Essay Scoring (AES) saves the efforts of human graders and hence significantly reduces costs and time.&lt;/p&gt;
&lt;p&gt;The essay data are from Kaggle.com, by the William and Flora Hewlett Foundation, containing 8 essay categories. For each essay, there are two human graders to grade the essay. The goal is to train the models to grade the essays as close as to the human graders.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Features&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is traditional NLP method, so input features were extraced manually, they are: + Bag of Words (BOW) + Number of words + Number of sentences + Variation of sentences + Average word length + Number of lemms + Number of spelling errors + Number of nouns, adjectives, verbs and adverbs&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Model and results&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Four regression models are compared: + random forest + ridge regression + lasson regression + gradient boosting regressor Gradient boosting regressor performed better in terms of Variance Explained metric, so gradi- ent boosting was used to make prediction on the validate essay set for set1 and set 5, respectively.&lt;/p&gt;
&lt;p&gt;After applying the early stopping, the results of gradient boosting regressor of the 3-fold cross validation results for essay set1 and essay set5 are (the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; for other categories are 0.5-0.6, which is relatively low):&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2015-07-23-r-rmarkdown_files/figure-html/results_AES_TDI.jpg&#34; /&gt;

&lt;/div&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Problems&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model has overfitting problem, need to tune the parameters further; change the website waiting mode when requesting.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>R Package “MMeM”: Multivariate Mixed-effects Model</title>
      <link>/post/multivariate-mixed-effects-model-r-package/</link>
      <pubDate>Thu, 07 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/multivariate-mixed-effects-model-r-package/</guid>
      <description>


&lt;p&gt;This package estimates the variance covariance components for data under multivariate mixed effects model using multivariate REML and multivariate Henderson3 methods. See Meyer (1985) &lt;a href=&#34;doi:10.2307/2530651&#34; class=&#34;uri&#34;&gt;doi:10.2307/2530651&lt;/a&gt; and Wesolowska Janczarek (1984) &lt;a href=&#34;doi:10.1002/bimj.4710260613&#34; class=&#34;uri&#34;&gt;doi:10.1002/bimj.4710260613&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It is available on CRAN and my github page:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pengluyaoyao/MMeM&#34;&gt;Link to the MMeM github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/MMeM/index.html&#34;&gt;Link to the MMeM CRAN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The package supports the variance covariance component estimations for the multivariate mixed effects model for one-way randomized block design with equal design matrices:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y} = (\mathbf{I} \otimes \mathbf{X})\mathbf{b} + (\mathbf{I} \otimes \mathbf{Z} )\mathbf{u} + \mathbf{e}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y} = \left\{\mathbf{y}_i\right\}_c, \mathbf{b} = \left\{\mathbf{b}_i\right\}_c, \mathbf{u} = \left\{\mathbf{u}_i\right\}_c, \mathbf{e} = \left\{\mathbf{e}_i\right\}_c, i =1, \dots, q\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y}\)&lt;/span&gt; is n&lt;em&gt;q by 1 response vector; &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt; is n by p design matrix for the fixed effects; &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{b}\)&lt;/span&gt; is p&lt;/em&gt;q by 1 coefficients vector for the fixed effects; &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Z}\)&lt;/span&gt; is n by s design matrix for the random effects; &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}\)&lt;/span&gt; is s&lt;em&gt;q by 1 vector of the random effects; &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{e}\)&lt;/span&gt; is n&lt;/em&gt;q by 1 vector of random errors.&lt;/p&gt;
&lt;p&gt;It is assumed that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(cov(\mathbf{u}_i, \mathbf{u}_j&amp;#39;) = \mathbf{G}_{ij}, cov(\mathbf{e}_i, \mathbf{e}_j&amp;#39;) = \mathbf{R}_{ij}, cov(\mathbf{u}_i, \mathbf{e}_j&amp;#39;) = \mathbf{0}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(var(\mathbf{u}) = \mathbf{G} = \mathbf{T}\otimes \mathbf{I}_s, var(\mathbf{e}) = \mathbf{R} = \mathbf{E} \otimes \mathbf{I}_n, var(\mathbf{y}) = \mathbf{V} = \mathbf{T}\otimes \mathbf{ZZ}&amp;#39; + \mathbf{E} \otimes \mathbf{I}_n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The goal is to estimate the variance-covariance matrix of random effects &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{e}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; response variates, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{E}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The model also supports the variance component estimations using univariate mixed-effects model regression, which is equivalent to R package lme4:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y} = \mathbf{Xb} + \mathbf{Zu} + \mathbf{e}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y}\)&lt;/span&gt; is n by 1 response vector; &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Z}\)&lt;/span&gt; are n by p and n by s known design matrix; &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{b}\)&lt;/span&gt; is p by 1 coefficients vector for the fixed effects; &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}\)&lt;/span&gt; is s by 1 matrix for the random effects, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{e}\)&lt;/span&gt; is n by 1 vector of random errors. }&lt;/p&gt;
&lt;p&gt;The goal is to estimate &lt;span class=&#34;math inline&#34;&gt;\(\sigma_u^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_e^2\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u} \sim N(\mathbf{0}, \sigma_u^2\mathbf{I}), \mathbf{e} \sim N(\mathbf{0}, \sigma_e^2\mathbf{I})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Currently this package supports multivariate mixed effects model with two response variables, one fixed effects and one random effects, the package only estimates the variance covariance components under multivariate mixed effects model, it doesn’t provide the estimation and hypothesis testing for the fixed effects parameters, which will be incorporated in the future. Users can compute the estimated fixed effects parameters by using the generalized least square formula for the fixed effect parameters after estimating the variance covariance components using this package.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>NLP with Deep Learning: CS224n A2Q2</title>
      <link>/post/assignment2/</link>
      <pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/assignment2/</guid>
      <description>


&lt;p&gt;This blog is a detailed annotation of the python code for cs224n Assignment#2 Q2. The goal of this assignment is to build a three layer neural network using TensorFlow to parse the dependency structure of sentences. The orignal code is for Python2, I revised some of them so that it runs in Python3. The training data for the neural dependency parser model is Penn Treebank, each sentence has ‘word’, ‘POS’, ‘head’ and ‘label’, where the ‘head’ is the correct dependency relation. Some code still puzzles me, so I left question mark as comment and will remove them if I understand them later.&lt;/p&gt;
&lt;p&gt;First, import necessary modules and set up the configurations for the parser:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import time
import os
import logging
from collections import Counter
import numpy as np

P_PREFIX = &amp;#39;&amp;lt;p&amp;gt;:&amp;#39;
L_PREFIX = &amp;#39;&amp;lt;l&amp;gt;:&amp;#39;
UNK = &amp;#39;&amp;lt;UNK&amp;gt;&amp;#39;
NULL = &amp;#39;&amp;lt;NULL&amp;gt;&amp;#39;
ROOT = &amp;#39;&amp;lt;ROOT&amp;gt;&amp;#39;

class Config(object): 
    language = &amp;#39;english&amp;#39;
    with_punct = True
    unlabeled = True 
    lowercase = True
    use_pos = True # use part of speech as input features
    use_dep = True #?
    use_dep = use_dep and (not unlabeled)
    data_path = &amp;#39;./data&amp;#39;
    train_file = &amp;#39;train.conll&amp;#39;
    dev_file = &amp;#39;dev.conll&amp;#39;
    test_file = &amp;#39;test.conll&amp;#39;
    embedding_file = &amp;#39;./data/en-cw.txt&amp;#39;   
    n_features = 36 #?
    n_classes = 3 # want to predict shift, left-arc or right-arc
    dropout = 0.5
    embed_size = 50
    hidden_size = 200
    batch_size = 2048
    n_epochs = 10
    lr = 0.001&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define some utility functions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# read the training data in:
# The first 5 rows in the training data:
&amp;#39;&amp;#39;&amp;#39;
1   In  _   ADP IN  _   5   case    _   _
2   an  _   DET DT  _   5   det _   _
3   Oct.    _   PROPN   NNP _   5   compound    _   _
4   19  _   NUM CD  _   5   nummod  _   _
5   review  _   NOUN    NN  _   45  nmod    _   _
&amp;#39;&amp;#39;&amp;#39;

def read_conll(in_file, lowercase=False, max_example=None):
    examples = []
    with open(in_file) as f:
        word, pos, head, label = [], [], [], []
        for line in f.readlines():
            sp = line.strip().split(&amp;#39;\t&amp;#39;) #tokenize each line of the file
            if len(sp) == 10:
                if &amp;#39;-&amp;#39; not in sp[0]:  #that means the sentence is not complete, continue accumulate words in the sentence
                    word.append(sp[1].lower() if lowercase else sp[1])
                    pos.append(sp[4])
                    head.append(int(sp[6]))
                    label.append(sp[7])
            elif len(word) &amp;gt; 0: #accumulate to a dictionary
                examples.append({&amp;#39;word&amp;#39;: word, &amp;#39;pos&amp;#39;: pos, &amp;#39;head&amp;#39;: head, &amp;#39;label&amp;#39;: label})
                word, pos, head, label = [], [], [], []
                if (max_example is not None) and (len(examples) == max_example):
                    break
        if len(word) &amp;gt; 0: #if the sentence has &amp;gt;0 words, accumulate the sentence dictionary to a list called examples.
            examples.append({&amp;#39;word&amp;#39;: word, &amp;#39;pos&amp;#39;: pos, &amp;#39;head&amp;#39;: head, &amp;#39;label&amp;#39;: label})
    return examples&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;def build_dict(keys, n_max=None, offset=0):
    count = Counter()
    for key in keys:
        count[key] += 1
    ls = count.most_common() if n_max is None \
        else count.most_common(n_max)

    return {w[0]: index + offset for (index, w) in enumerate(ls)}

def punct(language, pos):
    if language == &amp;#39;english&amp;#39;:
        return pos in [&amp;quot;&amp;#39;&amp;#39;&amp;quot;, &amp;quot;,&amp;quot;, &amp;quot;.&amp;quot;, &amp;quot;:&amp;quot;, &amp;quot;``&amp;quot;, &amp;quot;-LRB-&amp;quot;, &amp;quot;-RRB-&amp;quot;]
    elif language == &amp;#39;chinese&amp;#39;:
        return pos == &amp;#39;PU&amp;#39;
    elif language == &amp;#39;french&amp;#39;:
        return pos == &amp;#39;PUNC&amp;#39;
    elif language == &amp;#39;german&amp;#39;:
        return pos in [&amp;quot;$.&amp;quot;, &amp;quot;$,&amp;quot;, &amp;quot;$[&amp;quot;]
    elif language == &amp;#39;spanish&amp;#39;:
        # http://nlp.stanford.edu/software/spanish-faq.shtml
        return pos in [&amp;quot;f0&amp;quot;, &amp;quot;faa&amp;quot;, &amp;quot;fat&amp;quot;, &amp;quot;fc&amp;quot;, &amp;quot;fd&amp;quot;, &amp;quot;fe&amp;quot;, &amp;quot;fg&amp;quot;, &amp;quot;fh&amp;quot;,
                       &amp;quot;fia&amp;quot;, &amp;quot;fit&amp;quot;, &amp;quot;fp&amp;quot;, &amp;quot;fpa&amp;quot;, &amp;quot;fpt&amp;quot;, &amp;quot;fs&amp;quot;, &amp;quot;ft&amp;quot;,
                       &amp;quot;fx&amp;quot;, &amp;quot;fz&amp;quot;]
    elif language == &amp;#39;universal&amp;#39;:
        return pos == &amp;#39;PUNCT&amp;#39;
    else:
        raise ValueError(&amp;#39;language: %s is not supported.&amp;#39; % language)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, define Preprocessor object (in the assignment it is called Parser. I was confused about the parse method in that object, so I take the parse method out as an independent method, so the Parser object was renamed as Preprocessor, which will be used as an argument in the parse() method) to extract features of the context words of each word in the training data (word, pos, label features), get the correct transitions, get the legal transitions , then create instances containing [a long list of tuples of (features, legal labels, gold_t (correct transitions))]&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class Preprocessor(object):
    # initialize the dictionary {token: id}, and {id: token},  
    def __init__(self, dataset):
        root_labels = list([l for ex in dataset
                           for (h, l) in zip(ex[&amp;#39;head&amp;#39;], ex[&amp;#39;label&amp;#39;]) if h == 0]) #h==0 is root
        counter = Counter(root_labels)
        if len(counter) &amp;gt; 1:
            logging.info(&amp;#39;Warning: more than one root label&amp;#39;)
            logging.info(counter)
        self.root_label = counter.most_common()[0][0]
        deprel = [self.root_label] + list(set([w for ex in dataset
                                               for w in ex[&amp;#39;label&amp;#39;]
                                               if w != self.root_label]))
        &amp;#39;&amp;#39;&amp;#39;
        the deprel (dependent relations) are:
        
        &amp;#39;&amp;#39;&amp;#39;
        
        tok2id = {L_PREFIX + l: i for (i, l) in enumerate(deprel)}
        tok2id[L_PREFIX + NULL] = self.L_NULL = len(tok2id)

        config = Config()
        self.unlabeled = config.unlabeled
        self.with_punct = config.with_punct
        self.use_pos = config.use_pos
        self.use_dep = config.use_dep
        self.language = config.language

        if self.unlabeled: #just S, LA, RA
            trans = [&amp;#39;L&amp;#39;, &amp;#39;R&amp;#39;, &amp;#39;S&amp;#39;]
            self.n_deprel = 1
        else: #S_NN
            trans = [&amp;#39;L-&amp;#39; + l for l in deprel] + [&amp;#39;R-&amp;#39; + l for l in deprel] + [&amp;#39;S&amp;#39;]
            self.n_deprel = len(deprel)

        self.n_trans = len(trans)
        self.tran2id = {t: i for (i, t) in enumerate(trans)}
        self.id2tran = {i: t for (i, t) in enumerate(trans)}
        
        &amp;#39;&amp;#39;&amp;#39;
        example of id2tran is:
        {0: &amp;#39;L&amp;#39;, 1: &amp;#39;R&amp;#39;, 2: &amp;#39;S&amp;#39;}
        &amp;#39;&amp;#39;&amp;#39;
        # logging.info(&amp;#39;Build dictionary for part-of-speech tags.&amp;#39;)
        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex[&amp;#39;pos&amp;#39;]],
                                  offset=len(tok2id)))
        tok2id[P_PREFIX + UNK] = self.P_UNK = len(tok2id)
        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)
        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)

        # logging.info(&amp;#39;Build dictionary for words.&amp;#39;)
        tok2id.update(build_dict([w for ex in dataset for w in ex[&amp;#39;word&amp;#39;]],
                                  offset=len(tok2id)))
        tok2id[UNK] = self.UNK = len(tok2id)
        tok2id[NULL] = self.NULL = len(tok2id)
        tok2id[ROOT] = self.ROOT = len(tok2id)
        
        &amp;#39;&amp;#39;&amp;#39;
        example of id2tok is:
        {0: &amp;#39;&amp;lt;l&amp;gt;:root&amp;#39;, 1: &amp;#39;&amp;lt;l&amp;gt;:nsubj&amp;#39;, 2: &amp;#39;&amp;lt;l&amp;gt;:advmod&amp;#39;, 3: &amp;#39;&amp;lt;l&amp;gt;:dobj&amp;#39;, 4: &amp;#39;&amp;lt;l&amp;gt;:acl&amp;#39;, 5: &amp;#39;&amp;lt;l&amp;gt;:nummod&amp;#39;, 6: &amp;#39;&amp;lt;l&amp;gt;:nmod&amp;#39;, 7: &amp;#39;&amp;lt;l&amp;gt;:compound&amp;#39;, 8: &amp;#39;&amp;lt;l&amp;gt;:xcomp&amp;#39;, 
        9: &amp;#39;&amp;lt;l&amp;gt;:auxpass&amp;#39;, 10: &amp;#39;&amp;lt;l&amp;gt;:det&amp;#39;, 11: &amp;#39;&amp;lt;l&amp;gt;:punct&amp;#39;, 12: &amp;#39;&amp;lt;l&amp;gt;:mark&amp;#39;, 13: &amp;#39;&amp;lt;l&amp;gt;:case&amp;#39;, 14: &amp;#39;&amp;lt;l&amp;gt;:conj&amp;#39;, 15: &amp;#39;&amp;lt;l&amp;gt;:appos&amp;#39;, 16: &amp;#39;&amp;lt;l&amp;gt;:nmod:tmod&amp;#39;, 
        17: &amp;#39;&amp;lt;l&amp;gt;:dep&amp;#39;, 18: &amp;#39;&amp;lt;l&amp;gt;:amod&amp;#39;, 19: &amp;#39;&amp;lt;l&amp;gt;:nmod:poss&amp;#39;, 20: &amp;#39;&amp;lt;l&amp;gt;:nsubjpass&amp;#39;, 21: &amp;#39;&amp;lt;l&amp;gt;:cc&amp;#39;, 22: &amp;#39;&amp;lt;l&amp;gt;:ccomp&amp;#39;, 23: &amp;#39;&amp;lt;l&amp;gt;:&amp;lt;NULL&amp;gt;&amp;#39;, 24: &amp;#39;&amp;lt;p&amp;gt;:NNP&amp;#39;, 
        25: &amp;#39;&amp;lt;p&amp;gt;:NN&amp;#39;, 26: &amp;#39;&amp;lt;p&amp;gt;:IN&amp;#39;, 27: &amp;#39;&amp;lt;p&amp;gt;:DT&amp;#39;, 28: &amp;#39;&amp;lt;p&amp;gt;:,&amp;#39;, 29: &amp;#39;&amp;lt;p&amp;gt;:NNS&amp;#39;, 30: &amp;#39;&amp;lt;p&amp;gt;:JJ&amp;#39;, 31: &amp;#39;&amp;lt;p&amp;gt;:CD&amp;#39;, 32: &amp;#39;&amp;lt;p&amp;gt;:CC&amp;#39;, 33: &amp;#39;&amp;lt;p&amp;gt;:VBD&amp;#39;, 34: &amp;#39;&amp;lt;p&amp;gt;:.&amp;#39;, 
        35: &amp;#39;&amp;lt;p&amp;gt;:VBN&amp;#39;, 36: &amp;#39;&amp;lt;p&amp;gt;:VBZ&amp;#39;, 37: &amp;#39;&amp;lt;p&amp;gt;:``&amp;#39;, 38: &amp;quot;&amp;lt;p&amp;gt;:&amp;#39;&amp;#39;&amp;quot;, 39: &amp;#39;&amp;lt;p&amp;gt;:VB&amp;#39;, 40: &amp;#39;&amp;lt;p&amp;gt;:TO&amp;#39;, 41: &amp;#39;&amp;lt;p&amp;gt;:PRP&amp;#39;, 42: &amp;#39;&amp;lt;p&amp;gt;:POS&amp;#39;, 43: &amp;#39;&amp;lt;p&amp;gt;:-LRB-&amp;#39;, 
        44: &amp;#39;&amp;lt;p&amp;gt;:-RRB-&amp;#39;, 45: &amp;#39;&amp;lt;p&amp;gt;:RB&amp;#39;, 46: &amp;#39;&amp;lt;p&amp;gt;:NNPS&amp;#39;, 47: &amp;#39;&amp;lt;p&amp;gt;:PRP$&amp;#39;, 48: &amp;#39;&amp;lt;p&amp;gt;:&amp;lt;UNK&amp;gt;&amp;#39;, 49: &amp;#39;&amp;lt;p&amp;gt;:&amp;lt;NULL&amp;gt;&amp;#39;, 50: &amp;#39;&amp;lt;p&amp;gt;:&amp;lt;ROOT&amp;gt;&amp;#39;, 51: &amp;#39;,&amp;#39;, 52: &amp;#39;in&amp;#39;, 53: &amp;#39;the&amp;#39;, 
        54: &amp;#39;.&amp;#39;, 55: &amp;#39;cars&amp;#39;, 56: &amp;#39;and&amp;#39;, 57: &amp;#39;of&amp;#39;, 58: &amp;#39;``&amp;#39;, 59: &amp;quot;&amp;#39;&amp;#39;&amp;quot;, 60: &amp;#39;at&amp;#39;, 61: &amp;#39;to&amp;#39;, 62: &amp;#39;haag&amp;#39;, 63: &amp;#39;said&amp;#39;, 64: &amp;#39;u.s.&amp;#39;, 65: &amp;#39;luxury&amp;#39;, 66: &amp;#39;auto&amp;#39;, 
        67: &amp;#39;maker&amp;#39;, 68: &amp;#39;an&amp;#39;, 69: &amp;#39;oct.&amp;#39;, 70: &amp;#39;19&amp;#39;, 71: &amp;#39;review&amp;#39;, 72: &amp;#39;misanthrope&amp;#39;, 73: &amp;#39;chicago&amp;#39;, 74: &amp;quot;&amp;#39;s&amp;quot;, 75: &amp;#39;goodman&amp;#39;, 76: &amp;#39;theatre&amp;#39;, 77: &amp;#39;-lrb-&amp;#39;, 
        78: &amp;#39;revitalized&amp;#39;, 79: &amp;#39;classics&amp;#39;, 80: &amp;#39;take&amp;#39;, 81: &amp;#39;stage&amp;#39;, 82: &amp;#39;windy&amp;#39;, 83: &amp;#39;city&amp;#39;, 84: &amp;#39;leisure&amp;#39;, 85: &amp;#39;&amp;amp;&amp;#39;, 86: &amp;#39;arts&amp;#39;, 87: &amp;#39;-rrb-&amp;#39;, 88: &amp;#39;role&amp;#39;,
        89: &amp;#39;celimene&amp;#39;, 90: &amp;#39;played&amp;#39;, 91: &amp;#39;by&amp;#39;, 92: &amp;#39;kim&amp;#39;, 93: &amp;#39;cattrall&amp;#39;, 94: &amp;#39;was&amp;#39;, 95: &amp;#39;mistakenly&amp;#39;, 96: &amp;#39;attributed&amp;#39;, 97: &amp;#39;christina&amp;#39;, 98: &amp;#39;ms.&amp;#39;, 
        99: &amp;#39;plays&amp;#39;, 100: &amp;#39;elianti&amp;#39;, 101: &amp;#39;rolls-royce&amp;#39;, 102: &amp;#39;motor&amp;#39;, 103: &amp;#39;inc.&amp;#39;, 104: &amp;#39;it&amp;#39;, 105: &amp;#39;expects&amp;#39;, 106: &amp;#39;its&amp;#39;, 107: &amp;#39;sales&amp;#39;, 108: &amp;#39;remain&amp;#39;, 
        109: &amp;#39;steady&amp;#39;, 110: &amp;#39;about&amp;#39;, 111: &amp;#39;1,200&amp;#39;, 112: &amp;#39;1990&amp;#39;, 113: &amp;#39;last&amp;#39;, 114: &amp;#39;year&amp;#39;, 115: &amp;#39;sold&amp;#39;, 116: &amp;#39;1,214&amp;#39;, 117: &amp;#39;howard&amp;#39;, 118: &amp;#39;mosher&amp;#39;, 
        119: &amp;#39;president&amp;#39;, 120: &amp;#39;chief&amp;#39;, 121: &amp;#39;executive&amp;#39;, 122: &amp;#39;officer&amp;#39;, 123: &amp;#39;he&amp;#39;, 124: &amp;#39;anticipates&amp;#39;, 125: &amp;#39;growth&amp;#39;, 126: &amp;#39;for&amp;#39;, 127: &amp;#39;britain&amp;#39;, 
        128: &amp;#39;europe&amp;#39;, 129: &amp;#39;far&amp;#39;, 130: &amp;#39;eastern&amp;#39;, 131: &amp;#39;markets&amp;#39;, 132: &amp;#39;&amp;lt;UNK&amp;gt;&amp;#39;, 133: &amp;#39;&amp;lt;NULL&amp;gt;&amp;#39;, 134: &amp;#39;&amp;lt;ROOT&amp;gt;&amp;#39;}
        &amp;#39;&amp;#39;&amp;#39;
        self.tok2id = tok2id
        self.id2tok = {v: k for (k, v) in tok2id.items()}

        self.n_features = 18 + (18 if config.use_pos else 0) + (12 if config.use_dep else 0) #? why 18?
        self.n_tokens = len(tok2id) 
    
    &amp;#39;&amp;#39;&amp;#39;
    arranging each set as this form: [{&amp;#39;word&amp;#39;: [corresponding id of each token], &amp;#39;pos&amp;#39;: [corresponding id in pos from tok2id], &amp;#39;head&amp;#39;: [id...], &amp;#39;label&amp;#39;: [id...]},...,      #{last sentence in the set}]
        The resulting vec_examples are (this is vectorized training data in id form from tok2id):
        [{&amp;#39;word&amp;#39;: [134, 52, 68, 69, 70, 71, 57, 58, 53, 72, 59, 60, 73, 74, 75, 76, 77, 58, 78, 79, 80, 53, 81, 52, 82, 83, 51, 59, 84, 85, 86, 87, 51, 53, 88, 57, 89, 51, 90, 91, 92, 93, 51, 94, 95, 96, 61, 97, 62, 54], 
        &amp;#39;pos&amp;#39;: [50, 26, 27, 24, 31, 25, 26, 37, 27, 25, 38, 26, 24, 42, 24, 24, 43, 37, 35, 29, 39, 27, 25, 26, 24, 24, 28, 38, 25, 32, 29, 44, 28, 27, 25, 26, 24, 28, 35, 26, 24, 24, 28, 33, 45, 35, 40, 24, 24, 34], 
        &amp;#39;head&amp;#39;: [-1, 5, 5, 5, 5, 45, 9, 9, 9, 5, 9, 15, 15, 12, 15, 9, 20, 20, 19, 20, 5, 22, 20, 25, 25, 20, 20, 20, 20, 28, 28, 20, 45, 34, 45, 36, 34, 34, 34, 41, 41, 38, 34, 45, 45, 0, 48, 48, 45, 45], 
        &amp;#39;label&amp;#39;: [-1, 13, 10, 7, 5, 6, 13, 11, 10, 6, 11, 13, 19, 13, 7, 6, 11, 11, 18, 1, 17, 10, 3, 13, 7, 6, 11, 11, 17, 21, 14, 11, 11, 10, 20, 13, 6, 11, 4, 13, 7, 6, 11, 9, 2, 0, 13, 7, 6, 11]}, 
        {&amp;#39;word&amp;#39;: [134, 98, 62, 99, 100, 54], &amp;#39;pos&amp;#39;: [50, 24, 24, 36, 24, 34], &amp;#39;head&amp;#39;: [-1, 2, 3, 0, 3, 3], &amp;#39;label&amp;#39;: [-1, 7, 1, 0, 3, 11]},{...}...{...}
    &amp;#39;&amp;#39;&amp;#39;
    def vectorize(self, examples): 
        vec_examples = []
        for ex in examples:
            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id
                                  else self.UNK for w in ex[&amp;#39;word&amp;#39;]] # a list of word id for each sentence in dataset
            pos = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id # a list of POS id
                                   else self.P_UNK for w in ex[&amp;#39;pos&amp;#39;]]
            head = [-1] + ex[&amp;#39;head&amp;#39;]
            label = [-1] + [self.tok2id[L_PREFIX + w] if L_PREFIX + w in self.tok2id
                            else -1 for w in ex[&amp;#39;label&amp;#39;]]
            vec_examples.append({&amp;#39;word&amp;#39;: word, &amp;#39;pos&amp;#39;: pos,
                                 &amp;#39;head&amp;#39;: head, &amp;#39;label&amp;#39;: label})
        return vec_examples # each sentence is a dictionary with list of &amp;#39;word&amp;#39;,&amp;#39;POS&amp;#39;, &amp;#39;head&amp;#39; and &amp;#39;label
    
    &amp;#39;&amp;#39;&amp;#39;
    This function is important, it extracts context words first, then extract the corresping word from ex[&amp;#39;word&amp;#39;], pos from ex[&amp;#39;pos&amp;#39;]
    and label features from ex[&amp;#39;label&amp;#39;], finally concatenate them into a long vector for each sentence, an example of one set of features for one sentence is:
    [133, 133, 134, 52, 68, 69, 133, 133, 133, 133, 133, 133, 133, 133, 133, 133, 133, 133, 49, 49, 50, 26, 27, 24, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49] #36 features of context words with corresponding id from tok2id,  that is a long list containing [features, p_features, l_features] 
    &amp;#39;&amp;#39;&amp;#39;
    def extract_features(self, stack, buf, arcs, ex):
        if stack[0] == &amp;quot;ROOT&amp;quot;:
            stack[0] = 0

        def get_lc(k): # lc: left context, since arc = (stack[-1], stack[-2], gold_t) or (stack[-2], stack[-1], gold_t), if arc[1]&amp;#39;s id &amp;lt; arc[0]&amp;#39;s id, then extract lc
            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] &amp;lt; k])

        def get_rc(k): #if arc[1]&amp;#39;s id &amp;gt; arc[0]&amp;#39;s id, rc
            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] &amp;gt; k],
                          reverse=True)

        p_features = []
        l_features = []
        features = [self.NULL] * (3 - len(stack)) + [ex[&amp;#39;word&amp;#39;][x] for x in stack[-3:]] #if len(stack)&amp;gt;=3, no NULL, features contain the last 3 words in the stack
        features += [ex[&amp;#39;word&amp;#39;][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf)) # extract the first 3 words in the buffer if len(buf) &amp;gt;=3
        if self.use_pos: # extracting the corresponding pos features, 3 from stack, 3 from buffer if stack and buffer have length&amp;gt;=3
            p_features = [self.P_NULL] * (3 - len(stack)) + [ex[&amp;#39;pos&amp;#39;][x] for x in stack[-3:]]
            p_features += [ex[&amp;#39;pos&amp;#39;][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))

        for i in range(2): #i = 0, 1
            if i &amp;lt; len(stack):
                k = stack[-i-1] #the kth word&amp;#39;s context: i=0, the -1 word&amp;#39;s context, i=1, the -2 word&amp;#39;s context
                lc = get_lc(k) # the left context word of kth word in stack = [(s3,) s2, s1]
                rc = get_rc(k) # the right context word
                llc = get_lc(lc[0]) if len(lc) &amp;gt; 0 else [] #llc
                rrc = get_rc(rc[0]) if len(rc) &amp;gt; 0 else [] #rrc

                features.append(ex[&amp;#39;word&amp;#39;][lc[0]] if len(lc) &amp;gt; 0 else self.NULL)
                features.append(ex[&amp;#39;word&amp;#39;][rc[0]] if len(rc) &amp;gt; 0 else self.NULL)
                features.append(ex[&amp;#39;word&amp;#39;][lc[1]] if len(lc) &amp;gt; 1 else self.NULL)
                features.append(ex[&amp;#39;word&amp;#39;][rc[1]] if len(rc) &amp;gt; 1 else self.NULL)
                features.append(ex[&amp;#39;word&amp;#39;][llc[0]] if len(llc) &amp;gt; 0 else self.NULL)
                features.append(ex[&amp;#39;word&amp;#39;][rrc[0]] if len(rrc) &amp;gt; 0 else self.NULL)

                if self.use_pos:
                    p_features.append(ex[&amp;#39;pos&amp;#39;][lc[0]] if len(lc) &amp;gt; 0 else self.P_NULL)
                    p_features.append(ex[&amp;#39;pos&amp;#39;][rc[0]] if len(rc) &amp;gt; 0 else self.P_NULL)
                    p_features.append(ex[&amp;#39;pos&amp;#39;][lc[1]] if len(lc) &amp;gt; 1 else self.P_NULL)
                    p_features.append(ex[&amp;#39;pos&amp;#39;][rc[1]] if len(rc) &amp;gt; 1 else self.P_NULL)
                    p_features.append(ex[&amp;#39;pos&amp;#39;][llc[0]] if len(llc) &amp;gt; 0 else self.P_NULL)
                    p_features.append(ex[&amp;#39;pos&amp;#39;][rrc[0]] if len(rrc) &amp;gt; 0 else self.P_NULL)

                if self.use_dep:
                    l_features.append(ex[&amp;#39;label&amp;#39;][lc[0]] if len(lc) &amp;gt; 0 else self.L_NULL)
                    l_features.append(ex[&amp;#39;label&amp;#39;][rc[0]] if len(rc) &amp;gt; 0 else self.L_NULL)
                    l_features.append(ex[&amp;#39;label&amp;#39;][lc[1]] if len(lc) &amp;gt; 1 else self.L_NULL)
                    l_features.append(ex[&amp;#39;label&amp;#39;][rc[1]] if len(rc) &amp;gt; 1 else self.L_NULL)
                    l_features.append(ex[&amp;#39;label&amp;#39;][llc[0]] if len(llc) &amp;gt; 0 else self.L_NULL)
                    l_features.append(ex[&amp;#39;label&amp;#39;][rrc[0]] if len(rrc) &amp;gt; 0 else self.L_NULL)
            else:
                features += [self.NULL] * 6
                if self.use_pos:
                    p_features += [self.P_NULL] * 6
                if self.use_dep:
                    l_features += [self.L_NULL] * 6

        features += p_features + l_features
        assert len(features) == self.n_features
        return features 

    &amp;#39;&amp;#39;&amp;#39;
    This is to get the correct transitions for training purposes. 
    1. if there is only ROOT in stack, the only correct transition is shift
    2. if stack = [i1, i0] or more, i2 is not ROOT, i1&amp;#39;s head == i0, then left arc is the correct transition
    3. if stack = [i1, i0] or more, i0&amp;#39;s head == i1, i1 can be ROOT or not ROOT, right arc is the correct transition
    4. ow shift
    &amp;#39;&amp;#39;&amp;#39;
    def get_oracle(self, stack, buf, ex):
        if len(stack) &amp;lt; 2:
            return self.n_trans - 1 #如果stack只有root, 就执行shift, shift对应id是48 or 2: n_tran = 3-1 if unlabel or 49-1  if label1
        i0 = stack[-1]
        i1 = stack[-2]
        h0 = ex[&amp;#39;head&amp;#39;][i0]
        h1 = ex[&amp;#39;head&amp;#39;][i1]
        l0 = ex[&amp;#39;label&amp;#39;][i0]
        l1 = ex[&amp;#39;label&amp;#39;][i1]

        if self.unlabeled:
            if (i1 &amp;gt; 0) and (h1 == i0): #if i1 is not ROOT, stack[i1, i0], head is i0, left_arc i1
                return 0
            elif (i1 &amp;gt;= 0) and (h0 == i1) and (not any([x for x in buf if ex[&amp;#39;head&amp;#39;][x] == i0])): 
                return 1 # if head is i1, no i0 head in buffer , right_arc
            else:
                return None if len(buf) == 0 else 2 #if len(buf) =0, no transition, ow, shift
        else:
            if (i1 &amp;gt; 0) and (h1 == i0):
                return l1 if (l1 &amp;gt;= 0) and (l1 &amp;lt; self.n_deprel) else None
            elif (i1 &amp;gt;= 0) and (h0 == i1) and \
                 (not any([x for x in buf if ex[&amp;#39;head&amp;#39;][x] == i0])):
                return l0 + self.n_deprel if (l0 &amp;gt;= 0) and (l0 &amp;lt; self.n_deprel) else None
            else:
                return None if len(buf) == 0 else self.n_trans - 1
    
    &amp;#39;&amp;#39;&amp;#39;
    for each sentence in training data, create stack with only ROOT in id form [0], buf with all tokens of the sentence, and empty arcs, get oracle for the 
    stack[ROOT] and full buffer (oracle should be shift for this case), get legal labels (only shift is legal for stack[ROOT] and full buffer), so in this
    round the legal_labels will return [0,0,1], then update stack, buffer and arcs, repeate this procedure for the updated stack, buffer and arcs for 
    2*#ofwords times. for each update, instances are accumulated as walking through the whole sentence as [], each sentence is an instances, all_instances  =
    [[instances1], [instances2], ...[]]
    &amp;#39;&amp;#39;&amp;#39;
    def create_instances(self, examples):
        all_instances = []
        succ = 0
        for n_ex, ex in enumerate(examples):
            n_words = len(ex[&amp;#39;word&amp;#39;]) - 1

            # arcs = {(h, t, label)}
            stack = [0]
            buf = [i + 1 for i in range(n_words)]
            arcs = []
            instances = []
            for i in range(n_words * 2): # rolling over each word to get the features (legal label and gold_t) for each transition in (llc, lc, w, rc, rrc) for each sentence, the transitions have 2*n_word times 
                gold_t = self.get_oracle(stack, buf, ex)
                if gold_t is None:
                    break
                legal_labels = self.legal_labels(stack, buf)
                assert legal_labels[gold_t] == 1 # correct transition must belong to legal lables
                instances.append((self.extract_features(stack, buf, arcs, ex),
                                  legal_labels, gold_t)) 
                if gold_t == self.n_trans - 1: # gold_t is shift= 2
                    stack.append(buf[0])
                    buf = buf[1:]
                elif gold_t &amp;lt; self.n_deprel: #n_deprel = 1, when gold_t ==0,
                    arcs.append((stack[-1], stack[-2], gold_t)) # i.e. L_arc, next gold_t must be 2, i.e. shift, stack is all the words except the -2 word, then next loop will be adding the first word in buffer to stack (execute the first if).
                    stack = stack[:-2] + [stack[-1]]
                else:
                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))
                    stack = stack[:-1] #when gold_t ==1, R_arc, next gold_t must be 0, i.e. shift, stack will be all the words except the last word, then next loop will be adding the first word in buffer to stack (execute the first if).
            succ += 1
            all_instances += instances # one instance is one sentence with 2*n_word tuples, each tuple contains context features, legal_labels, gold_t
        return all_instances

    def legal_labels(self, stack, buf):
        labels = ([1] if len(stack) &amp;gt; 2 else [0]) * self.n_deprel #stack&amp;lt;=2 must not be L-arc, stack&amp;gt;2, L-arc is legal
        labels += ([1] if len(stack) &amp;gt;= 2 else [0]) * self.n_deprel
        labels += [1] if len(buf) &amp;gt; 0 else [0] #buf &amp;gt; 0,  shift is legal 
        return labels # labels&amp;#39; length == n_tran, 3 or 49&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After defining the Preprocessor, it is ready to load and preprocess data. It read in the train set, dev set and test set first, and slice sentences for debugging purpose, initialize Preprocessor, initalize embedding matrix for all tokens in train set and then map each row in embedding matrix to corresponding token (this is the pretrained embedding matrix), vectorize train, dev and test sets.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def load_and_preprocess_data(reduced=True):
    config = Config()

    print(&amp;quot;Loading data...&amp;quot;),
    start = time.time()
    train_set = read_conll(os.path.join(config.data_path, config.train_file),
                           lowercase=config.lowercase)
    dev_set = read_conll(os.path.join(config.data_path, config.dev_file),
                         lowercase=config.lowercase)
    test_set = read_conll(os.path.join(config.data_path, config.test_file),
                          lowercase=config.lowercase)
    if reduced:
        train_set = train_set[:1000]
        dev_set = dev_set[:500]
        test_set = test_set[:500]
    print(&amp;quot;took {:.2f} seconds&amp;quot;.format(time.time() - start))

    print(&amp;quot;Building parser...&amp;quot;),
    start = time.time()
    preprocessor = Preprocessor(train_set)
    print(&amp;quot;took {:.2f} seconds&amp;quot;.format(time.time() - start))

    print(&amp;quot;Loading pretrained embeddings...&amp;quot;),
    start = time.time()
    word_vectors = {}
    for line in open(config.embedding_file).readlines():
        sp = line.strip().split()
        word_vectors[sp[0]] = [float(x) for x in sp[1:]] #sp[0] is all vocabulary, sp[1:] is the corresponding word vector 
    embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (preprocessor.n_tokens, 50)), dtype=&amp;#39;float32&amp;#39;) #embeding: is n_tokens * 50

    for token in preprocessor.tok2id: #all token/ POS/ label in train set
        i = preprocessor.tok2id[token]
        if token in word_vectors:
            embeddings_matrix[i] = word_vectors[token]
        elif token.lower() in word_vectors:
            embeddings_matrix[i] = word_vectors[token.lower()] #fill embedding matrix with corresponding word_vectors as input x
    print(&amp;quot;took {:.2f} seconds&amp;quot;.format(time.time() - start))
    
    print(&amp;quot;Vectorizing data...&amp;quot;),
    start = time.time()
    train_set = preprocessor.vectorize(train_set)
    dev_set = preprocessor.vectorize(dev_set) # each sentence conresponds to a dictionary of {&amp;#39;word&amp;#39;:id, &amp;#39;head&amp;#39;: id, &amp;#39;pos&amp;#39;: id, &amp;#39;label&amp;#39;: id}
    test_set = preprocessor.vectorize(test_set)
    print(&amp;quot;took {:.2f} seconds&amp;quot;.format(time.time() - start))

    print(&amp;quot;Preprocessing training data...&amp;quot;),
    start = time.time()
    train_examples = preprocessor.create_instances(train_set) # each sentence contain 2*n_word tuple of context features in word, POS, legal lable and gold_t
    print(&amp;quot;took {:.2f} seconds&amp;quot;.format(time.time() - start))

    return preprocessor, embeddings_matrix, train_examples, dev_set, test_set,&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is creating partial parses object. Treating each sentence as a partial parse with attribute ‘stack’, ‘buffer’, ‘dependencies’, and with method ‘parse_step’ to update words in stack and buffers according to predicted transitions, the parse method returns dependencies for each transition.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class PartialParse(object):
    def __init__(self, sentence):
        &amp;quot;&amp;quot;&amp;quot;Initializes this partial parse.
        Your code should initialize the following fields:
            self.stack: The current stack represented as a list with the top of the stack as the
                        last element of the list.
            self.buffer: The current buffer represented as a list with the first item on the
                         buffer as the first item of the list
            self.dependencies: The list of dependencies produced so far. Represented as a list of
                    tuples where each tuple is of the form (head, dependent).
                    Order for this list doesn&amp;#39;t matter.
        The root token should be represented with the string &amp;quot;ROOT&amp;quot;
        Args:
            sentence: The sentence to be parsed as a list of words.
                      Your code should not modify the sentence.
        &amp;quot;&amp;quot;&amp;quot;
        # The sentence being parsed is kept for bookkeeping purposes. Do not use it in your code.
        self.sentence = sentence

        ### YOUR CODE HERE
        self.stack = [&amp;#39;ROOT&amp;#39;]
        self.buffer = sentence[:]
        self.dependencies = []
        ### END YOUR CODE

    def parse_step(self, transition):
        &amp;quot;&amp;quot;&amp;quot;Performs a single parse step by applying the given transition to this partial parse
        Args:
            transition: A string that equals &amp;quot;S&amp;quot;, &amp;quot;LA&amp;quot;, or &amp;quot;RA&amp;quot; representing the shift, left-arc,
                        and right-arc transitions.
        &amp;quot;&amp;quot;&amp;quot;
        ### YOUR CODE HERE
        if transition == &amp;quot;S&amp;quot;:
            self.stack.append(self.buffer[0])
            self.buffer.pop(0)
        elif transition == &amp;quot;LA&amp;quot;:
            self.dependencies.append((self.stack[-1], self.stack[-2]))
            self.stack.pop(-2)
        else:
            self.dependencies.append((self.stack[-2], self.stack[-1]))
            self.stack.pop(-1)
            ### END YOUR CODE

    def parse(self, transitions):
        &amp;quot;&amp;quot;&amp;quot;Applies the provided transitions to this PartialParse
        Args:
            transitions: The list of transitions in the order they should be applied
        Returns:
            dependencies: The list of dependencies produced when parsing the sentence. Represented
                          as a list of tuples where each tuple is of the form (head, dependent)
        &amp;quot;&amp;quot;&amp;quot;
        for transition in transitions:
            self.parse_step(transition)
        return self.dependencies&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, define the model used to predict the transitions for each set of features in each sentences. This is used when evaluating the dev_set and test_set. It is important to use sess as the argument in the predict method so that the most updated variables (the weight matrices and the bias terms in this cases) in TensorFlow in the training stage can be applied to the evaluation set.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from utils.model import Model #want to use its predict_on_batch method

class ModelWrapper(object):
    def __init__(self, sess, preprocessor, dataset, sentence_id_to_idx):
        self.sess = sess
        self.preprocessor = preprocessor
        self.dataset = dataset
        self.sentence_id_to_idx = sentence_id_to_idx

    def predict(self, model, partial_parses): #model and partial_parses are imported and defined above.
        mb_x = [self.preprocessor.extract_features(p.stack, p.buffer, p.dependencies,
                                                   self.dataset[self.sentence_id_to_idx[id(p.sentence)]]) #get context features, pos, label in ID FORM for each sentence in partial_parses, since dev_set&amp;#39;s word are expressed in id form, rather than character like in train_set, mb_x can be type of &amp;#39;int32&amp;#39;
                for p in partial_parses] #partial parses are initialized in parse() method below 
        mb_x = np.array(mb_x).astype(&amp;#39;int32&amp;#39;) #inputs_batch, which is from dev_set
        mb_l = [self.preprocessor.legal_labels(p.stack, p.buffer) for p in partial_parses]
        
        pred = model.predict_on_batch(self.sess, mb_x)
        pred = np.argmax(pred + 10000 * np.array(mb_l).astype(&amp;#39;float32&amp;#39;), 1)
        pred = [&amp;quot;S&amp;quot; if p == 2 else (&amp;quot;LA&amp;quot; if p == 0 else &amp;quot;RA&amp;quot;) for p in pred]
        return pred

def parse(sess, model, preprocessor, dataset, eval_batch_size=5000): #here dataset is dev_set
    sentences = []
    sentence_id_to_idx = {}
    for i, example in enumerate(dataset):
        n_words = len(example[&amp;#39;word&amp;#39;]) - 1
        sentence = [j + 1 for j in range(n_words)]
        #sentence = [j for j in example[&amp;#39;word&amp;#39;]]
        sentences.append(sentence)
        sentence_id_to_idx[id(sentence)] = i
    
    MW = ModelWrapper(sess, preprocessor, dataset, sentence_id_to_idx)
    #dependencies = minibatch_parse(sentences, model, eval_batch_size)
    partial_parses = [PartialParse(s) for s in sentences]

    unfinished_parse = partial_parses

    while len(unfinished_parse) &amp;gt; 0:
        minibatch = unfinished_parse[0:eval_batch_size]
        # perform transition and single step parser on the minibatch until it is empty
        while len(minibatch) &amp;gt; 0:
            transitions =MW.predict(model, minibatch) #the model remembers the optimized variables for the sess, so it can predict on the minibatch from dev_set, get the predicted transitions, feature extractions are done within predict() method in ModelWrapper
            for index, action in enumerate(transitions):
                minibatch[index].parse_step(action) #update the stack and buff for each features (list of context word, pos and labels) in each sentence
            minibatch = [parse for parse in minibatch if len(parse.stack) &amp;gt; 1 or len(parse.buffer) &amp;gt; 0]

        # move to the next batch
        unfinished_parse = unfinished_parse[eval_batch_size:]

    dependencies = []
    for n in range(len(sentences)):
        dependencies.append(partial_parses[n].dependencies)
    
    # get the Unlabeled Accuracy Score (maybe)
    UAS = all_tokens = 0.0 
    for i, ex in enumerate(dataset):
        head = [-1] * len(ex[&amp;#39;word&amp;#39;])
        for h, t, in dependencies[i]:
            head[t] = h
        for pred_h, gold_h, gold_l, pos in zip(head[1:], ex[&amp;#39;head&amp;#39;][1:], ex[&amp;#39;label&amp;#39;][1:], ex[&amp;#39;pos&amp;#39;][1:]):
            assert preprocessor.id2tok[pos].startswith(P_PREFIX)
            pos_str = preprocessor.id2tok[pos][len(P_PREFIX):]
            if (preprocessor.with_punct) or (not punct(preprocessor.language, pos_str)):
                UAS += 1 if pred_h == gold_h else 0
                all_tokens += 1
    UAS /= all_tokens
    return UAS, dependencies &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, the deep neural network can be built in TensorFlow framework&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import os
import time
import tensorflow as tf
import pickle

class Config(object):
    &amp;quot;&amp;quot;&amp;quot;Holds model hyperparams and data information.
    The config class is used to store various hyperparameters and dataset
    information parameters. Model objects are passed a Config() object at
    instantiation.
    &amp;quot;&amp;quot;&amp;quot;
    n_features = 36
    n_classes = 3
    dropout = 0.5
    embed_size = 50
    hidden_size = 200
    batch_size = 2048
    n_epochs = 10
    lr = 0.001
    language = &amp;#39;english&amp;#39;
    with_punct = True
    unlabeled = True
    lowercase = True
    use_pos = True
    use_dep = True
    use_dep = use_dep and (not unlabeled)
    data_path = &amp;#39;./data&amp;#39;
    train_file = &amp;#39;train.conll&amp;#39;
    dev_file = &amp;#39;dev.conll&amp;#39;
    test_file = &amp;#39;test.conll&amp;#39;
    embedding_file = &amp;#39;./data/en-cw.txt&amp;#39;
    
class ParserModel(Model):
    &amp;quot;&amp;quot;&amp;quot;
    Implements a feedforward neural network with an embedding layer and single hidden layer.
    This network will predict which transition should be applied to a given partial parse
    configuration.
    &amp;quot;&amp;quot;&amp;quot;

    def add_placeholders(self):
 
        ### YOUR CODE HERE
        self.input_placeholder = tf.placeholder(tf.int32, [None, self.config.n_features])
        self.labels_placeholder = tf.placeholder(tf.float32, [None, self.config.n_classes])
        self.dropout_placeholder = tf.placeholder(tf.float32)
        self.beta_regul = tf.placeholder(tf.float32)
        ### END YOUR CODE

    def create_feed_dict(self, inputs_batch, labels_batch=None, dropout=1, beta_regul=10e-7):
  
        ### YOUR CODE HERE
        feed_dict = {self.input_placeholder: inputs_batch, \
                     self.dropout_placeholder: dropout, \
                     self.beta_regul: beta_regul
                     }
        if labels_batch is not None:
            feed_dict[self.labels_placeholder] = labels_batch
        ### END YOUR CODE
        return feed_dict

    def add_embedding(self):

        ### YOUR CODE HERE
        embedded = tf.Variable(self.pretrained_embeddings)
        embeddings = tf.nn.embedding_lookup(embedded,self.input_placeholder)
        embeddings = tf.reshape(embeddings, [-1, self.config.n_features * self.config.embed_size])
        ### END YOUR CODE
        return embeddings

    def add_prediction_op(self):

        x = self.add_embedding()
        ### YOUR CODE HERE
        xavier = xavier_weight_init()
        with tf.variable_scope(&amp;quot;transformation&amp;quot;):
            b1 = tf.Variable(tf.random_uniform([self.config.hidden_size,]))
            b2 = tf.Variable(tf.random_uniform([self.config.n_classes]))

            self.W = W = xavier([self.config.n_features * self.config.embed_size, self.config.hidden_size])
            U = xavier([self.config.hidden_size, self.config.n_classes])

            z1 = tf.matmul(x,W) + b1
            h = tf.nn.relu(z1)
            h_drop = tf.nn.dropout(h,self.dropout_placeholder)
            pred = tf.matmul(h_drop, U) + b2
        ### END YOUR CODE
        return pred

    def add_loss_op(self, pred):
 
        ### YOUR CODE HERE
        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=self.labels_placeholder)
        loss += self.beta_regul * tf.nn.l2_loss(self.W)
        loss = tf.reduce_mean(loss)
        ### END YOUR CODE
        return loss

    def add_training_op(self, loss):

        ### YOUR CODE HERE
        adam_optim = tf.train.AdamOptimizer(self.config.lr)
        train_op = adam_optim.minimize(loss)
        ### END YOUR CODE
        return train_op

    def train_on_batch(self, sess, inputs_batch, labels_batch):
        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch,
                                     dropout=self.config.dropout)
        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)
        return loss

    def run_epoch(self, sess, model, preprocessor,train_examples, dev_set):
        prog = tf.keras.utils.Progbar(target=1 + len(train_examples) / self.config.batch_size)
        for i, (train_x, train_y) in enumerate(minibatches(train_examples, self.config.batch_size)):
            loss = self.train_on_batch(sess, train_x, train_y)
            prog.update(i + 1, [(&amp;quot;train loss&amp;quot;, loss)])

        print(&amp;quot;Evaluating on dev set&amp;quot;),
        dev_UAS, _ = parse(sess, model, preprocessor, dev_set)
        print(&amp;quot;- dev UAS: {:.2f}&amp;quot;.format(dev_UAS * 100.0))
        return dev_UAS

    def fit(self, sess, model, saver, preprocessor,train_examples, dev_set):
        best_dev_UAS = 0
        for epoch in range(self.config.n_epochs):
            print(&amp;quot;Epoch {:} out of {:}&amp;quot;.format(epoch + 1, self.config.n_epochs))
            dev_UAS = self.run_epoch(sess, model, preprocessor,train_examples, dev_set)
            if dev_UAS &amp;gt; best_dev_UAS:
                best_dev_UAS = dev_UAS
                if saver:
                    print(&amp;quot;New best dev UAS! Saving model in ./data/weights/parser.weights&amp;quot;)
                    saver.save(sess, &amp;#39;./data/weights/parser.weights&amp;#39;)
            print

    def __init__(self, config, pretrained_embeddings):
        self.pretrained_embeddings = pretrained_embeddings
        self.config = config
        self.build()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before combining everything together, define the follow methods. minibatches() are used to extract the input batch and lable batch for the feeder in tensorflow; xavier_weight_init() used to randomly generate W and U matrices in the prediction functions.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from utils.general_utils import get_minibatches

def minibatches(data, batch_size):
    x = np.array([d[0] for d in data])
    y = np.array([d[2] for d in data])
    one_hot = np.zeros((y.size, 3))
    one_hot[np.arange(y.size), y] = 1
    return get_minibatches([x, one_hot], batch_size)

def xavier_weight_init():
    &amp;quot;&amp;quot;&amp;quot;Returns function that creates random tensor.
    The specified function will take in a shape (tuple or 1-d array) and
    returns a random tensor of the specified shape drawn from the
    Xavier initialization distribution.
    Hint: You might find tf.random_uniform useful.
    &amp;quot;&amp;quot;&amp;quot;
    def _xavier_initializer(shape, **kwargs):
        &amp;quot;&amp;quot;&amp;quot;Defines an initializer for the Xavier distribution.
        Specifically, the output should be sampled uniformly from [-epsilon, epsilon] where
            epsilon = sqrt(6) / &amp;lt;sum of the sizes of shape&amp;#39;s dimensions&amp;gt;
        e.g., if shape = (2, 3), epsilon = sqrt(6 / (2 + 3))
        This function will be used as a variable initializer.
        Args:
            shape: Tuple or 1-d array that species the dimensions of the requested tensor.
        Returns:
            out: tf.Tensor of specified shape sampled from the Xavier distribution.
        &amp;quot;&amp;quot;&amp;quot;
        ### YOUR CODE HERE
        epsilon = np.sqrt(6 / np.sum(shape))
        out = tf.Variable(tf.random_uniform(shape=shape, minval=-epsilon, maxval=epsilon))
        ### END YOUR CODE
        return out
    # Returns defined initializer function.
    return _xavier_initializer&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Combining everything to the main() function:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def main(debug=True):
    print(80 * &amp;quot;=&amp;quot;)
    print(&amp;quot;INITIALIZING&amp;quot;)
    print(80 * &amp;quot;=&amp;quot;)
    config = Config()
    preprocessor, embeddings, train_examples, dev_set, test_set = load_and_preprocess_data(debug)
    if not os.path.exists(&amp;#39;./data/weights/&amp;#39;):
        os.makedirs(&amp;#39;./data/weights/&amp;#39;)

    with tf.Graph().as_default():
        print(&amp;quot;Building model...&amp;quot;),
        start = time.time()
        model = ParserModel(config, embeddings)
        #parser.model = model
        print(&amp;quot;took {:.2f} seconds\n&amp;quot;.format(time.time() - start))

        init = tf.global_variables_initializer()
        # If you are using an old version of TensorFlow, you may have to use
        # this initializer instead.
        # init = tf.initialize_all_variables()
        saver = None if debug else tf.train.Saver()

        with tf.Session() as sess:
            #parser.session = sess
            sess.run(init)

            print(80 * &amp;quot;=&amp;quot;)
            print(&amp;quot;TRAINING&amp;quot;)
            print(80 * &amp;quot;=&amp;quot;)
            model.fit(sess, model, saver, preprocessor,train_examples, dev_set) #train_examples provide the input_batch and label batch by minibatches() method defined above 

            if not debug:
                print(80 * &amp;quot;=&amp;quot;)
                print(&amp;quot;TESTING&amp;quot;)
                print(80 * &amp;quot;=&amp;quot;)
                print(&amp;quot;Restoring the best model weights found on the dev set&amp;quot;)
                saver.restore(sess, &amp;#39;./data/weights/parser.weights&amp;#39;)
                print(&amp;quot;Final evaluation on test set&amp;quot;,)
                UAS, dependencies = parse(sess, model, preprocessor, test_set)
                print(&amp;quot;- test UAS: {:.2f}&amp;quot;.format(UAS * 100.0))
                print(&amp;quot;Writing predictions&amp;quot;)
                with open(&amp;#39;q2_test.predicted.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
                    pickle.dump(dependencies, f, -1)
                print(&amp;quot;Done!&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, run the main function with debug mode:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;main(True)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results under debug mode. After 10 epoch of training, UAS is 68.56.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;================================================================================
INITIALIZING
================================================================================
Loading data...
took 11.97 seconds
Building parser...
took 0.09 seconds
Loading pretrained embeddings...
took 6.36 seconds
Vectorizing data...
took 0.17 seconds
Preprocessing training data...
took 3.79 seconds
Building model...
took 0.52 seconds

================================================================================
TRAINING
================================================================================
Epoch 1 out of 10
24/24 [============================&amp;gt;.] - ETA: 0s - train loss: 1.0069Evaluating on dev set
- dev UAS: 45.86
Epoch 2 out of 10
24/24 [============================&amp;gt;.] - ETA: 0s - train loss: 0.4400Evaluating on dev set
- dev UAS: 53.38
Epoch 3 out of 10
24/24 [============================&amp;gt;.] - ETA: 0s - train loss: 0.3473Evaluating on dev set
- dev UAS: 58.92
Epoch 4 out of 10
24/24 [============================&amp;gt;.] - ETA: 0s - train loss: 0.3010Evaluating on dev set
- dev UAS: 61.86
Epoch 5 out of 10
24/24 [============================&amp;gt;.] - ETA: 0s - train loss: 0.2675Evaluating on dev set
- dev UAS: 62.74
Epoch 6 out of 10
24/24 [============================&amp;gt;.] - ETA: 0s - train loss: 0.2419Evaluating on dev set
- dev UAS: 64.17
Epoch 7 out of 10
24/24 [============================&amp;gt;.] - ETA: 0s - train loss: 0.2214Evaluating on dev set
- dev UAS: 65.05
Epoch 8 out of 10
24/24 [============================&amp;gt;.] - ETA: 0s - train loss: 0.2016Evaluating on dev set
- dev UAS: 67.05
Epoch 9 out of 10
24/24 [============================&amp;gt;.] - ETA: 0s - train loss: 0.1881Evaluating on dev set
- dev UAS: 67.45
Epoch 10 out of 10
24/24 [============================&amp;gt;.] - ETA: 0s - train loss: 0.1733Evaluating on dev set
- dev UAS: 68.56&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Training on the full data, the final UAS is 88.91.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;================================================================================
INITIALIZING
================================================================================
Loading data...
took 5.62 seconds
Building parser...
took 4.05 seconds
Loading pretrained embeddings...
took 6.82 seconds
Vectorizing data...
took 4.56 seconds
Preprocessing training data...
took 144.24 seconds
Building model...
took 0.65 seconds

================================================================================
TRAINING
================================================================================
Epoch 1 out of 10
928/928 [============================&amp;gt;.] - ETA: 0s - train loss: 0.2253Evaluating on dev set
- dev UAS: 82.99
New best dev UAS! Saving model in ./data/weights/parser.weights
Epoch 2 out of 10
928/928 [============================&amp;gt;.] - ETA: 0s - train loss: 0.1231Evaluating on dev set
- dev UAS: 85.37
New best dev UAS! Saving model in ./data/weights/parser.weights
Epoch 3 out of 10
928/928 [============================&amp;gt;.] - ETA: 0s - train loss: 0.1069Evaluating on dev set
- dev UAS: 86.63
New best dev UAS! Saving model in ./data/weights/parser.weights
Epoch 4 out of 10
928/928 [============================&amp;gt;.] - ETA: 0s - train loss: 0.0969Evaluating on dev set
- dev UAS: 87.34
New best dev UAS! Saving model in ./data/weights/parser.weights
Epoch 5 out of 10
928/928 [============================&amp;gt;.] - ETA: 0s - train loss: 0.0902Evaluating on dev set
- dev UAS: 87.09
Epoch 6 out of 10
928/928 [============================&amp;gt;.] - ETA: 0s - train loss: 0.0839Evaluating on dev set
- dev UAS: 87.84
New best dev UAS! Saving model in ./data/weights/parser.weights
Epoch 7 out of 10
928/928 [============================&amp;gt;.] - ETA: 0s - train loss: 0.0792Evaluating on dev set
- dev UAS: 88.21
New best dev UAS! Saving model in ./data/weights/parser.weights
Epoch 8 out of 10
928/928 [============================&amp;gt;.] - ETA: 0s - train loss: 0.0747Evaluating on dev set
- dev UAS: 88.08
Epoch 9 out of 10
928/928 [============================&amp;gt;.] - ETA: 0s - train loss: 0.0711Evaluating on dev set
- dev UAS: 88.26
New best dev UAS! Saving model in ./data/weights/parser.weights
Epoch 10 out of 10
928/928 [============================&amp;gt;.] - ETA: 0s - train loss: 0.0675Evaluating on dev set
- dev UAS: 88.52
New best dev UAS! Saving model in ./data/weights/parser.weights
================================================================================
TESTING
================================================================================
Restoring the best model weights found on the dev set
INFO:tensorflow:Restoring parameters from ./data/weights/parser.weights
Final evaluation on test set
- test UAS: 88.91
Writing predictions
Done!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final dependencies for each transition is in id form and will look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[(7, 6), (7, 5), (7, 4), (7, 3), (7, 2), (7, 1), (7, 8), (0, 7)], [],...]&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
  </channel>
</rss>