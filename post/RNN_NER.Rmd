---
autoCollapseToc: true
title: "CS224n/assignment3/: Detailed Explanation of RNN Name Entity Recognition"
author: "Luyao Peng"
date: '2019-06-29'
categories:
  - Stanford NLP and Deep Learning
tags:
  - Deep Learning
  - NLP
  - Name Entity Recognition
  - RNN
draft: false
toc: false
header-includes: \usepackage{amsmath} \usepackage{bm} 
---

This post deals with the name entity recognition task using RNN model.

# The RNN model for NER

Let $\mathbf{x}_t$ be a one-hot vector for word at time $t$, define $\mathbf{E}\in \mathbb{R}^{V\times D}, \mathbf{W}_h \in \mathbb{R}^{H\times H}, \mathbf{W}_e\in \mathbb{R}^{D\times H}, \mathbf{U} \in \mathbb{R}^{H\times (C=5)},\mathbf{b}_1\in \mathbb{R}^{H}, \mathbf{b}_2 \in \mathbb{R}^{C}$, the RNN model to make prediction at time step $t$ can be expressed as

$$\mathbf{e}^t = \mathbf{x}^t\mathbf{E}\\ \mathbf{h}^t = \sigma(\mathbf{h}^{t-1}\mathbf{W}_h + \mathbf{e}^t\mathbf{W}_e+\mathbf{b}_1) \\ \hat{\mathbf{y}}^t = softmax(\mathbf{h}^t\mathbf{U} + \mathbf{b}_2)\\ J = \sum_tCE(\mathbf{y}^t-\hat{\mathbf{y}}^t)  =-\sum_t\sum_iy_i^tlog(\hat{y}_i^t).$$



# Code:


Using **load_and_preprocess_data()**, the raw train data and raw test data 

```python
[(['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'],
  ['ORG', 'O', 'MISC', 'O', 'O', 'O', 'MISC', 'O', 'O']),
 (['Peter', 'Blackburn'], ['PER', 'PER'])]
```

become list of tuples ([[ids, case types]], [labels]) as
```python
[([[1, 12],
   [2, 11],
   [3, 13],
   [4, 11],
   [5, 11],
   [6, 11],
   [7, 13],
   [8, 11],
   [9, 14]],
  [1, 4, 3, 4, 4, 4, 3, 4, 4]),
 ([[10, 13], [11, 13]], [0, 0])]
```

**load_embeddings()** matches the word vectors with the corresponding word in each sentence by searching in *helper.tok2id* dictionary. The embedding's shape is $19\times 50$ for the 2-sentence example and embed size is 50.

The two steps above are the same as in the window-based model. The followings are the different steps in creating RNN model from creating window-based model. 

## Defining RNN cell

Essentially, this step is to create the hidden cell by

$$\mathbf{h}^t = \sigma(\mathbf{h}^{t-1}\mathbf{W}_h + \mathbf{e}^t\mathbf{W}_e+\mathbf{b}_1).$$

- define the variables $\mathbf{W}_h, \mathbf{W}_e, \mathbf{b}_1$ with shape $\mathbf{W}_h\in \mathbb{R}^{H\times H}, \mathbf{W}_e \in \mathbb{R}^{(n window features\times D)\times H}, \mathbf{b}_1 \in \mathbb{R}^{1\times H}$. 

- create $\mathbf{h}^t$ following the formula

- return *output* and *new_state*. In RNN, *output* is the same as the new state $\mathbf{h}^t$.


```python
class RNNCell(tf.nn.rnn_cell.RNNCell):
    """Wrapper around our RNN cell implementation that allows us to play
    nicely with TensorFlow.
    """
    def __init__(self, input_size, state_size):
        self.input_size = input_size
        self._state_size = state_size

    @property
    def state_size(self):
        return self._state_size

    @property
    def output_size(self):
        return self._state_size

    def __call__(self, inputs, state, scope=None):
        scope = scope or type(self).__name__

        with tf.variable_scope(scope):
            b = tf.get_variable(name='b', shape = [self.state_size], initializer=tf.contrib.layers.xavier_initializer(seed=1))
            W_x = tf.get_variable(name='W_x', shape = [self.input_size, self.state_size], initializer=tf.contrib.layers.xavier_initializer(seed=2))
            W_h = tf.get_variable(name='W_h', shape = [self.state_size, self.state_size], initializer=tf.contrib.layers.xavier_initializer(seed=3))
            z1 = tf.matmul(inputs,W_x) + tf.matmul(state, W_h) + b
            new_state = tf.nn.sigmoid(z1)
            
        output = new_state
        return output, new_state
```

## Padding and masking
Implementing an RNN requires us to unroll the computation over the
whole sentence. Unfortunately, each sentence can be of arbitrary length and this would cause the
RNN to be unrolled a different number of times for different sentences, making it impossible to
BATCH PROCESS the data (assignment#3).

The most common way to address the unequal length of the sentences is *pad*. For example, suppose the longest sentence in the inputs is $M$ tokens, then for our example with 2 sentences, the lengths are 9 and 2, respectively, which are $<M$, so we pad the two sentences with zeros until they reaches length $M$. Meanwhile, a mask vector is also created for each sentence. The mask vector has `True` wherever there was a token in the original sequence and `False` for padded positions:

```python
# before
[([[89, 2647],
   [1070, 2646],
   [115, 2648],
   [288, 2646],
   [7, 2646],
   [1071, 2646],
   [78, 2648],
   [392, 2646],
   [1, 2649]],
  [1, 4, 3, 4, 4, 4, 3, 4, 4]),
 ([[1072, 2648], [175, 2648]], [0, 0])]
```

```python
# after
[([[89, 2647],
   [1070, 2646],
   [115, 2648],
   [288, 2646],
   [7, 2646],
   [1071, 2646],
   [78, 2648],
   [392, 2646],
   [1, 2649],
   [0, 0]],
  [1, 4, 3, 4, 4, 4, 3, 4, 4, 4],
  [True, True, True, True, True, True, True, True, True, False]),
 ([[1072, 2648],
   [175, 2648],
   [0, 0],
   [0, 0],
   [0, 0],
   [0, 0],
   [0, 0],
   [0, 0],
   [0, 0],
   [0, 0]],
  [0, 0, 4, 4, 4, 4, 4, 4, 4, 4],
  [True, True, False, False, False, False, False, False, False, False])]
```

The returned *ret* is a list of tuples containing three lists: padded sentences (each token is represented by 2 features), labels and mask vector.

## RNN model




