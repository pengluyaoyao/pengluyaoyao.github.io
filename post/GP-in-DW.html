

<p>Previous studies have shown the equivalency between Gaussian Process (GP) and infinitely wide fully-connected Neural Networks (NNs), which implies that if we choose a fully-connected NN with infinite width, the function computed by the NN is equivalent to a function drawn from a GP under appropriate statistical assumptions. The analytic forms of GP kernel for DNNs were also subsequentially developed.</p>
<p><img src="mvn_check" alt="Luyao Peng" width=90% height=80%/></p>
<p>We can show the correspondence by the following:</p>
<div id="define" class="section level3">
<h3>Define:</h3>
</div>
<div id="dnn-model-architecture" class="section level3">
<h3>DNN Model Architecture:</h3>
</div>
<div id="gp-component-in-dnn" class="section level3">
<h3>GP Component in DNN:</h3>
</div>
<div id="advantage-of-gpdnn" class="section level2">
<h2>Advantage of GPDNN:</h2>
<ol style="list-style-type: decimal">
<li>Ensemble learning</li>
<li>Closed-form predictions</li>
</ol>
</div>
<div id="references" class="section level2">
<h2>References:</h2>
<ol style="list-style-type: decimal">
<li><p>Rasmussen and Williams, 2006: Gaussian Processes for Machine Learning</p></li>
<li><p>Lee et al., 2018: DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES</p></li>
<li><p>Zhang et al., 2019: Sequential Gaussian Processes for Online Learning of Nonstationary Functions</p></li>
<li><p>Williams, 1997: Computing with infinite networks</p></li>
</ol>
</div>

