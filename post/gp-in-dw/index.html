<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Correspondence Between Infinitely Wide Deep Neural Networks and Gaussian Process - Luyao Peng&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Luyao Peng" /><meta name="description" content="Equivalency between Gaussian Process and DNNs Previous studies have shown the equivalency between Gaussian Process (GP) [1] and infinitely wide fully-connected Neural Networks (NNs) [2][4], which implies that if we choose a fully-connected NN with infinite width, the function computed by the NN is equivalent to a function drawn from a GP under appropriate statistical assumptions. The analytic forms of GP kernel for DNNs were also subsequentially developed.
The following figure shows the consistency between the theoretical GP (contour rings) and the sampled \(z_i\) at the initial layer (left) and the third layer (right) in a DNN." /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.73.0 with even 4.0.0" />


<link rel="canonical" href="/post/gp-in-dw/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">


<link href="/dist/even.c2a46f00.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Correspondence Between Infinitely Wide Deep Neural Networks and Gaussian Process" />
<meta property="og:description" content="Equivalency between Gaussian Process and DNNs Previous studies have shown the equivalency between Gaussian Process (GP) [1] and infinitely wide fully-connected Neural Networks (NNs) [2][4], which implies that if we choose a fully-connected NN with infinite width, the function computed by the NN is equivalent to a function drawn from a GP under appropriate statistical assumptions. The analytic forms of GP kernel for DNNs were also subsequentially developed.
The following figure shows the consistency between the theoretical GP (contour rings) and the sampled \(z_i\) at the initial layer (left) and the third layer (right) in a DNN." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/gp-in-dw/" />
<meta property="article:published_time" content="2019-12-25T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-12-25T00:00:00+00:00" />
<meta itemprop="name" content="Correspondence Between Infinitely Wide Deep Neural Networks and Gaussian Process">
<meta itemprop="description" content="Equivalency between Gaussian Process and DNNs Previous studies have shown the equivalency between Gaussian Process (GP) [1] and infinitely wide fully-connected Neural Networks (NNs) [2][4], which implies that if we choose a fully-connected NN with infinite width, the function computed by the NN is equivalent to a function drawn from a GP under appropriate statistical assumptions. The analytic forms of GP kernel for DNNs were also subsequentially developed.
The following figure shows the consistency between the theoretical GP (contour rings) and the sampled \(z_i\) at the initial layer (left) and the third layer (right) in a DNN.">
<meta itemprop="datePublished" content="2019-12-25T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2019-12-25T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="940">



<meta itemprop="keywords" content="Deep Neural Networks,Gaussian Process," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Correspondence Between Infinitely Wide Deep Neural Networks and Gaussian Process"/>
<meta name="twitter:description" content="Equivalency between Gaussian Process and DNNs Previous studies have shown the equivalency between Gaussian Process (GP) [1] and infinitely wide fully-connected Neural Networks (NNs) [2][4], which implies that if we choose a fully-connected NN with infinite width, the function computed by the NN is equivalent to a function drawn from a GP under appropriate statistical assumptions. The analytic forms of GP kernel for DNNs were also subsequentially developed.
The following figure shows the consistency between the theoretical GP (contour rings) and the sampled \(z_i\) at the initial layer (left) and the third layer (right) in a DNN."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">LP&#39;s NLP Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/post/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">LP&#39;s NLP Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/post/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<script async type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Correspondence Between Infinitely Wide Deep Neural Networks and Gaussian Process</h1>

      <div class="post-meta">
        <span class="post-time"> 2019-12-25 </span>
        <div class="post-category">
            <a href="/categories/deep-learning-and-nlp/"> Deep Learning and NLP </a>
            </div>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content always-active">
    
  </div>
</div>
    <div class="post-content">
      


<div id="equivalency-between-gaussian-process-and-dnns" class="section level2">
<h2>Equivalency between Gaussian Process and DNNs</h2>
<p>Previous studies have shown the equivalency between Gaussian Process (GP) [1] and infinitely wide fully-connected Neural Networks (NNs) [2][4], which implies that if we choose a fully-connected NN with infinite width, the function computed by the NN is equivalent to a function drawn from a GP under appropriate statistical assumptions. The analytic forms of GP kernel for DNNs were also subsequentially developed.</p>
<p>The following figure shows the consistency between the theoretical GP (contour rings) and the sampled <span class="math inline">\(z_i\)</span> at the initial layer (left) and the third layer (right) in a DNN. Note the kernel <span class="math inline">\(K\)</span> in MVN is computed based on the method in [2].</p>
<p><img src="/post/Window_based_Name_Entity_Recognition_files/figure-html/mvn_check.png" alt="Luyao Peng" width=90% height=80%/></p>
<p>We can also show the correspondence theoretically.</p>
<p>Consider a fully-connected feed-forward NN with <span class="math inline">\(L\)</span> layers. Let <span class="math inline">\(\phi\)</span> be pointwise nonlinear function, <span class="math inline">\(X^l\)</span> and <span class="math inline">\(y\)</span> denote the input features at the <span class="math inline">\(l\)</span>th layer and the target, respectively, <span class="math inline">\(z_i^l\)</span> is the ith post-linear transformation component at the <span class="math inline">\(l\)</span>th layer, <span class="math inline">\(i=1, \dots, n^l\)</span>, <span class="math inline">\(Z^l=[z_1^l, \dots, z_{n^l}^l]\)</span>, <span class="math inline">\(n^l\)</span> is the width at the <span class="math inline">\(l\)</span>th layer, <span class="math inline">\(w_i^l\)</span> and <span class="math inline">\(b_i^l\)</span> are the weight and bias vectors for <span class="math inline">\(z_i^l\)</span>, then</p>
<p><span class="math display">\[\begin{equation}\label{eq:1}
\begin{aligned}[l]
z_i^0&amp;=X^0 w_i^0+b_i^0\\
z_i^1 &amp;= X^1w_i^1+b_i^1\\
&amp;\qquad \vdots\\
\quad z_i^{L-1} &amp;= X^{L-1}w_i^{L-1}+b_i^{L-1} \quad\\
\quad z^{L} &amp;=X^{L}w^{L}+b^{L}\\
\mbox{prediction:}\quad y &amp;= z^L+\epsilon\\
\end{aligned}
\begin{aligned}[c]
X^1 &amp;= \phi(Z^0) \\
X^2 &amp;= \phi(Z^1)\\
&amp;\quad\vdots\\
X^L &amp;= \phi(Z^{L-1})\\
&amp;\quad\\
&amp;\quad\\
\end{aligned}
\end{equation}\]</span></p>
<p>If we assume <span class="math inline">\(w_i^{l} \stackrel{iid}{\sim} MVN(0, \frac{\sigma_w^{2}}{n^l}I),b_i^{l}\stackrel{iid}{\sim} MVN(0, \sigma_b^{2}I)\)</span>, then</p>
<ul>
<li><p>Layer 0:
Each element in layer 0 is <span class="math inline">\(z_{ki}^0 = \mathlarger{\sum}_{j=1}^{n^0} x_{kj}w_{ji}^0+b_{ki}\)</span>, which is a sum of iid random variables for <span class="math inline">\(k=1, \dots, N\)</span>. If <span class="math inline">\(n^0 \rightarrow \infty\)</span>, we have <span class="math inline">\(z_i^0 \stackrel{iid}{\dot{\sim}} MVN(0, \frac{\sigma_w^2}{n^0}XX&#39;+\sigma_b^2I)\)</span> by Central Limit Theorem (CLT), where the <span class="math inline">\(\cdot\)</span> indicates ‘approximately’.</p></li>
<li><p>Layer 1-L:
Each element in <span class="math inline">\(z_{i}^l\)</span> is also a sum of iid random variables, where <span class="math inline">\(z_i^l \stackrel{iid}{\dot{\sim}} MVN\left(0, K^l(X^l, X^{l})\right)\)</span>, where</p></li>
</ul>
<p><span class="math display">\[\begin{equation}\label{eq:2}
K^l(X^l, X^{l}) = \frac{\sigma_w^2}{n^l}\mathbb{E}\left[\phi(X^{l-1})\phi(X^{l-1})&#39;\right]+\sigma_b^2I, [2]
\end{equation}\]</span></p>
</div>
<div id="advantages-of-gpdnn" class="section level2">
<h2>Advantages of GPDNN:</h2>
<ol style="list-style-type: decimal">
<li>GPDNNs are essentially a DNN based on bayesian method to weight the final predictions by the posterior probabilities of the parameters, weights and bias in each layer, which implies the ensemble learning nature of GPDNN, because it takes into account of the probabilities of all possible values of weights and bias parameters. This is important in reducing the prediction variance, achieving a stable predictions and avoid overfitting problems that exist in deep learning techniques.</li>
<li>The gaussian process regression or classification model [1] can be implemented after driving the GP forms of a DNN, which yields closed-form predictions/ This saves the training time dramatically compared to deep learning methods.</li>
<li>GPDNNs is scalable to large-scale data by using gpytorch library to solve Cholesky Decomposition in parallel.</li>
</ol>
</div>
<div id="references" class="section level2">
<h2>References:</h2>
<ol style="list-style-type: decimal">
<li><p>Rasmussen and Williams, 2006: Gaussian Processes for Machine Learning</p></li>
<li><p>Lee et al., 2018: Deep Neural Networks as Gaussian Processes</p></li>
<li><p>Zhang et al., 2019: Sequential Gaussian Processes for Online Learning of Nonstationary Functions</p></li>
<li><p>Williams, 1997: Computing with infinite networks</p></li>
</ol>
</div>
<div id="equivalency-between-gaussian-process-and-dnns-1" class="section level2">
<h2>Equivalency between Gaussian Process and DNNs</h2>
<p>Previous studies have shown the equivalency between Gaussian Process (GP) [1] and infinitely wide fully-connected Neural Networks (NNs) [2][4], which implies that if we choose a fully-connected NN with infinite width, the function computed by the NN is equivalent to a function drawn from a GP under appropriate statistical assumptions. The analytic forms of GP kernel for DNNs were also subsequentially developed.</p>
<p>The following figure shows the consistency between the theoretical GP (contour rings) and the sampled <span class="math inline">\(z_i\)</span> at the initial layer (left) and the third layer (right) in a DNN. Note the kernel <span class="math inline">\(K\)</span> in MVN is computed based on the method in [2].</p>
<p><img src="/post/Window_based_Name_Entity_Recognition_files/figure-html/mvn_check.png" alt="Luyao Peng" width=90% height=80%/></p>
<p>We can also show the correspondence theoretically.</p>
<p>Consider a fully-connected feed-forward NN with <span class="math inline">\(L\)</span> layers. Let <span class="math inline">\(\phi\)</span> be pointwise nonlinear function, <span class="math inline">\(X^l\)</span> and <span class="math inline">\(y\)</span> denote the input features at the <span class="math inline">\(l\)</span>th layer and the target, respectively, <span class="math inline">\(z_i^l\)</span> is the ith post-linear transformation component at the <span class="math inline">\(l\)</span>th layer, <span class="math inline">\(i=1, \dots, n^l\)</span>, <span class="math inline">\(Z^l=[z_1^l, \dots, z_{n^l}^l]\)</span>, <span class="math inline">\(n^l\)</span> is the width at the <span class="math inline">\(l\)</span>th layer, <span class="math inline">\(w_i^l\)</span> and <span class="math inline">\(b_i^l\)</span> are the weight and bias vectors for <span class="math inline">\(z_i^l\)</span>, then</p>
<p><span class="math display">\[\begin{equation}\label{eq:1}
\begin{aligned}[l]
z_i^0&amp;=X^0 w_i^0+b_i^0\\
z_i^1 &amp;= X^1w_i^1+b_i^1\\
&amp;\qquad \vdots\\
\quad z_i^{L-1} &amp;= X^{L-1}w_i^{L-1}+b_i^{L-1} \quad\\
\quad z^{L} &amp;=X^{L}w^{L}+b^{L}\\
\mbox{prediction:}\quad y &amp;= z^L+\epsilon\\
\end{aligned}
\begin{aligned}[c]
X^1 &amp;= \phi(Z^0) \\
X^2 &amp;= \phi(Z^1)\\
&amp;\quad\vdots\\
X^L &amp;= \phi(Z^{L-1})\\
&amp;\quad\\
&amp;\quad\\
\end{aligned}
\end{equation}\]</span></p>
<p>If we assume <span class="math inline">\(w_i^{l} \stackrel{iid}{\sim} MVN(0, \frac{\sigma_w^{2}}{n^l}I),b_i^{l}\stackrel{iid}{\sim} MVN(0, \sigma_b^{2}I)\)</span>, then</p>
<ul>
<li><p>Layer 0:
Each element in layer 0 is <span class="math inline">\(z_{ki}^0 = \mathlarger{\sum}_{j=1}^{n^0} x_{kj}w_{ji}^0+b_{ki}\)</span>, which is a sum of iid random variables for <span class="math inline">\(k=1, \dots, N\)</span>. If <span class="math inline">\(n^0 \rightarrow \infty\)</span>, we have <span class="math inline">\(z_i^0 \stackrel{iid}{\dot{\sim}} MVN(0, \frac{\sigma_w^2}{n^0}XX&#39;+\sigma_b^2I)\)</span> by Central Limit Theorem (CLT), where the <span class="math inline">\(\cdot\)</span> indicates ‘approximately’.</p></li>
<li><p>Layer 1-L:
Each element in <span class="math inline">\(z_{i}^l\)</span> is also a sum of iid random variables, where <span class="math inline">\(z_i^l \stackrel{iid}{\dot{\sim}} MVN\left(0, K^l(X^l, X^{l})\right)\)</span>, where</p></li>
</ul>
<p><span class="math display">\[\begin{equation}\label{eq:2}
K^l(X^l, X^{l}) = \frac{\sigma_w^2}{n^l}\mathbb{E}\left[\phi(X^{l-1})\phi(X^{l-1})&#39;\right]+\sigma_b^2I, [2]
\end{equation}\]</span></p>
</div>
<div id="advantages-of-gpdnn-1" class="section level2">
<h2>Advantages of GPDNN:</h2>
<ol style="list-style-type: decimal">
<li>GPDNNs are essentially a DNN based on bayesian method to weight the final predictions by the posterior probabilities of the parameters, weights and bias in each layer, which implies the ensemble learning nature of GPDNN, because it takes into account of the probabilities of all possible values of weights and bias parameters. This is important in reducing the prediction variance, achieving a stable predictions and avoid overfitting problems that exist in deep learning techniques.</li>
<li>The gaussian process regression or classification model [1] can be implemented after driving the GP forms of a DNN, which yields closed-form predictions/ This saves the training time dramatically compared to deep learning methods.</li>
<li>GPDNNs is scalable to large-scale data by using gpytorch library to solve Cholesky Decomposition in parallel.</li>
</ol>
</div>
<div id="references-1" class="section level2">
<h2>References:</h2>
<ol style="list-style-type: decimal">
<li><p>Rasmussen and Williams, 2006: Gaussian Processes for Machine Learning</p></li>
<li><p>Lee et al., 2018: Deep Neural Networks as Gaussian Processes</p></li>
<li><p>Zhang et al., 2019: Sequential Gaussian Processes for Online Learning of Nonstationary Functions</p></li>
<li><p>Williams, 1997: Computing with infinite networks</p></li>
</ol>
</div>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">Luyao Peng</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        2019-12-25
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/deep-neural-networks/">Deep Neural Networks</a>
          <a href="/tags/gaussian-process/">Gaussian Process</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/2020-03-07-content-generation/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Neural Text Generation</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/rnn_ner/">
            <span class="next-text nav-default">CS224n/assignment3/: RNN/GRU Name Entity Recognition</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  <div id="disqus_thread"></div>
  <script>
  (function() {
  var d = document, s = d.createElement('script');
  s.src = 'https://luyao-1.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:your@email.com" class="iconfont icon-email" title="email"></a>
      <a href="http://localhost:1313" class="iconfont icon-stack-overflow" title="stack-overflow"></a>
      <a href="http://localhost:1313" class="iconfont icon-twitter" title="twitter"></a>
      <a href="http://localhost:1313" class="iconfont icon-facebook" title="facebook"></a>
      <a href="http://localhost:1313" class="iconfont icon-linkedin" title="linkedin"></a>
      <a href="http://localhost:1313" class="iconfont icon-google" title="google"></a>
      <a href="http://localhost:1313" class="iconfont icon-github" title="github"></a>
      <a href="http://localhost:1313" class="iconfont icon-weibo" title="weibo"></a>
      <a href="http://localhost:1313" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="http://localhost:1313" class="iconfont icon-douban" title="douban"></a>
      <a href="http://localhost:1313" class="iconfont icon-pocket" title="pocket"></a>
      <a href="http://localhost:1313" class="iconfont icon-tumblr" title="tumblr"></a>
      <a href="http://localhost:1313" class="iconfont icon-instagram" title="instagram"></a>
      <a href="http://localhost:1313" class="iconfont icon-gitlab" title="gitlab"></a>
      <a href="http://localhost:1313" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  
  
  <span class="copyright-year">
    &copy; 
    2017 - 
    2020
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Luyao Peng</span>
  </span>
</div>


    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
<script type="text/javascript" src="/dist/even.26188efa.min.js"></script>








</body>
</html>
